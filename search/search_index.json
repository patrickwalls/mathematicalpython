{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mathematical Python Mathematical Python is an introduction to mathematical computing including: Jupyter notebooks, markdown and $\\LaTeX$ Basic Python programming: datatypes, variables, logic, loops and functions Scientific computing with NumPy , SciPy and Matplotlib Applications in calculus, linear algebra and differential equations Prerequisites We assume the reader has completed undergraduate courses in: Differential calculus: derivatives, Taylor series and optimization Integral calculus: integrals, Riemann sums, sequences and series Linear algebra: vector and matrix operations, systems of equations, eigenvalues and eigenvectors Differential equations: first and second order equations, Euler's method and systems of equations Author Patrick Walls is Associate Professor of Teaching in the Department of Mathematics at the University of British Columbia . Feedback Comments and suggestions are always welcome! Please contact Patrick Walls , make a pull request to the GitHub repo or share your thoughts in the Google form . Acknowledgements Thank you ... Pacific Institute for the Mathematical Science (PIMS) for creating Syzygy and hosting Jupyter notebooks for thousands of students and researchers across Canada Jupyter , Python and SciPy developers for creating open source scientific software MkDocs developers and Martin Donath for creating a Material Design theme for MkDocs License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License . Last Modified August 16 2022 14:12 PST","title":"About"},{"location":"#mathematical-python","text":"Mathematical Python is an introduction to mathematical computing including: Jupyter notebooks, markdown and $\\LaTeX$ Basic Python programming: datatypes, variables, logic, loops and functions Scientific computing with NumPy , SciPy and Matplotlib Applications in calculus, linear algebra and differential equations","title":"Mathematical Python"},{"location":"#prerequisites","text":"We assume the reader has completed undergraduate courses in: Differential calculus: derivatives, Taylor series and optimization Integral calculus: integrals, Riemann sums, sequences and series Linear algebra: vector and matrix operations, systems of equations, eigenvalues and eigenvectors Differential equations: first and second order equations, Euler's method and systems of equations","title":"Prerequisites"},{"location":"#author","text":"Patrick Walls is Associate Professor of Teaching in the Department of Mathematics at the University of British Columbia .","title":"Author"},{"location":"#feedback","text":"Comments and suggestions are always welcome! Please contact Patrick Walls , make a pull request to the GitHub repo or share your thoughts in the Google form .","title":"Feedback"},{"location":"#acknowledgements","text":"Thank you ... Pacific Institute for the Mathematical Science (PIMS) for creating Syzygy and hosting Jupyter notebooks for thousands of students and researchers across Canada Jupyter , Python and SciPy developers for creating open source scientific software MkDocs developers and Martin Donath for creating a Material Design theme for MkDocs","title":"Acknowledgements"},{"location":"#license","text":"This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License .","title":"License"},{"location":"#last-modified","text":"August 16 2022 14:12 PST","title":"Last Modified"},{"location":"calculus/polynomials/","text":"Polynomials Polynomials as Lists A polynomial of degree $d$ is a function of the form $$ p(x) = c_0 + c_1 x + \\cdots + c_d x^d $$ where $c_0,\\dots,c_d$ are numbers (such that $c_d \\ne 0$). A polynomial is determined by its coefficients $c_0,\\dots,c_d$ therefore we can represent a polynomial as a Python list of numbers p = [c0,c1,...,cd] For example: the list p = [1,3,5] represents the polynomial $p(x) = 1 + 3x + 5x^2$ the list p = [1,0,0,-1] represents the polynomial $p(x) = 1 - x^3$ the list p = [0,0,1,0,2] represents the polynomial $p(x) = x^2 + 2x^4$ Note that p[-1] corresponds to $c_d$ must be nonzero and len(p) - 1 is the degree $d$ of $p(x)$. Evaluation Write a function called poly_eval which takes a list p (representing a polynomial $p(x)$) and a number $a$ and returns $p(a)$. def poly_eval(p,a): pa = sum([p[n]*a**n for n in range(0,len(p))]) return pa Verify our function returns the correct value. For example, if $p(x) = 1 + x + x^2 + x^3$ and $a = -2$ then $p(a) = -5$. p = [1,1,1,1] a = -2 poly_eval(p,a) -5 Differentiation Write a function called poly_diff which takes a list p representing a polynomial $p(x)$ and returns the list representing the derivative $p'(x)$. def poly_diff(p): if len(p) == 1: dp = [0] else: dp = [n*p[n] for n in range(1,len(p))] return dp Verify our function returns the correct value. For example, if $p(x) = 1 + x + x^2 + x^3$ then $p'(x) = 1 + 2x + 3x^2$. p = [1,1,1,1] poly_diff(p) [1, 2, 3] If $p(x) = 1$ then $p'(x) = 0$. p = [1] poly_diff(p) [0] Integration Exercises","title":"Polynomials"},{"location":"calculus/polynomials/#polynomials","text":"","title":"Polynomials"},{"location":"calculus/polynomials/#polynomials-as-lists","text":"A polynomial of degree $d$ is a function of the form $$ p(x) = c_0 + c_1 x + \\cdots + c_d x^d $$ where $c_0,\\dots,c_d$ are numbers (such that $c_d \\ne 0$). A polynomial is determined by its coefficients $c_0,\\dots,c_d$ therefore we can represent a polynomial as a Python list of numbers p = [c0,c1,...,cd] For example: the list p = [1,3,5] represents the polynomial $p(x) = 1 + 3x + 5x^2$ the list p = [1,0,0,-1] represents the polynomial $p(x) = 1 - x^3$ the list p = [0,0,1,0,2] represents the polynomial $p(x) = x^2 + 2x^4$ Note that p[-1] corresponds to $c_d$ must be nonzero and len(p) - 1 is the degree $d$ of $p(x)$.","title":"Polynomials as Lists"},{"location":"calculus/polynomials/#evaluation","text":"Write a function called poly_eval which takes a list p (representing a polynomial $p(x)$) and a number $a$ and returns $p(a)$. def poly_eval(p,a): pa = sum([p[n]*a**n for n in range(0,len(p))]) return pa Verify our function returns the correct value. For example, if $p(x) = 1 + x + x^2 + x^3$ and $a = -2$ then $p(a) = -5$. p = [1,1,1,1] a = -2 poly_eval(p,a) -5","title":"Evaluation"},{"location":"calculus/polynomials/#differentiation","text":"Write a function called poly_diff which takes a list p representing a polynomial $p(x)$ and returns the list representing the derivative $p'(x)$. def poly_diff(p): if len(p) == 1: dp = [0] else: dp = [n*p[n] for n in range(1,len(p))] return dp Verify our function returns the correct value. For example, if $p(x) = 1 + x + x^2 + x^3$ then $p'(x) = 1 + 2x + 3x^2$. p = [1,1,1,1] poly_diff(p) [1, 2, 3] If $p(x) = 1$ then $p'(x) = 0$. p = [1] poly_diff(p) [0]","title":"Differentiation"},{"location":"calculus/polynomials/#integration","text":"","title":"Integration"},{"location":"calculus/polynomials/#exercises","text":"","title":"Exercises"},{"location":"calculus/taylor-series/","text":"Taylor Series Under construction","title":"Taylor Series"},{"location":"calculus/taylor-series/#taylor-series","text":"Under construction","title":"Taylor Series"},{"location":"differential-equations/first-order/","text":"First Order Equations A differential equation is an equation involving an unknown function $y(t)$ and its derivatives $y',y'',\\dots$, and the order of a differential equation is the highest order derivative of $y(t)$ appearing in the equation. There are methods to solve first order equations which are separable and/or linear however most differential equations cannot be solved explicitly with elementary functions. We can always use graphical methods and numerical methods to approximate solutions of any first order differential equation. import numpy as np import matplotlib.pyplot as plt Linear Equations A first order differential equation is linear if it is of the form $$ y' + p(t)y = q(t) $$ for some functions $p(t)$ and $q(t)$. For example, the equation $$ y' + y = \\cos(t) $$ is a first order linear equation. Use the method of the integrating factor to compute the general solution $$ y(t) = C e^{-t} + \\frac{\\cos(t) + \\sin(t)}{2} $$ The constant $C$ is determined by the initial value $y(0) = C + 1/2$. Plot the solution $y(t)$ over the interval $0 \\leq t \\leq 10$ for each initial value $y(0)=-3,-2,-1,0,1,2,3$. t = np.linspace(0,10,100) for y0 in range(-3,4): C = y0 - 1/2 y = C*np.exp(-t) + (np.cos(t) + np.sin(t))/2 plt.plot(t,y,'b') plt.title(\"$y' + y = \\cos(t)$\"), plt.grid(True) plt.show() Notice that all solutions in this example converge to the solution $$ y(t) = \\frac{\\cos(t) + \\sin(t)}{2} $$ as $t \\to \\infty$. Separable Equations A first order equation is separable if it is of the form $$ y' = f(t) g(y) $$ for some functions $f(t)$ and $g(y)$. For example, the equation $$ y' = -2ty^2 $$ is a first order separable equation. Note that the equation is nonlinear. Use the method of separation of variables to compute the general solution $$ y(t) = \\frac{1}{t^2 + C} $$ The constant $C$ is determined by the initial value $y(0) = 1/C$ (except $y(t) = 0$ if $y(0)=0$). Plot the solution $y(t)$ over the interval $0 \\leq t \\leq 5$ for each initial value $y(0)=1,\\dots,5$. t = np.linspace(0,5,100) for y0 in range(1,6): C = 1/y0 y = 1/(t**2 + C) plt.plot(t,y,'b') plt.title(\"$y' = -ty$\"), plt.grid(True) plt.show() Notice that all solutions in this example converge $y(t) \\to 0$ as $t \\to \\infty$. Autonomous Equations A first order equation is autonomous if it is of the form $$ y' = f(y) $$ where the right side $f(y)$ does not depend on the independent variable $t$. Note that an autonomous equation is also separable. For example, the equation $$ y' = y(1 - y) $$ is a first order autonomous equation. Compute the general solution using separation of variables $$ y(t) = \\frac{Ce^t}{1 + Ce^t} $$ The constant $C$ is determined by the initial value $y(0) = C/(1 + C)$ (except $y(t)=0$ when $y(0)=1$). Plot the solution $y(t)$ over the interval $0 \\leq t \\leq 2$ for each initial value $$ y(0)=-0.1,0.0,0.5,1.0,1.5,2.0,3.0 $$ t = np.linspace(0,2,20) for y0 in [-0.1,0.0,0.5,1.5,2,3]: C = y0/(1 - y0) y = C*np.exp(t)/(1 + C*np.exp(t)) plt.plot(t,y,'b') plt.plot([0,2],[1,1],'b') # Plot constant solution y(t)=1 plt.title(\"$y' = y(1-y)$\"), plt.grid(True) plt.show() Notice that all solutions $y(t)$ with intial value $y(0)>0$ converge $y(t) \\to 1$ as $t \\to \\infty$. Also $y(t)=0$ for all $t$ if $y(0)=0$. Finally $y(t) \\to -\\infty$ as $t \\to \\infty$ if $y(0) < 0$. Slope Fields In the examples above, we were able to find the general solution of the first order differential equation and plot the solution for different intial values. However most differential equations cannot be solved explicitly with elementary functions. So what do we do? We can always approximate solutions with numerical methods and graphical methods. The slope field of a first order differential equation $y'=f(t,y)$ is a graphical method for visualizing solutions. The idea is that an equation $y'=f(t,y)$ gives us complete information about the slope of a solution at any point even if we don't know a formula for the solution. Create a slope field by simply drawing a small line of slope $f(t,y)$ at various points $(t,y)$ in a grid in the $ty$-plane. For example, plot the slope field of $y' = \\sin(2\\pi t) - \\cos(2 \\pi y)$ in the range $-1 \\leq t \\leq 2$, $-2 \\leq y \\leq 2$ with grid step size $h=0.1$. f = lambda t,y: np.sin(2*np.pi*t) - np.cos(2*np.pi*y) h = 0.1; L = 0.5*h; t_grid = np.arange(0,2,h) y_grid = np.arange(-1,1,h) for t in t_grid: for y in y_grid: m = f(t,y) theta = np.arctan(m) plt.plot([t,t + L*np.cos(theta)], [y,y + L*np.sin(theta)],'b') plt.title(\"$y' = \\sin(2 \\pi t) - \\cos(2 \\pi t)$\") plt.grid(True), plt.xlim([0,2]), plt.ylim([-1,1]) plt.show() The slope field allows us to describe the behaviour of solutions. For example, estimate the value $y(2)$ for the unique solution $y(t)$ of the equation $y' = \\sin(2 \\pi t) - \\cos(2 \\pi t)$ satisfying the intial condition $y(0)=0$. Starting at $y(0)=0$ we trace the path through the slope field to find $y(2) \\approx -0.4$. Euler's Method The simplest numerical method for approximating solutions of differential equations is Euler's method . Consider a first order differential equation with an initial condition: $$ y' = f(t,y) \\ , \\ \\ y(t_0)=y_0 $$ The idea behind Euler's method is: Contruct the equation of the tangent line to the unknown function $y(t)$ at $t=t_0$: $$ y = y(t_0) + f(t_0,y_0)(t - t_0) $$ where $y'(t_0) = f(t_0,y_0)$ is the slope of $y(t)$ at $t=t_0$. Use the tangent line to approximate $y(t)$ at a small time step $t_1 = t_0 + h$: $$ y_1 = y_0 + f(t_0,y_0)(t_1 - t_0) $$ where $y_1 \\approx y(t_1)$. Repeat! The formula for Euler's method defines a recursive sequence: $$ y_{n+1} = y_n + f(t_n,y_n)(t_{n+1} - t_n) \\ , \\ \\ y_0 = y(t_0) $$ where $y_n \\approx y(t_n)$ for each $n$. If we choose equally spaced $t$ values then the formula becomes: $$ y_{n+1} = y_n + f(t_n,y_n)h \\ \\ , \\ \\ y_0 = y(t_0) \\ , \\ \\ t_n = t_0 + nh $$ with time step $h = t_{n+1} - t_n$. If we implement $N$ iterations of Euler's method from $t_0$ to $t_f$ then the time step is $$ h = \\frac{t_f - t_0}{N} $$ Note two very important points about Euler's method and numerical methods in general: A smaller time step $h$ reduces the error in the approximation. A smaller time step $h$ requires more computations! Implementation Write a function called odeEuler which takes 3 input parameters f , t and y0 where: f is a function which represents the right side of a first order differential equation $y' = f(t,y)$ t is a 1D NumPy array y0 is an intial value $y(t_0)=y_0$ where $t_0$ is the value t[0] The function odeEuler implements Euler's method and returns a 1D NumPy array of $y$ values (with length len(t) ) which approximates the solution $y(t)$ of the differential equation $y' = f(t,y)$, $y(t_0)=y_0$. def odeEuler(f,t,y0): y = np.zeros(len(t)) y[0] = y0 for n in range(0,len(t)-1): y[n+1] = y[n] + f(t[n],y[n])*(t[n+1] - t[n]) return y Examples Example 1 Compute the Euler's method approximation of the solution of $y' = -y,y(0)=1$ using step size $h=0.25$. Plot the approximation along with the exact solution $y(t)=e^{-t}$ on the interval $0 \\leq t \\leq 2$. f = lambda t,y: -y t0 = 0; tf = 2; h = 0.25; N = int((tf - t0)/h); t = np.linspace(t0,tf,N+1); y0 = 1; y = odeEuler(f,t,y0) plt.plot(t,y,'b.-'), plt.grid(True) t_exact = np.linspace(t0,tf,50) y_exact = np.exp(-t_exact) plt.plot(t_exact,y_exact,'r') plt.title(\"$y'=-y,y(0)=1$\"), plt.legend([\"Euler\",\"Exact\"]) plt.show() Example 2 Consider the equation $y' = 1 - y$. Plot the Euler's method approximation for different initial values along with the slope field. f = lambda t,y: 1 - y h = 0.2; L = 0.5*h; t_grid = np.arange(0,4,h) y_grid = np.arange(-1,2,h) for t in t_grid: for y in y_grid: m = f(t,y) theta = np.arctan(m) plt.plot([t,t + L*np.cos(theta)], [y,y + L*np.sin(theta)],'b') t = np.linspace(0,4,20) for y0 in range(-1,3): y = odeEuler(f,t,y0) plt.plot(t,y,'r-') plt.grid(True), plt.xlim([0,4]), plt.ylim([-1,2]) plt.title(\"$y' = 1 - y$\") plt.show() Example 3 Consider the equation $y' = \\sin(2 \\pi t) - \\cos(2 \\pi y)$. Plot the Euler's method approximation for different initial values along with the slope field. f = lambda t,y: np.sin(2*np.pi*t) - np.cos(2*np.pi*y) h = 0.1; L = 0.5*h; t_grid = np.arange(0,2,h) y_grid = np.arange(-1,1,h) for t in t_grid: for y in y_grid: m = f(t,y) theta = np.arctan(m) plt.plot([t,t + L*np.cos(theta)], [y,y + L*np.sin(theta)],'b') t = np.linspace(0,2,26) for y0 in [-0.75,-0.5,0.0,0.25,0.5]: y = odeEuler(f,t,y0) plt.plot(t,y,'r-') plt.grid(True), plt.xlim([0,2]), plt.ylim([-1,1]) plt.title(\"$y' = \\sin(2 \\pi t) - \\cos(2 \\pi t)$\") plt.show() Exercises Exercise 1. Plot the slope field of $y' = \\sin(t) + \\cos(y)$ in the range $0 \\leq t \\leq 2\\pi$, $-2 \\leq y \\leq 2$ with step size $h = 0.2$ in the grid. If $y(t)$ is the solution with intial value $y(0) = 0.5$, estimate the value $y(2\\pi)$.","title":"First Order Equations"},{"location":"differential-equations/first-order/#first-order-equations","text":"A differential equation is an equation involving an unknown function $y(t)$ and its derivatives $y',y'',\\dots$, and the order of a differential equation is the highest order derivative of $y(t)$ appearing in the equation. There are methods to solve first order equations which are separable and/or linear however most differential equations cannot be solved explicitly with elementary functions. We can always use graphical methods and numerical methods to approximate solutions of any first order differential equation. import numpy as np import matplotlib.pyplot as plt","title":"First Order Equations"},{"location":"differential-equations/first-order/#linear-equations","text":"A first order differential equation is linear if it is of the form $$ y' + p(t)y = q(t) $$ for some functions $p(t)$ and $q(t)$. For example, the equation $$ y' + y = \\cos(t) $$ is a first order linear equation. Use the method of the integrating factor to compute the general solution $$ y(t) = C e^{-t} + \\frac{\\cos(t) + \\sin(t)}{2} $$ The constant $C$ is determined by the initial value $y(0) = C + 1/2$. Plot the solution $y(t)$ over the interval $0 \\leq t \\leq 10$ for each initial value $y(0)=-3,-2,-1,0,1,2,3$. t = np.linspace(0,10,100) for y0 in range(-3,4): C = y0 - 1/2 y = C*np.exp(-t) + (np.cos(t) + np.sin(t))/2 plt.plot(t,y,'b') plt.title(\"$y' + y = \\cos(t)$\"), plt.grid(True) plt.show() Notice that all solutions in this example converge to the solution $$ y(t) = \\frac{\\cos(t) + \\sin(t)}{2} $$ as $t \\to \\infty$.","title":"Linear Equations"},{"location":"differential-equations/first-order/#separable-equations","text":"A first order equation is separable if it is of the form $$ y' = f(t) g(y) $$ for some functions $f(t)$ and $g(y)$. For example, the equation $$ y' = -2ty^2 $$ is a first order separable equation. Note that the equation is nonlinear. Use the method of separation of variables to compute the general solution $$ y(t) = \\frac{1}{t^2 + C} $$ The constant $C$ is determined by the initial value $y(0) = 1/C$ (except $y(t) = 0$ if $y(0)=0$). Plot the solution $y(t)$ over the interval $0 \\leq t \\leq 5$ for each initial value $y(0)=1,\\dots,5$. t = np.linspace(0,5,100) for y0 in range(1,6): C = 1/y0 y = 1/(t**2 + C) plt.plot(t,y,'b') plt.title(\"$y' = -ty$\"), plt.grid(True) plt.show() Notice that all solutions in this example converge $y(t) \\to 0$ as $t \\to \\infty$.","title":"Separable Equations"},{"location":"differential-equations/first-order/#autonomous-equations","text":"A first order equation is autonomous if it is of the form $$ y' = f(y) $$ where the right side $f(y)$ does not depend on the independent variable $t$. Note that an autonomous equation is also separable. For example, the equation $$ y' = y(1 - y) $$ is a first order autonomous equation. Compute the general solution using separation of variables $$ y(t) = \\frac{Ce^t}{1 + Ce^t} $$ The constant $C$ is determined by the initial value $y(0) = C/(1 + C)$ (except $y(t)=0$ when $y(0)=1$). Plot the solution $y(t)$ over the interval $0 \\leq t \\leq 2$ for each initial value $$ y(0)=-0.1,0.0,0.5,1.0,1.5,2.0,3.0 $$ t = np.linspace(0,2,20) for y0 in [-0.1,0.0,0.5,1.5,2,3]: C = y0/(1 - y0) y = C*np.exp(t)/(1 + C*np.exp(t)) plt.plot(t,y,'b') plt.plot([0,2],[1,1],'b') # Plot constant solution y(t)=1 plt.title(\"$y' = y(1-y)$\"), plt.grid(True) plt.show() Notice that all solutions $y(t)$ with intial value $y(0)>0$ converge $y(t) \\to 1$ as $t \\to \\infty$. Also $y(t)=0$ for all $t$ if $y(0)=0$. Finally $y(t) \\to -\\infty$ as $t \\to \\infty$ if $y(0) < 0$.","title":"Autonomous Equations"},{"location":"differential-equations/first-order/#slope-fields","text":"In the examples above, we were able to find the general solution of the first order differential equation and plot the solution for different intial values. However most differential equations cannot be solved explicitly with elementary functions. So what do we do? We can always approximate solutions with numerical methods and graphical methods. The slope field of a first order differential equation $y'=f(t,y)$ is a graphical method for visualizing solutions. The idea is that an equation $y'=f(t,y)$ gives us complete information about the slope of a solution at any point even if we don't know a formula for the solution. Create a slope field by simply drawing a small line of slope $f(t,y)$ at various points $(t,y)$ in a grid in the $ty$-plane. For example, plot the slope field of $y' = \\sin(2\\pi t) - \\cos(2 \\pi y)$ in the range $-1 \\leq t \\leq 2$, $-2 \\leq y \\leq 2$ with grid step size $h=0.1$. f = lambda t,y: np.sin(2*np.pi*t) - np.cos(2*np.pi*y) h = 0.1; L = 0.5*h; t_grid = np.arange(0,2,h) y_grid = np.arange(-1,1,h) for t in t_grid: for y in y_grid: m = f(t,y) theta = np.arctan(m) plt.plot([t,t + L*np.cos(theta)], [y,y + L*np.sin(theta)],'b') plt.title(\"$y' = \\sin(2 \\pi t) - \\cos(2 \\pi t)$\") plt.grid(True), plt.xlim([0,2]), plt.ylim([-1,1]) plt.show() The slope field allows us to describe the behaviour of solutions. For example, estimate the value $y(2)$ for the unique solution $y(t)$ of the equation $y' = \\sin(2 \\pi t) - \\cos(2 \\pi t)$ satisfying the intial condition $y(0)=0$. Starting at $y(0)=0$ we trace the path through the slope field to find $y(2) \\approx -0.4$.","title":"Slope Fields"},{"location":"differential-equations/first-order/#eulers-method","text":"The simplest numerical method for approximating solutions of differential equations is Euler's method . Consider a first order differential equation with an initial condition: $$ y' = f(t,y) \\ , \\ \\ y(t_0)=y_0 $$ The idea behind Euler's method is: Contruct the equation of the tangent line to the unknown function $y(t)$ at $t=t_0$: $$ y = y(t_0) + f(t_0,y_0)(t - t_0) $$ where $y'(t_0) = f(t_0,y_0)$ is the slope of $y(t)$ at $t=t_0$. Use the tangent line to approximate $y(t)$ at a small time step $t_1 = t_0 + h$: $$ y_1 = y_0 + f(t_0,y_0)(t_1 - t_0) $$ where $y_1 \\approx y(t_1)$. Repeat! The formula for Euler's method defines a recursive sequence: $$ y_{n+1} = y_n + f(t_n,y_n)(t_{n+1} - t_n) \\ , \\ \\ y_0 = y(t_0) $$ where $y_n \\approx y(t_n)$ for each $n$. If we choose equally spaced $t$ values then the formula becomes: $$ y_{n+1} = y_n + f(t_n,y_n)h \\ \\ , \\ \\ y_0 = y(t_0) \\ , \\ \\ t_n = t_0 + nh $$ with time step $h = t_{n+1} - t_n$. If we implement $N$ iterations of Euler's method from $t_0$ to $t_f$ then the time step is $$ h = \\frac{t_f - t_0}{N} $$ Note two very important points about Euler's method and numerical methods in general: A smaller time step $h$ reduces the error in the approximation. A smaller time step $h$ requires more computations!","title":"Euler's Method"},{"location":"differential-equations/first-order/#implementation","text":"Write a function called odeEuler which takes 3 input parameters f , t and y0 where: f is a function which represents the right side of a first order differential equation $y' = f(t,y)$ t is a 1D NumPy array y0 is an intial value $y(t_0)=y_0$ where $t_0$ is the value t[0] The function odeEuler implements Euler's method and returns a 1D NumPy array of $y$ values (with length len(t) ) which approximates the solution $y(t)$ of the differential equation $y' = f(t,y)$, $y(t_0)=y_0$. def odeEuler(f,t,y0): y = np.zeros(len(t)) y[0] = y0 for n in range(0,len(t)-1): y[n+1] = y[n] + f(t[n],y[n])*(t[n+1] - t[n]) return y","title":"Implementation"},{"location":"differential-equations/first-order/#examples","text":"","title":"Examples"},{"location":"differential-equations/first-order/#exercises","text":"Exercise 1. Plot the slope field of $y' = \\sin(t) + \\cos(y)$ in the range $0 \\leq t \\leq 2\\pi$, $-2 \\leq y \\leq 2$ with step size $h = 0.2$ in the grid. If $y(t)$ is the solution with intial value $y(0) = 0.5$, estimate the value $y(2\\pi)$.","title":"Exercises"},{"location":"differential-equations/numerical-methods/","text":"Numerical Methods Most differential equations cannot be solved explicitly using elementary functions however we can always approximate solutions using numerical methods . The order of a numerical method describes how much the error decreases as the step size decreases. Higher order methods are more accurate however they require more computations to implement. Euler's method is the simplest method however the Runge-Kutta method (RK4) is the most commonly used in practice. import numpy as np import matplotlib.pyplot as plt Setup Throughout this section, let $y(t)$ denote the unique solution of a first order differential equation with an initial condition: $$ y' = f(t,y) \\ , \\ \\ y(t_0) = y_0 $$ A numerical method is an algorithm which approximates the solution $y(t)$. In particular, given a sequence of values $t_1,t_2,t_3,\\dots$, a numerical method computes a sequence $y_1,y_2,y_3,\\dots$ which approximates the solution at the given $t$ values: $$ y_n \\approx y(t_n) $$ The $t$ values are usually chosen to be equally spaced with step size $h$: $$ t_n = t_0 + n h $$ All the numerical methods we consider below are examples of explicit Runge-Kutta methods which follow the same general procedure: Given a point $(t_n,y_n)$, approximate slopes $k_1,\\dots,k_s$ nearby using $f(t,y)$. Compute an average $\\tilde{k}$ of the slopes $k_1,\\dots,k_s$. Compute the next value: $y_{n+1} = y_n + \\tilde{k} h$. Repeat! A method which computes $s$ slopes $k_1,\\dots,k_s$ is called an $s$-stage method and the formula for the average $\\tilde{k}$ depends on the method. Euler's Method The simplest numerical method is Euler's method which uses the tangent line approximation: $$ y(t + h) \\approx y(t) + y'(t) h $$ Euler's method is given by the recursive formula: \\begin{align} h &= t_{n+1} - t_n \\\\ k_1 &= f(t_n,y_n) \\\\ y_{n+1} &= y_n + k_1 h \\end{align} Write a function called odeEuler which takes input parameters f , t and y0 where: f is a function which represents the right side of the equation $y' = f(t,y)$ t is a 1D NumPy array y0 is an intial value $y(t_0)=y_0$ where $t_0$ is the value t[0] The function odeEuler returns a 1D NumPy array of $y$ values which approximate the solution $y(t)$ using Euler's method. def odeEuler(f,t,y0): y = np.zeros(len(t)) y[0] = y0 for n in range(0,len(t)-1): h = t[n+1] - t[n] k1 = f(t[n],y[n]) y[n+1] = y[n] + k1*h return y Consider the equation $$ y' = y \\cos(t) \\ , \\ \\ y(0)=1 $$ The equation is separable and we solve using separation of variables: $$ y(t) = e^{\\sin(t)} $$ Plot the approximation by Euler's method with step size $h=0.25$ and plot the exact solution on the interval $0 \\leq t \\leq 2 \\pi$. f = lambda t,y: y*np.cos(t) y0 = 1; t0 = 0; tf = 2*np.pi; h = 0.25; N = int((tf - t0)/h) + 1; t = np.linspace(t0,tf,N+1) y = odeEuler(f,t,y0) plt.plot(t,y,'b.-') t_exact = np.linspace(t0,tf,50) y_exact = np.exp(np.sin(t_exact)) plt.plot(t_exact,y_exact,'r') plt.grid(True), plt.title(\"$y' = y \\, \\cos(t) \\ , \\ y(0)=1$\") plt.legend([\"Euler\",\"Exact\"]) plt.show() Heun's Method Euler's method uses the degree 1 Taylor polynomial (ie. the tangent line) to approximate $y(t)$. Heun's method is constructed from the degree 2 Taylor polynomial: $$ y(t + h) \\approx y(t) + y'(t)h + \\frac{y''(t)}{2}h^2 $$ We only want to use $y' = f(t,y)$ in our approximation therefore introduce the forward difference formula to approximate $y''$ in terms of $y'$: $$ y''(t) \\approx \\frac{y'(t + h) - y'(t)}{h} $$ Put these together to approximate $y(t + h)$: $$ y(t+h) \\approx y(t) + \\frac{y'(t + h) + y'(t)}{2}h $$ Use Euler's method $y(t+h) \\approx y(t) + y'(t)h$ to approximate: $$ y'(t+h) = f(t+h,y(t+h)) \\approx f(t+h,y(t)+y'(t)h) $$ Heun's method is given by the 2-stage recursive formula: \\begin{align} h &= t_{n+1} - t_n \\\\ k_1 &= f(t_n,y_n) \\\\ k_2 &= f(t_n + h,y_n + k_1 h) \\\\ y_{n+1} &= y_n + \\left( \\frac{k_1 + k_2}{2} \\right)h \\end{align} Write a function called odeHeun which takes input parameters f , t and y0 where: f is a function which represents the right side of the equation $y' = f(t,y)$ t is a 1D NumPy array y0 is an intial value $y(t_0)=y_0$ where $t_0$ is the value t[0] The function odeHeun returns a 1D NumPy array of $y$ values which approximate the solution $y(t)$ using Heun's method. def odeHeun(f,t,y0): y = np.zeros(len(t)) y[0] = y0 for n in range(0,len(t)-1): h = t[n+1] - t[n] k1 = f(t[n],y[n]) k2 = f(t[n+1],y[n] + k1*h) y[n+1] = y[n] + (k1 + k2)/2*h return y Let us again consider the equation $$ y' = y \\cos(t) \\ , \\ \\ y(0)=1 $$ and the exact solution $$ y(t) = e^{\\sin(t)} $$ Plot the approximation using both Euler's method and Heun's method with step size $h=0.25$ and plot the exact solution on the interval $0 \\leq t \\leq 2 \\pi$. f = lambda t,y: y*np.cos(t) y0 = 1; t0 = 0; tf = 2*np.pi; h = 0.25; N = int((tf - t0)/h) + 1; t = np.linspace(t0,tf,N+1) y_euler = odeEuler(f,t,y0); plt.plot(t,y_euler,'g.-'); y_heun = odeHeun(f,t,y0); plt.plot(t,y_heun,'b.-'); t_exact = np.linspace(t0,tf,50) y_exact = np.exp(np.sin(t_exact)) plt.plot(t_exact,y_exact,'r') plt.grid(True), plt.title(\"$y' = y \\, \\cos(t) \\ , \\ y(0)=1$\") plt.legend([\"Euler\",\"Heun\",\"Exact\"]) plt.show() Heun's method computes a better approximation compared to Euler's method as expected. RK4 Method Under construction Error Analysis Order of Accuracy Let $y_1$ be the approximation of $y(t_1)$ by one step of some numerical method using step size $h = t_1 - t_0$. The (local) truncation error (for the given differential equation and method) is $$ E(h) = | y(t_1) - y_1 | $$ The word local means we are looking at just one step of the method and the word truncation has to do with truncating the Taylor series. Most numerical methods are based on Taylor series therefore the error may be expressed in terms of Taylor's theorem. For example, consider the Taylor series up to order $p$ evaluated at $t_1 = t_0 + h$: $$ y(t_1) = y(t_0) + y'(t_0)h + \\cdots + \\frac{y^{(p)}(t_0)}{p!} h^p + \\frac{y^{(p+1)}(c)}{(p+1)!} h^{p+1} $$ for some $c \\in [t_0,t_1]$. If a numerical method computes $y(t_1)$ using the Taylor polynomial of degree $p$ then the local truncation error is $$ E(h) = | y(t_1) - y_1 | = \\frac{| y^{(p+1)}(c) |}{(p+1)!} h^{p+1} $$ Therefore we can roughly say that a numerical method is order $p$ if the local truncation error looks like $Ch^{p+1}$ for some constant $C$. More precisely, a numerical method is order $p$ if the local truncation error satisfies $$ E(h) \\leq C h^{p+1} $$ for any equation $y' = f(t,y)$, $y(t_0)=y_0$. The constant $C$ depends on $f$. Note that the order is a positive integer. It is usually quite difficult to determine the order of a numerical method given the formula. Instead, we can determine the order experimentally. The idea is that the local truncation error should satisfy $$ E(h) \\approx C h^{p+1} $$ when applied to most differential equations. Therefore we may observe the slope in the loglog plot: $$ \\log(E(h)) \\approx (p+1) \\log(h) + \\log(C) $$ The procedure to experimentally determine the order $p$ of a numerical method is: Apply the numerical method to the equation $y' = y,y(0)=1$ for different steps size $h_1$ and $h_2$. Compute the local truncation errors $E(h_1)$ and $E(h_2)$ using the exact solution $y(t)=e^t$. Compute the slope of the loglog plot: $$ p+1 \\approx \\frac{\\log(E(h_2)) - \\log(E(h_1))}{\\log(h_2) - \\log(h_1)} $$ Examples Euler's Method is Order 1 Euler's method is built using the degree 1 Taylor polynomial. Talyor's theorem says $$ y(t_1) = y(t_0) + y'(t_0)(t_1 - t_0) + \\frac{y''(c)}{2}(t_1 - t_0)^2 $$ for some $c \\in [t_0,t_1]$. Therefore, if $|y''(t)|\\leq K_2$ for all $t \\in [t_0,t_1]$, then $$ E(h) = \\left| \\frac{y''(c)}{2}(t_1 - t_0)^2 \\right| \\leq \\frac{K_2 h^2}{2} $$ Therefore Euler's method is order 1. Let's verify the order of Euler's method experimentally by plotting the local truncation error for Euler's method applied to $y'=y$, $y(0)=1$. f = lambda t,y: y y0 = 1; h = [0.1,0.05,0.01,0.005,0.001] E = np.zeros(len(h)) for n in range(0,len(h)): y = odeEuler(f,[0,h[n]],y0) y1 = y[1] y1_exact = np.exp(h[n]) E[n] = np.abs(y1_exact - y1) plt.loglog(h,E,'b.-'), plt.grid(True) plt.title(\"Euler's Method, $y'=y,y(0)=1$\") plt.xlabel(\"Step Size, $h$\"), plt.ylabel(\"Local Truncation Error, $E$\") plt.show() The loglog plot has slope 2 as expected from the error formula and we verify that Euler's method is order 1. Heun's Method is Order 2 Heun's method is built using the degree 2 Taylor polynomial therefore we expect the method to be order 2. Let's verify the order of Heun's method experimentally by plotting the local truncation error for Heun's method applied to $y'=y$, $y(0)=1$. f = lambda t,y: y y0 = 1; h = [0.1,0.05,0.01,0.005,0.001] E = np.zeros(len(h)) for n in range(0,len(h)): y = odeHeun(f,[0,h[n]],y0) y1 = y[1] y1_exact = np.exp(h[n]) E[n] = np.abs(y1_exact - y1) plt.loglog(h,E,'b.-'), plt.grid(True) plt.title(\"Heun's Method, $y'=y,y(0)=1$\") plt.xlabel(\"Step Size, $h$\"), plt.ylabel(\"Local Truncation Error, $E$\") plt.show() The loglog plot has slope 3 therefore Heun's method is order 2 as expected since Heun's method is built from the degree 2 Taylor approximation. RK4 is Order 4 Under construction scipy.integrate.odeint The main ODE solver in SciPy is scipy.integrate.odeint . Just like our functions odeEuler and others defined above, the function odeint takes input parameters f , y0 and t where: f is a function which represents the right side of the equation $y' = f(y,t)$ (note the order $y$ then $t$) y0 is an intial value $y(t_0)=y_0$ where $t_0$ is the value t[0] t is a 1D NumPy array The function odeint returns a 1D NumPy array of $y$ values which approximate the solution $y(t)$. Note that our notation for first order equations (and the notation used by most mathematicians) is $y'=f(t,y)$ with $t$ appearing first and then $y$. The function odeint expects a different order of variables with $y$ appearing first and then $t$ in the equation $y' = f(y,t)$. For example, use odeint to approximate the solution of $y' = \\sin(y^2) + \\cos(t)$ for various initial values. import scipy.integrate as spi f = lambda y,t: np.sin(y**2) + np.cos(t) t = np.linspace(0,10,200) for y0 in np.linspace(-2,2,9): y = spi.odeint(f,y0,t) plt.plot(t,y,'b') plt.grid(True), plt.title(\"$y' = \\sin(y^2) + \\cos(t)$\") plt.show() Exercises Exercise 1. Write a function called odeMidpoint which implements the midpoint method: \\begin{align} k_1 &= f(t_n,y_n) \\\\ k_2 &= f(t_n + h/2, y_n + k_1 h/2) \\\\ y_{n+1} &= y_n + k_2 h \\end{align} Determine the order of the midpoint method. Exercise 2. Use scipy.integrate.odeint to approximate $y(1)$ where $y(t)$ is the solution of the equations $$ y' = t - y^3 \\ , \\ \\ y(0)=1 $$","title":"Numerical Methods"},{"location":"differential-equations/numerical-methods/#numerical-methods","text":"Most differential equations cannot be solved explicitly using elementary functions however we can always approximate solutions using numerical methods . The order of a numerical method describes how much the error decreases as the step size decreases. Higher order methods are more accurate however they require more computations to implement. Euler's method is the simplest method however the Runge-Kutta method (RK4) is the most commonly used in practice. import numpy as np import matplotlib.pyplot as plt","title":"Numerical Methods"},{"location":"differential-equations/numerical-methods/#setup","text":"Throughout this section, let $y(t)$ denote the unique solution of a first order differential equation with an initial condition: $$ y' = f(t,y) \\ , \\ \\ y(t_0) = y_0 $$ A numerical method is an algorithm which approximates the solution $y(t)$. In particular, given a sequence of values $t_1,t_2,t_3,\\dots$, a numerical method computes a sequence $y_1,y_2,y_3,\\dots$ which approximates the solution at the given $t$ values: $$ y_n \\approx y(t_n) $$ The $t$ values are usually chosen to be equally spaced with step size $h$: $$ t_n = t_0 + n h $$ All the numerical methods we consider below are examples of explicit Runge-Kutta methods which follow the same general procedure: Given a point $(t_n,y_n)$, approximate slopes $k_1,\\dots,k_s$ nearby using $f(t,y)$. Compute an average $\\tilde{k}$ of the slopes $k_1,\\dots,k_s$. Compute the next value: $y_{n+1} = y_n + \\tilde{k} h$. Repeat! A method which computes $s$ slopes $k_1,\\dots,k_s$ is called an $s$-stage method and the formula for the average $\\tilde{k}$ depends on the method.","title":"Setup"},{"location":"differential-equations/numerical-methods/#eulers-method","text":"The simplest numerical method is Euler's method which uses the tangent line approximation: $$ y(t + h) \\approx y(t) + y'(t) h $$ Euler's method is given by the recursive formula: \\begin{align} h &= t_{n+1} - t_n \\\\ k_1 &= f(t_n,y_n) \\\\ y_{n+1} &= y_n + k_1 h \\end{align} Write a function called odeEuler which takes input parameters f , t and y0 where: f is a function which represents the right side of the equation $y' = f(t,y)$ t is a 1D NumPy array y0 is an intial value $y(t_0)=y_0$ where $t_0$ is the value t[0] The function odeEuler returns a 1D NumPy array of $y$ values which approximate the solution $y(t)$ using Euler's method. def odeEuler(f,t,y0): y = np.zeros(len(t)) y[0] = y0 for n in range(0,len(t)-1): h = t[n+1] - t[n] k1 = f(t[n],y[n]) y[n+1] = y[n] + k1*h return y Consider the equation $$ y' = y \\cos(t) \\ , \\ \\ y(0)=1 $$ The equation is separable and we solve using separation of variables: $$ y(t) = e^{\\sin(t)} $$ Plot the approximation by Euler's method with step size $h=0.25$ and plot the exact solution on the interval $0 \\leq t \\leq 2 \\pi$. f = lambda t,y: y*np.cos(t) y0 = 1; t0 = 0; tf = 2*np.pi; h = 0.25; N = int((tf - t0)/h) + 1; t = np.linspace(t0,tf,N+1) y = odeEuler(f,t,y0) plt.plot(t,y,'b.-') t_exact = np.linspace(t0,tf,50) y_exact = np.exp(np.sin(t_exact)) plt.plot(t_exact,y_exact,'r') plt.grid(True), plt.title(\"$y' = y \\, \\cos(t) \\ , \\ y(0)=1$\") plt.legend([\"Euler\",\"Exact\"]) plt.show()","title":"Euler's Method"},{"location":"differential-equations/numerical-methods/#heuns-method","text":"Euler's method uses the degree 1 Taylor polynomial (ie. the tangent line) to approximate $y(t)$. Heun's method is constructed from the degree 2 Taylor polynomial: $$ y(t + h) \\approx y(t) + y'(t)h + \\frac{y''(t)}{2}h^2 $$ We only want to use $y' = f(t,y)$ in our approximation therefore introduce the forward difference formula to approximate $y''$ in terms of $y'$: $$ y''(t) \\approx \\frac{y'(t + h) - y'(t)}{h} $$ Put these together to approximate $y(t + h)$: $$ y(t+h) \\approx y(t) + \\frac{y'(t + h) + y'(t)}{2}h $$ Use Euler's method $y(t+h) \\approx y(t) + y'(t)h$ to approximate: $$ y'(t+h) = f(t+h,y(t+h)) \\approx f(t+h,y(t)+y'(t)h) $$ Heun's method is given by the 2-stage recursive formula: \\begin{align} h &= t_{n+1} - t_n \\\\ k_1 &= f(t_n,y_n) \\\\ k_2 &= f(t_n + h,y_n + k_1 h) \\\\ y_{n+1} &= y_n + \\left( \\frac{k_1 + k_2}{2} \\right)h \\end{align} Write a function called odeHeun which takes input parameters f , t and y0 where: f is a function which represents the right side of the equation $y' = f(t,y)$ t is a 1D NumPy array y0 is an intial value $y(t_0)=y_0$ where $t_0$ is the value t[0] The function odeHeun returns a 1D NumPy array of $y$ values which approximate the solution $y(t)$ using Heun's method. def odeHeun(f,t,y0): y = np.zeros(len(t)) y[0] = y0 for n in range(0,len(t)-1): h = t[n+1] - t[n] k1 = f(t[n],y[n]) k2 = f(t[n+1],y[n] + k1*h) y[n+1] = y[n] + (k1 + k2)/2*h return y Let us again consider the equation $$ y' = y \\cos(t) \\ , \\ \\ y(0)=1 $$ and the exact solution $$ y(t) = e^{\\sin(t)} $$ Plot the approximation using both Euler's method and Heun's method with step size $h=0.25$ and plot the exact solution on the interval $0 \\leq t \\leq 2 \\pi$. f = lambda t,y: y*np.cos(t) y0 = 1; t0 = 0; tf = 2*np.pi; h = 0.25; N = int((tf - t0)/h) + 1; t = np.linspace(t0,tf,N+1) y_euler = odeEuler(f,t,y0); plt.plot(t,y_euler,'g.-'); y_heun = odeHeun(f,t,y0); plt.plot(t,y_heun,'b.-'); t_exact = np.linspace(t0,tf,50) y_exact = np.exp(np.sin(t_exact)) plt.plot(t_exact,y_exact,'r') plt.grid(True), plt.title(\"$y' = y \\, \\cos(t) \\ , \\ y(0)=1$\") plt.legend([\"Euler\",\"Heun\",\"Exact\"]) plt.show() Heun's method computes a better approximation compared to Euler's method as expected.","title":"Heun's Method"},{"location":"differential-equations/numerical-methods/#rk4-method","text":"Under construction","title":"RK4 Method"},{"location":"differential-equations/numerical-methods/#error-analysis","text":"","title":"Error Analysis"},{"location":"differential-equations/numerical-methods/#order-of-accuracy","text":"Let $y_1$ be the approximation of $y(t_1)$ by one step of some numerical method using step size $h = t_1 - t_0$. The (local) truncation error (for the given differential equation and method) is $$ E(h) = | y(t_1) - y_1 | $$ The word local means we are looking at just one step of the method and the word truncation has to do with truncating the Taylor series. Most numerical methods are based on Taylor series therefore the error may be expressed in terms of Taylor's theorem. For example, consider the Taylor series up to order $p$ evaluated at $t_1 = t_0 + h$: $$ y(t_1) = y(t_0) + y'(t_0)h + \\cdots + \\frac{y^{(p)}(t_0)}{p!} h^p + \\frac{y^{(p+1)}(c)}{(p+1)!} h^{p+1} $$ for some $c \\in [t_0,t_1]$. If a numerical method computes $y(t_1)$ using the Taylor polynomial of degree $p$ then the local truncation error is $$ E(h) = | y(t_1) - y_1 | = \\frac{| y^{(p+1)}(c) |}{(p+1)!} h^{p+1} $$ Therefore we can roughly say that a numerical method is order $p$ if the local truncation error looks like $Ch^{p+1}$ for some constant $C$. More precisely, a numerical method is order $p$ if the local truncation error satisfies $$ E(h) \\leq C h^{p+1} $$ for any equation $y' = f(t,y)$, $y(t_0)=y_0$. The constant $C$ depends on $f$. Note that the order is a positive integer. It is usually quite difficult to determine the order of a numerical method given the formula. Instead, we can determine the order experimentally. The idea is that the local truncation error should satisfy $$ E(h) \\approx C h^{p+1} $$ when applied to most differential equations. Therefore we may observe the slope in the loglog plot: $$ \\log(E(h)) \\approx (p+1) \\log(h) + \\log(C) $$ The procedure to experimentally determine the order $p$ of a numerical method is: Apply the numerical method to the equation $y' = y,y(0)=1$ for different steps size $h_1$ and $h_2$. Compute the local truncation errors $E(h_1)$ and $E(h_2)$ using the exact solution $y(t)=e^t$. Compute the slope of the loglog plot: $$ p+1 \\approx \\frac{\\log(E(h_2)) - \\log(E(h_1))}{\\log(h_2) - \\log(h_1)} $$","title":"Order of Accuracy"},{"location":"differential-equations/numerical-methods/#examples","text":"","title":"Examples"},{"location":"differential-equations/numerical-methods/#scipyintegrateodeint","text":"The main ODE solver in SciPy is scipy.integrate.odeint . Just like our functions odeEuler and others defined above, the function odeint takes input parameters f , y0 and t where: f is a function which represents the right side of the equation $y' = f(y,t)$ (note the order $y$ then $t$) y0 is an intial value $y(t_0)=y_0$ where $t_0$ is the value t[0] t is a 1D NumPy array The function odeint returns a 1D NumPy array of $y$ values which approximate the solution $y(t)$. Note that our notation for first order equations (and the notation used by most mathematicians) is $y'=f(t,y)$ with $t$ appearing first and then $y$. The function odeint expects a different order of variables with $y$ appearing first and then $t$ in the equation $y' = f(y,t)$. For example, use odeint to approximate the solution of $y' = \\sin(y^2) + \\cos(t)$ for various initial values. import scipy.integrate as spi f = lambda y,t: np.sin(y**2) + np.cos(t) t = np.linspace(0,10,200) for y0 in np.linspace(-2,2,9): y = spi.odeint(f,y0,t) plt.plot(t,y,'b') plt.grid(True), plt.title(\"$y' = \\sin(y^2) + \\cos(t)$\") plt.show()","title":"scipy.integrate.odeint"},{"location":"differential-equations/numerical-methods/#exercises","text":"Exercise 1. Write a function called odeMidpoint which implements the midpoint method: \\begin{align} k_1 &= f(t_n,y_n) \\\\ k_2 &= f(t_n + h/2, y_n + k_1 h/2) \\\\ y_{n+1} &= y_n + k_2 h \\end{align} Determine the order of the midpoint method. Exercise 2. Use scipy.integrate.odeint to approximate $y(1)$ where $y(t)$ is the solution of the equations $$ y' = t - y^3 \\ , \\ \\ y(0)=1 $$","title":"Exercises"},{"location":"differential-equations/systems/","text":"Systems of Equations A system of differential equations is a collection of equations involving unknown functions $u_0,\\dots,u_{N-1}$ and their derivatives. The dimension of a system is the number $N$ of unknown functions. The order of the system is the highest order derivative appearing in the collection of equations. Every system of differential equations is equivalent to a first order system in a higher dimension. import numpy as np import scipy.integrate as spi import matplotlib.pyplot as plt First Order Systems Every system of differential equations is equivalent to a first order system in a higher dimension. For example, consider a second order differential equation $$ ay'' + by' + cy = F(t) $$ where $a,b,c$ are constants (with $a \\ne 0$) and $F(t)$ is a known function. The unknown function $y(t)$ is second order in the equation therefore we introduce two new variables $u_0 = y$ and $u_1 = y'$ and write the 1-dimensional second order equation as a 2-dimensional first order system \\begin{align} u_0' &= u_1 \\\\ u_1' &= (F(t) - b u_1 - c u_0)/a \\end{align} The procedure for any system is similar: Identify the order of each unknown function in the system. If $y$ has order $n$ then introduce $n$ new variables $u_0 = y,u_1=y',\\dots,u_{n-1}=y^{(n-1)}$. Rewrite the equations in terms of the new variables only. For example, consider the system \\begin{align} x' &= x + y \\\\ y'' &= xy' + y \\end{align} Introduce new variables $u_0 = x, u_1 = y, u_2 = y'$ and rewrite the system \\begin{align} u_0' &= u_0 + u_1 \\\\ u_1' &= u_2 \\\\ u_2' &= u_0 u_2 + u_1 \\end{align} Vector Notation An $N$-dimensional first order system of differential equations is of the form \\begin{align} u_0' &= f_0(t,u_0,\\dots,u_{N-1}) \\\\ u_1' &= f_1(t,u_0,\\dots,u_{N-1}) \\\\ & \\ \\ \\vdots \\\\ u_{N-1}' &= f_{N-1}(t,u_0,\\dots,u_{N-1}) \\end{align} Write the system in vector notation $$ \\frac{d \\mathbf{u}}{dt} = \\mathbf{f}(t,\\mathbf{u}) $$ where $$ \\mathbf{u} = \\begin{bmatrix} u_0 \\\\ \\vdots \\\\ u_{N-1} \\end{bmatrix} \\hspace{10mm} \\frac{d \\mathbf{u}}{dt} = \\begin{bmatrix} u_0' \\\\ \\vdots \\\\ u_{N-1}' \\end{bmatrix} \\hspace{10mm} \\mathbf{f}(t,\\mathbf{u}) = \\begin{bmatrix} f_0(t,u_0,\\dots,u_{N-1}) \\\\ \\vdots \\\\ f_{N-1}(t,u_0,\\dots,u_{N-1}) \\end{bmatrix} $$ Euler's Method How do we apply Euler's method to a first order system of equations? Simply apply the method to each unknown function in the system. Second Order Equations Consider a second order differential equation with constant coefficients $$ ay'' + by' + cy = F(t) \\ , \\ y(0)=y_0 \\ , \\ y'(0)=v_0 $$ Apply Euler's method to both $y$ and $y'$ simultaneously: \\begin{align} y_{n+1} &= y_n + y_n' h \\\\ y_{n+1}' &= y_n' + y_n'' h \\end{align} where $$ y_n'' = (F(t_n) - cy_n - by_n')/a $$ For example, consider the equation $y'' + y = 0$, $y(0)=1$, $y'(0)=0$. We know the exact solution is $y(t) = \\cos(t)$. Compute the approximation by Euler's method and compare with the exact solution. y0 = 1; v0 = 0; t = np.linspace(0,2*np.pi,100) y = np.zeros(len(t)) y[0] = y0 dy = np.zeros(len(t)) dy[0] = v0 for n in range(0,len(t)-1): h = t[n+1] - t[n] y[n+1] = y[n] + dy[n]*h dy[n+1] = dy[n] - y[n]*h plt.plot(t,y,'b',t,np.cos(t),'r'), plt.grid(True) plt.title(\"$y'' + y = 0 , y(0) = 1 , y'(0) = 1$\") plt.legend([\"Euler\",\"Exact\"]) plt.show() Implementation Consider a first order system in vector notation $$ \\frac{d \\mathbf{u}}{dt} = \\mathbf{f}(t,\\mathbf{u}) $$ The formula for Euler's method is almost exactly the same as for scalar equations $$ \\mathbf{u}_{n+1} = \\mathbf{u}_n + \\mathbf{f}(t_n,\\mathbf{u}_n) h $$ where $\\mathbf{u}_n$ is the vector of values at step $n$ $$ \\begin{bmatrix} u_{0,n} \\\\ \\vdots \\\\ u_{N-1,n} \\end{bmatrix} $$ such that $u_{k,n} \\approx u_k(t_n)$ at step $n$ for $k=0,\\dots,N-1$. Write a function called odeEuler which takes input parameters f , t and u0 where: f is a function which represents the right side of the equation $\\mathbf{u}' = \\mathbf{f}(t,\\mathbf{u})$ t is a 1D NumPy array u0 is an intial value $\\mathbf{u}(t_0)=\\mathbf{u}_0$ where $t_0$ is the value t[0] The function odeEuler returns a 2D NumPy array U of size (len(t),len(u0)) with values $u_k(t_n)$ in column $k$ and row $n$. def odeEuler(f,t,u0): U = np.zeros((len(t),len(u0))) U[0,:] = u0 for n in range(0,len(t)-1): h = t[n+1] - t[n] k1 = f(t[n],U[n,:]) U[n+1,:] = U[n,:] + k1*h return U Test the function odeEuler on the example $y'' + y = 0, y(0)=1, y'(0)=0$ and compare to the exact solution $y(t) = \\cos(t)$. def f(t,u): dudt = np.zeros(2) dudt[0] = u[1] dudt[1] = -u[0] return dudt t = np.linspace(0,2*np.pi,100) u0 = [1,0] U = odeEuler(f,t,u0) plt.plot(t,U[:,0],'b',t,np.cos(t),'r'), plt.grid(True) plt.title(\"$y'' + y = 0 , y(0) = 1 , y'(0) = 1$\") plt.legend([\"Euler\",\"Exact\"]) plt.show() scipy.integrate.odeint The function scipy.integrate.odeint works the same way as our funciton odeEuler above except the order of $t$ and $\\mathbf{u}$ is reversed $$ \\frac{d \\mathbf{u}}{dt} = \\mathbf{f}(\\mathbf{u},t) $$ For example, use odeint to approximate the solution of the second order equation $$ y'' + y = 0 \\ , \\ \\ y(0)=1 \\ , \\ \\ y'(0)=0 $$ and compare to the exact solution. The result is much more accurate than Euler's method! def f(u,t): dudt = np.zeros(2) dudt[0] = u[1] dudt[1] = -u[0] return dudt t = np.linspace(0,2*np.pi,50) u0 = [1,0] U = spi.odeint(f,u0,t) plt.plot(t,U[:,0],'b.',t,np.cos(t),'r'), plt.grid(True) plt.title(\"$y'' + y = 0 , y(0) = 1 , y'(0) = 1$\") plt.legend([\"odeint\",\"Exact\"]) plt.show() Applications Mass-Spring-Damper System A mass-spring-damper system is a second order equation with constant coefficients: $$ my'' + cy' + ky = F(t) $$ where $m > 0, c \\geq 0, k \\geq 0$, and $F(t)$ is the forcing function. Write the equation as a 2-dimensional first order system by introducing new variables $u_0 = y$ and $u_1 = y'$: \\begin{align} u'_0 &= u_1 \\\\ u'_1 &= (F(t)-ku_0-cu_1)/m \\end{align} Write a function called mass_spring_damper which takes input parameters m , c , k , F , u0 and t where m , c , k are coefficients of the system F is the forcing function $F(t)$ u0 is the vector of initial conditions u0 = [y(0), y'(0)] t is a 1D NumPy array The function returns an array y of values approximating the solution $y(t)$ at the $t$ values given by t . def mass_spring_damper(m,c,k,F,u0,t): def f(u,t): dudt = np.zeros(2) dudt[0] = u[1] dudt[1] = (F(t) - k*u[0] - c*u[1])/m return dudt U = spi.odeint(f,u0,t) y = U[:,0] return y If $m=1$, $c=0$, $k=1$, $f(t)=0$, $y(0)=0$, $y'(0)=1$ then the equation is $$ y'' + y = 0 $$ for $y(0)=0$, $y'(0)=1$ and the solution is $$ y=\\sin(t) $$ m = 1; c = 0; k = 1; F = lambda t: 0 t = np.linspace(0,4*np.pi,100) u0 = [0,1] y = mass_spring_damper(m,c,k,F,u0,t) plt.plot(t,y), plt.grid(True) plt.show() Resonance occurs when there's no damping $c=0$ and the forcing frequency is equal to the natural frequency $\\omega_n = \\sqrt{k/m}$. m = 1; c = 0; k = 10; F = lambda t: np.sin(10**0.5*t) t = np.linspace(0,50,1000) u0 = [0,0] y = mass_spring_damper(m,c,k,F,u0,t) plt.plot(t,y), plt.grid(True) plt.show() Beats describes the behaviour of an undamped system $c=0$ when the forcing frequency is near but not equal to the natural frequency $\\omega_n = \\sqrt{k/m}$. m = 1; c = 0; k = 10; F = lambda t: np.sin(3*t) t = np.linspace(0,100,1000) u0 = [0,0] y = mass_spring_damper(m,c,k,F,u0,t) plt.plot(t,y), plt.grid(True) plt.show() In a damped system $c \\not= 0$ there cannot be resonance however there is a forcing frequency which produces the largest steady state amplitude. This frequency is called the damped natural frequency (or practical resonance ). m = 1; c = 2; k = 10; F = lambda t: np.sin(2.7*t) t = np.linspace(0,15,200) u0 = [0,0] y = mass_spring_damper(m,c,k,F,u0,t) plt.plot(t,y), plt.grid(True) plt.show() Van der Pol Oscillator The Van der pol oscillator equation is given by $$ x'' - \\mu(1 - x^2)x' + x = 0 $$ Write a function called van_der_pol which takes parameters mu , u0 and t and returns a 2D NumPy array of size (len(t),2) with $x$ values in the first column and $x'$ values in the second column for the solution with initial values u0 = [x(0),x'(0)] . def van_der_pol(mu,u0,t): def f(u,t): dudt = np.zeros(2) dudt[0] = u[1] dudt[1] = mu*(1 - u[0]**2)*u[1] - u[0] return dudt U = spi.odeint(f,u0,t) x = U[:,0] dxdt = U[:,1] return np.column_stack((x,dxdt)) Plot $x$ versus $x'$ for the solution with $\\mu = 1$, $x(0) = 1$ and $x'(0)=4$. U = van_der_pol(1,[1,4],np.linspace(0,20,500)) plt.plot(U[:,0],U[:,1]), plt.grid(True) plt.show() Plot $x$ versus $x'$ for $x(0) = 1$ and $x'(0)=4$ for each value $\\mu = 1,2,3,4$. for mu in [1,2,3,4]: U = van_der_pol(mu,[1,4],np.linspace(0,10,500)) plt.plot(U[:,0],U[:,1]) plt.title(\"Van der pol equation\") plt.grid(True), plt.legend(['$\\mu=1$','$\\mu=2$','$\\mu=3$','$\\mu=4$']) plt.show() Planetary Orbits Given an object with mass $M$, the magnitude of the force of gravity it exerts on another object with mass $m$ is $$ F = \\frac{GMm}{d^2} $$ where $d$ is the distance between the objects. Let's consider the trajectory of a planet of mass $m$ as it orbits a star of mass $M$ (fixed in space). Let's use the following units: solar mass (multiples of the mass of Earth's Sun) years astronomical units (approximately the distance from the Earth to the Sun) With these units, the gravitational constant $G$ is $4 \\pi^2$. Let $\\mathbf{p} = (x,y)$ be the position of the orbiting planet with the star fixed at the origin $(0,0)$, and let $m_P$ be the mass of the planet and let $m_S$ be the mass of the star. Starting with Newton's Second Law , we have $$ m_P \\frac{d^2 \\mathbf{p}}{dt^2} = \\mathbf{F} $$ where $\\mathbf{F}$ is the vector of total force acting on the planet (which is only due to gravity in this case). Newton's law of gravity states $$ \\mathbf{F} = - \\frac{G m_S m_P}{ || \\mathbf{p} ||^2 } \\frac{\\mathbf{p}}{ || \\mathbf{p} || } $$ where $|| \\mathbf{p} || = \\sqrt{x^2 + y^2}$ is distance between the star and the planet, and $-\\mathbf{p} / || \\mathbf{p} ||$ is the unit vector which points in the direction from the planet to the star. Putting these equations together, we obtain a 2-dimensional system of second-order differential equations \\begin{align} \\frac{d^2 x}{dt^2} &= - \\frac{G m_S x }{ (x^2 + y^2)^{3/2} } \\\\ \\frac{d^2 y}{dt^2} &= - \\frac{G m_S y }{ (x^2 + y^2)^{3/2} } \\end{align} Write the system as a first order system by introducing new variables: $u_0 = x$, $u_1 = x'$, $u_2 = y$ and $u_3 = y'$. The system becomes \\begin{align} u_0' &= u_1 \\\\ u_1' &= - \\frac{G m_S u_0 }{ (u_0^2 + u_2^2)^{3/2} } \\\\ u_2' &= u_3 \\\\ u_3' &= - \\frac{G m_S u_2 }{ (u_0^2 + u_2^2)^{3/2} } \\end{align} Plot the orbit for $m_S = 1$ with initial conditions $x(0) = 1, x'(0) = 0, y(0) = 0, y'(0) = 7$. G = 4*np.pi**2; mS = 1; def f(u,t): dudt = np.zeros(4) dudt[0] = u[1] dudt[1] = -G*mS*u[0]/(u[0]**2 + u[2]**2)**(3/2) dudt[2] = u[3] dudt[3] = -G*mS*u[2]/(u[0]**2 + u[2]**2)**(3/2) return dudt u0 = [1,0,0,7] t = np.linspace(0,2,1000) U = spi.odeint(f,u0,t) plt.plot(U[:,0],U[:,2],0,0,'r*') plt.axis('equal'), plt.grid(True) plt.show() Euler's 3-Body Problem Euler's 3-body problem is a simplified (and admittedly physically impossible) version of the general 3-body problem . Euler's problem considers two stars fixed in space and a planet orbiting the stars in 2 dimensions. We will derive the equations of motion of the planet and then plot trajectories using SciPy's ODE solver odeint . Use the following units: solar mass (multiples of the mass of Earth's Sun) years astronomical units (AU) (approximately the distance from the Earth to the Sun) With these units, the gravitational constant is $G = 4 \\pi^2$. Introduce variables for the planet and the stars: $m_{S_1}$ - mass of star 1 $m_{S_2}$ - mass of star 2 $m_P$ - mass of the planet $x_{S_1}$ - (fixed) $x$-position of star 1 $y_{S_1}$ - (fixed) $y$-position of star 1 $x_{S_2}$ - (fixed) $x$-position of star 2 $y_{S_2}$ - (fixed) $x$-position of star 2 $x_P$ - $x$-position of the planet $y_P$ - $y$-position of the planet $\\mathbf{p} = (x_P,y_P)$ - position vector of the planet Let $\\mathbf{F_1}$ be the force of gravity of star 1 acting on the planet, and let $\\mathbf{F_2}$ be the force of gravity of star 2 acting on the planet. Newton's Law of Gravity states: $$ \\mathbf{F_1} = - \\frac{ G m_P m_{S_1} }{ || \\mathbf{d_1} ||^2} \\frac{ \\mathbf{d_1} }{ || \\mathbf{d_1} || } $$ $$ \\mathbf{F_2} = - \\frac{ G m_P m_{S_2} }{ || \\mathbf{d_2} ||^2} \\frac{ \\mathbf{d_2} }{ || \\mathbf{d_2} || } $$ where $\\mathbf{d_1} = (x_P-x_{S_1},y_P-y_{S_1})$ is the vector from star 1 to the planet, and $\\mathbf{d_2} = (x_P-x_{S_2},y_P-y_{S_2})$ is the vector from star 2 to the planet. Newton's Second Law of Motion states: $$ \\frac{ d^2 \\mathbf{x} }{ dt^2 } = \\mathbf{F}_1 + \\mathbf{F}_2 $$ and this leads us to the system of second order ODEs which govern the motion of the planet: $$ \\frac{d^2 x_P}{dt^2} = - \\frac{ G m_{S_1} (x_P - x_{S_1}) }{ || \\mathbf{d_1} ||^3} - \\frac{ G m_{S_2} (x_P - x_{S_2}) }{ || \\mathbf{d_2} ||^3} $$ $$ \\frac{d^2 y_P}{dt^2} = - \\frac{ G m_{S_1} (y_P - y_{S_1}) }{ || \\mathbf{d_1} ||^3} - \\frac{ G m_{S_2} (y_P - y_{S_2}) }{ || \\mathbf{d_2} ||^3} $$ To plot trajectories of the planet using odeint , we first need to write the system as a first order system. Introduce new variables $u_1 = x_P$, $u_2 = x_P'$, $u_3 = y_P$ and $u_4 = y_P'$ and write \\begin{align} u_1' &= u_2 \\\\ u_2' &= - \\frac{ G m_{S_1} (u_1 - x_{S_1}) }{ || \\mathbf{d_1} ||^3} - \\frac{ G m_{S_2} (u_1 - x_{S_2}) }{ || \\mathbf{d_2} ||^3} \\\\ u_3' &= u_4 \\\\ u_4' &= - \\frac{ G m_{S_1} (u_3 - y_{S_1}) }{ || \\mathbf{d_1} ||^3} - \\frac{ G m_{S_2} (u_3 - y_{S_2}) }{ || \\mathbf{d_2} ||^3} \\end{align} where $\\mathbf{d_1} = (u_1-x_{S_1},u_3-y_{S_1})$ and $\\mathbf{d_2} = (u_1-x_{S_2},u_3-y_{S_2})$. G = 4*np.pi**2 # Gravitational constant S1 = [-2,0] # Coordinates of Star 1 S2 = [2,0] # Coordinates of Star 2 M1 = 2 # Mass of Star 1 (in solar mass) M2 = 1 # Mass of Star 2 (in solar mass) def f(u,t): d1 = np.linalg.norm([u[0] - S1[0],u[2] - S1[1]]) d2 = np.linalg.norm([u[0] - S2[0],u[2] - S2[1]]) dudt = [0,0,0,0] dudt[0] = u[1] dudt[1] = -G*M1*(u[0] - S1[0])/d1**3 - G*M2*(u[0] - S2[0])/d2**3 dudt[2] = u[3] dudt[3] = -G*M1*(u[2] - S1[1])/d1**3 - G*M2*(u[2] - S2[1])/d2**3 return dudt u0 = [0,5,3,0] # Initial conditions of the planet: [xposition,xvelocity,yposition,yvelocity] t = np.linspace(0,30,2000) # Array of time values (in years) u = spi.odeint(f,u0,t) # Solve system: u = [xposition,xvelocity,yposition,yvelocity] plt.plot(u[:,0],u[:,2]) # Plot trajectory of the planet plt.plot(S1[0],S1[1],'ro',markersize=5*M1) # Plot Star 1 as a red star plt.plot(S2[0],S2[1],'ro',markersize=5*M2) # Plot Star 2 as a red star plt.axis('equal') plt.show() Let's put the code above into a function that we can call with different initial conditions to see what kinds of orbits we can create! def euler3body(S1,S2,M1,M2,u0,tf,numpoints=1000): # Define the vector function on the right side of the system of the equations def f(u,t): d1 = np.linalg.norm([u[0]-S1[0],u[2]-S1[1]]) d2 = np.linalg.norm([u[0]-S2[0],u[2]-S2[1]]) dudt = [0,0,0,0] dudt[0] = u[1] dudt[1] = -G*M1*(u[0]-S1[0])/d1**3 - G*M2*(u[0]-S2[0])/d2**3 dudt[2] = u[3] dudt[3] = -G*M1*(u[2]-S1[1])/d1**3 - G*M2*(u[2]-S2[1])/d2**3 return dudt t = np.linspace(0,tf,numpoints) # Array of time values (in years) u = spi.odeint(f,u0,t) # Solve system: u = [xposition,xvelocity,yposition,yvelocity] plt.plot(u[:,0],u[:,2]) # Plot trajectory of the planet plt.plot(S1[0],S1[1],'ro',markersize=5*M1) # Plot Star 1 as a red star plt.plot(S2[0],S2[1],'ro',markersize=5*M2) # Plot Star 2 as a red star plt.axis('equal') plt.show() euler3body([-1,0],[1,0],2,1,[0,5,3,0],30) euler3body([-1,1],[1,-1],2,1,[0,10,0,5],30) euler3body([-2,0],[2,0],1,1,[0,0,0,5],5) euler3body([-2,0],[2,0],1.5,2.5,[0,4.8,3,0],20) euler3body([0,10],[0,-10],4.9,4.9,[0,np.pi,0,np.pi],100) Exercises Under construction","title":"Systems of Equations"},{"location":"differential-equations/systems/#systems-of-equations","text":"A system of differential equations is a collection of equations involving unknown functions $u_0,\\dots,u_{N-1}$ and their derivatives. The dimension of a system is the number $N$ of unknown functions. The order of the system is the highest order derivative appearing in the collection of equations. Every system of differential equations is equivalent to a first order system in a higher dimension. import numpy as np import scipy.integrate as spi import matplotlib.pyplot as plt","title":"Systems of Equations"},{"location":"differential-equations/systems/#first-order-systems","text":"Every system of differential equations is equivalent to a first order system in a higher dimension. For example, consider a second order differential equation $$ ay'' + by' + cy = F(t) $$ where $a,b,c$ are constants (with $a \\ne 0$) and $F(t)$ is a known function. The unknown function $y(t)$ is second order in the equation therefore we introduce two new variables $u_0 = y$ and $u_1 = y'$ and write the 1-dimensional second order equation as a 2-dimensional first order system \\begin{align} u_0' &= u_1 \\\\ u_1' &= (F(t) - b u_1 - c u_0)/a \\end{align} The procedure for any system is similar: Identify the order of each unknown function in the system. If $y$ has order $n$ then introduce $n$ new variables $u_0 = y,u_1=y',\\dots,u_{n-1}=y^{(n-1)}$. Rewrite the equations in terms of the new variables only. For example, consider the system \\begin{align} x' &= x + y \\\\ y'' &= xy' + y \\end{align} Introduce new variables $u_0 = x, u_1 = y, u_2 = y'$ and rewrite the system \\begin{align} u_0' &= u_0 + u_1 \\\\ u_1' &= u_2 \\\\ u_2' &= u_0 u_2 + u_1 \\end{align}","title":"First Order Systems"},{"location":"differential-equations/systems/#vector-notation","text":"An $N$-dimensional first order system of differential equations is of the form \\begin{align} u_0' &= f_0(t,u_0,\\dots,u_{N-1}) \\\\ u_1' &= f_1(t,u_0,\\dots,u_{N-1}) \\\\ & \\ \\ \\vdots \\\\ u_{N-1}' &= f_{N-1}(t,u_0,\\dots,u_{N-1}) \\end{align} Write the system in vector notation $$ \\frac{d \\mathbf{u}}{dt} = \\mathbf{f}(t,\\mathbf{u}) $$ where $$ \\mathbf{u} = \\begin{bmatrix} u_0 \\\\ \\vdots \\\\ u_{N-1} \\end{bmatrix} \\hspace{10mm} \\frac{d \\mathbf{u}}{dt} = \\begin{bmatrix} u_0' \\\\ \\vdots \\\\ u_{N-1}' \\end{bmatrix} \\hspace{10mm} \\mathbf{f}(t,\\mathbf{u}) = \\begin{bmatrix} f_0(t,u_0,\\dots,u_{N-1}) \\\\ \\vdots \\\\ f_{N-1}(t,u_0,\\dots,u_{N-1}) \\end{bmatrix} $$","title":"Vector Notation"},{"location":"differential-equations/systems/#eulers-method","text":"How do we apply Euler's method to a first order system of equations? Simply apply the method to each unknown function in the system.","title":"Euler's Method"},{"location":"differential-equations/systems/#second-order-equations","text":"Consider a second order differential equation with constant coefficients $$ ay'' + by' + cy = F(t) \\ , \\ y(0)=y_0 \\ , \\ y'(0)=v_0 $$ Apply Euler's method to both $y$ and $y'$ simultaneously: \\begin{align} y_{n+1} &= y_n + y_n' h \\\\ y_{n+1}' &= y_n' + y_n'' h \\end{align} where $$ y_n'' = (F(t_n) - cy_n - by_n')/a $$ For example, consider the equation $y'' + y = 0$, $y(0)=1$, $y'(0)=0$. We know the exact solution is $y(t) = \\cos(t)$. Compute the approximation by Euler's method and compare with the exact solution. y0 = 1; v0 = 0; t = np.linspace(0,2*np.pi,100) y = np.zeros(len(t)) y[0] = y0 dy = np.zeros(len(t)) dy[0] = v0 for n in range(0,len(t)-1): h = t[n+1] - t[n] y[n+1] = y[n] + dy[n]*h dy[n+1] = dy[n] - y[n]*h plt.plot(t,y,'b',t,np.cos(t),'r'), plt.grid(True) plt.title(\"$y'' + y = 0 , y(0) = 1 , y'(0) = 1$\") plt.legend([\"Euler\",\"Exact\"]) plt.show()","title":"Second Order Equations"},{"location":"differential-equations/systems/#implementation","text":"Consider a first order system in vector notation $$ \\frac{d \\mathbf{u}}{dt} = \\mathbf{f}(t,\\mathbf{u}) $$ The formula for Euler's method is almost exactly the same as for scalar equations $$ \\mathbf{u}_{n+1} = \\mathbf{u}_n + \\mathbf{f}(t_n,\\mathbf{u}_n) h $$ where $\\mathbf{u}_n$ is the vector of values at step $n$ $$ \\begin{bmatrix} u_{0,n} \\\\ \\vdots \\\\ u_{N-1,n} \\end{bmatrix} $$ such that $u_{k,n} \\approx u_k(t_n)$ at step $n$ for $k=0,\\dots,N-1$. Write a function called odeEuler which takes input parameters f , t and u0 where: f is a function which represents the right side of the equation $\\mathbf{u}' = \\mathbf{f}(t,\\mathbf{u})$ t is a 1D NumPy array u0 is an intial value $\\mathbf{u}(t_0)=\\mathbf{u}_0$ where $t_0$ is the value t[0] The function odeEuler returns a 2D NumPy array U of size (len(t),len(u0)) with values $u_k(t_n)$ in column $k$ and row $n$. def odeEuler(f,t,u0): U = np.zeros((len(t),len(u0))) U[0,:] = u0 for n in range(0,len(t)-1): h = t[n+1] - t[n] k1 = f(t[n],U[n,:]) U[n+1,:] = U[n,:] + k1*h return U Test the function odeEuler on the example $y'' + y = 0, y(0)=1, y'(0)=0$ and compare to the exact solution $y(t) = \\cos(t)$. def f(t,u): dudt = np.zeros(2) dudt[0] = u[1] dudt[1] = -u[0] return dudt t = np.linspace(0,2*np.pi,100) u0 = [1,0] U = odeEuler(f,t,u0) plt.plot(t,U[:,0],'b',t,np.cos(t),'r'), plt.grid(True) plt.title(\"$y'' + y = 0 , y(0) = 1 , y'(0) = 1$\") plt.legend([\"Euler\",\"Exact\"]) plt.show()","title":"Implementation"},{"location":"differential-equations/systems/#scipyintegrateodeint","text":"The function scipy.integrate.odeint works the same way as our funciton odeEuler above except the order of $t$ and $\\mathbf{u}$ is reversed $$ \\frac{d \\mathbf{u}}{dt} = \\mathbf{f}(\\mathbf{u},t) $$ For example, use odeint to approximate the solution of the second order equation $$ y'' + y = 0 \\ , \\ \\ y(0)=1 \\ , \\ \\ y'(0)=0 $$ and compare to the exact solution. The result is much more accurate than Euler's method! def f(u,t): dudt = np.zeros(2) dudt[0] = u[1] dudt[1] = -u[0] return dudt t = np.linspace(0,2*np.pi,50) u0 = [1,0] U = spi.odeint(f,u0,t) plt.plot(t,U[:,0],'b.',t,np.cos(t),'r'), plt.grid(True) plt.title(\"$y'' + y = 0 , y(0) = 1 , y'(0) = 1$\") plt.legend([\"odeint\",\"Exact\"]) plt.show()","title":"scipy.integrate.odeint"},{"location":"differential-equations/systems/#applications","text":"","title":"Applications"},{"location":"differential-equations/systems/#mass-spring-damper-system","text":"A mass-spring-damper system is a second order equation with constant coefficients: $$ my'' + cy' + ky = F(t) $$ where $m > 0, c \\geq 0, k \\geq 0$, and $F(t)$ is the forcing function. Write the equation as a 2-dimensional first order system by introducing new variables $u_0 = y$ and $u_1 = y'$: \\begin{align} u'_0 &= u_1 \\\\ u'_1 &= (F(t)-ku_0-cu_1)/m \\end{align} Write a function called mass_spring_damper which takes input parameters m , c , k , F , u0 and t where m , c , k are coefficients of the system F is the forcing function $F(t)$ u0 is the vector of initial conditions u0 = [y(0), y'(0)] t is a 1D NumPy array The function returns an array y of values approximating the solution $y(t)$ at the $t$ values given by t . def mass_spring_damper(m,c,k,F,u0,t): def f(u,t): dudt = np.zeros(2) dudt[0] = u[1] dudt[1] = (F(t) - k*u[0] - c*u[1])/m return dudt U = spi.odeint(f,u0,t) y = U[:,0] return y If $m=1$, $c=0$, $k=1$, $f(t)=0$, $y(0)=0$, $y'(0)=1$ then the equation is $$ y'' + y = 0 $$ for $y(0)=0$, $y'(0)=1$ and the solution is $$ y=\\sin(t) $$ m = 1; c = 0; k = 1; F = lambda t: 0 t = np.linspace(0,4*np.pi,100) u0 = [0,1] y = mass_spring_damper(m,c,k,F,u0,t) plt.plot(t,y), plt.grid(True) plt.show() Resonance occurs when there's no damping $c=0$ and the forcing frequency is equal to the natural frequency $\\omega_n = \\sqrt{k/m}$. m = 1; c = 0; k = 10; F = lambda t: np.sin(10**0.5*t) t = np.linspace(0,50,1000) u0 = [0,0] y = mass_spring_damper(m,c,k,F,u0,t) plt.plot(t,y), plt.grid(True) plt.show() Beats describes the behaviour of an undamped system $c=0$ when the forcing frequency is near but not equal to the natural frequency $\\omega_n = \\sqrt{k/m}$. m = 1; c = 0; k = 10; F = lambda t: np.sin(3*t) t = np.linspace(0,100,1000) u0 = [0,0] y = mass_spring_damper(m,c,k,F,u0,t) plt.plot(t,y), plt.grid(True) plt.show() In a damped system $c \\not= 0$ there cannot be resonance however there is a forcing frequency which produces the largest steady state amplitude. This frequency is called the damped natural frequency (or practical resonance ). m = 1; c = 2; k = 10; F = lambda t: np.sin(2.7*t) t = np.linspace(0,15,200) u0 = [0,0] y = mass_spring_damper(m,c,k,F,u0,t) plt.plot(t,y), plt.grid(True) plt.show()","title":"Mass-Spring-Damper System"},{"location":"differential-equations/systems/#van-der-pol-oscillator","text":"The Van der pol oscillator equation is given by $$ x'' - \\mu(1 - x^2)x' + x = 0 $$ Write a function called van_der_pol which takes parameters mu , u0 and t and returns a 2D NumPy array of size (len(t),2) with $x$ values in the first column and $x'$ values in the second column for the solution with initial values u0 = [x(0),x'(0)] . def van_der_pol(mu,u0,t): def f(u,t): dudt = np.zeros(2) dudt[0] = u[1] dudt[1] = mu*(1 - u[0]**2)*u[1] - u[0] return dudt U = spi.odeint(f,u0,t) x = U[:,0] dxdt = U[:,1] return np.column_stack((x,dxdt)) Plot $x$ versus $x'$ for the solution with $\\mu = 1$, $x(0) = 1$ and $x'(0)=4$. U = van_der_pol(1,[1,4],np.linspace(0,20,500)) plt.plot(U[:,0],U[:,1]), plt.grid(True) plt.show() Plot $x$ versus $x'$ for $x(0) = 1$ and $x'(0)=4$ for each value $\\mu = 1,2,3,4$. for mu in [1,2,3,4]: U = van_der_pol(mu,[1,4],np.linspace(0,10,500)) plt.plot(U[:,0],U[:,1]) plt.title(\"Van der pol equation\") plt.grid(True), plt.legend(['$\\mu=1$','$\\mu=2$','$\\mu=3$','$\\mu=4$']) plt.show()","title":"Van der Pol Oscillator"},{"location":"differential-equations/systems/#planetary-orbits","text":"Given an object with mass $M$, the magnitude of the force of gravity it exerts on another object with mass $m$ is $$ F = \\frac{GMm}{d^2} $$ where $d$ is the distance between the objects. Let's consider the trajectory of a planet of mass $m$ as it orbits a star of mass $M$ (fixed in space). Let's use the following units: solar mass (multiples of the mass of Earth's Sun) years astronomical units (approximately the distance from the Earth to the Sun) With these units, the gravitational constant $G$ is $4 \\pi^2$. Let $\\mathbf{p} = (x,y)$ be the position of the orbiting planet with the star fixed at the origin $(0,0)$, and let $m_P$ be the mass of the planet and let $m_S$ be the mass of the star. Starting with Newton's Second Law , we have $$ m_P \\frac{d^2 \\mathbf{p}}{dt^2} = \\mathbf{F} $$ where $\\mathbf{F}$ is the vector of total force acting on the planet (which is only due to gravity in this case). Newton's law of gravity states $$ \\mathbf{F} = - \\frac{G m_S m_P}{ || \\mathbf{p} ||^2 } \\frac{\\mathbf{p}}{ || \\mathbf{p} || } $$ where $|| \\mathbf{p} || = \\sqrt{x^2 + y^2}$ is distance between the star and the planet, and $-\\mathbf{p} / || \\mathbf{p} ||$ is the unit vector which points in the direction from the planet to the star. Putting these equations together, we obtain a 2-dimensional system of second-order differential equations \\begin{align} \\frac{d^2 x}{dt^2} &= - \\frac{G m_S x }{ (x^2 + y^2)^{3/2} } \\\\ \\frac{d^2 y}{dt^2} &= - \\frac{G m_S y }{ (x^2 + y^2)^{3/2} } \\end{align} Write the system as a first order system by introducing new variables: $u_0 = x$, $u_1 = x'$, $u_2 = y$ and $u_3 = y'$. The system becomes \\begin{align} u_0' &= u_1 \\\\ u_1' &= - \\frac{G m_S u_0 }{ (u_0^2 + u_2^2)^{3/2} } \\\\ u_2' &= u_3 \\\\ u_3' &= - \\frac{G m_S u_2 }{ (u_0^2 + u_2^2)^{3/2} } \\end{align} Plot the orbit for $m_S = 1$ with initial conditions $x(0) = 1, x'(0) = 0, y(0) = 0, y'(0) = 7$. G = 4*np.pi**2; mS = 1; def f(u,t): dudt = np.zeros(4) dudt[0] = u[1] dudt[1] = -G*mS*u[0]/(u[0]**2 + u[2]**2)**(3/2) dudt[2] = u[3] dudt[3] = -G*mS*u[2]/(u[0]**2 + u[2]**2)**(3/2) return dudt u0 = [1,0,0,7] t = np.linspace(0,2,1000) U = spi.odeint(f,u0,t) plt.plot(U[:,0],U[:,2],0,0,'r*') plt.axis('equal'), plt.grid(True) plt.show()","title":"Planetary Orbits"},{"location":"differential-equations/systems/#eulers-3-body-problem","text":"Euler's 3-body problem is a simplified (and admittedly physically impossible) version of the general 3-body problem . Euler's problem considers two stars fixed in space and a planet orbiting the stars in 2 dimensions. We will derive the equations of motion of the planet and then plot trajectories using SciPy's ODE solver odeint . Use the following units: solar mass (multiples of the mass of Earth's Sun) years astronomical units (AU) (approximately the distance from the Earth to the Sun) With these units, the gravitational constant is $G = 4 \\pi^2$. Introduce variables for the planet and the stars: $m_{S_1}$ - mass of star 1 $m_{S_2}$ - mass of star 2 $m_P$ - mass of the planet $x_{S_1}$ - (fixed) $x$-position of star 1 $y_{S_1}$ - (fixed) $y$-position of star 1 $x_{S_2}$ - (fixed) $x$-position of star 2 $y_{S_2}$ - (fixed) $x$-position of star 2 $x_P$ - $x$-position of the planet $y_P$ - $y$-position of the planet $\\mathbf{p} = (x_P,y_P)$ - position vector of the planet Let $\\mathbf{F_1}$ be the force of gravity of star 1 acting on the planet, and let $\\mathbf{F_2}$ be the force of gravity of star 2 acting on the planet. Newton's Law of Gravity states: $$ \\mathbf{F_1} = - \\frac{ G m_P m_{S_1} }{ || \\mathbf{d_1} ||^2} \\frac{ \\mathbf{d_1} }{ || \\mathbf{d_1} || } $$ $$ \\mathbf{F_2} = - \\frac{ G m_P m_{S_2} }{ || \\mathbf{d_2} ||^2} \\frac{ \\mathbf{d_2} }{ || \\mathbf{d_2} || } $$ where $\\mathbf{d_1} = (x_P-x_{S_1},y_P-y_{S_1})$ is the vector from star 1 to the planet, and $\\mathbf{d_2} = (x_P-x_{S_2},y_P-y_{S_2})$ is the vector from star 2 to the planet. Newton's Second Law of Motion states: $$ \\frac{ d^2 \\mathbf{x} }{ dt^2 } = \\mathbf{F}_1 + \\mathbf{F}_2 $$ and this leads us to the system of second order ODEs which govern the motion of the planet: $$ \\frac{d^2 x_P}{dt^2} = - \\frac{ G m_{S_1} (x_P - x_{S_1}) }{ || \\mathbf{d_1} ||^3} - \\frac{ G m_{S_2} (x_P - x_{S_2}) }{ || \\mathbf{d_2} ||^3} $$ $$ \\frac{d^2 y_P}{dt^2} = - \\frac{ G m_{S_1} (y_P - y_{S_1}) }{ || \\mathbf{d_1} ||^3} - \\frac{ G m_{S_2} (y_P - y_{S_2}) }{ || \\mathbf{d_2} ||^3} $$ To plot trajectories of the planet using odeint , we first need to write the system as a first order system. Introduce new variables $u_1 = x_P$, $u_2 = x_P'$, $u_3 = y_P$ and $u_4 = y_P'$ and write \\begin{align} u_1' &= u_2 \\\\ u_2' &= - \\frac{ G m_{S_1} (u_1 - x_{S_1}) }{ || \\mathbf{d_1} ||^3} - \\frac{ G m_{S_2} (u_1 - x_{S_2}) }{ || \\mathbf{d_2} ||^3} \\\\ u_3' &= u_4 \\\\ u_4' &= - \\frac{ G m_{S_1} (u_3 - y_{S_1}) }{ || \\mathbf{d_1} ||^3} - \\frac{ G m_{S_2} (u_3 - y_{S_2}) }{ || \\mathbf{d_2} ||^3} \\end{align} where $\\mathbf{d_1} = (u_1-x_{S_1},u_3-y_{S_1})$ and $\\mathbf{d_2} = (u_1-x_{S_2},u_3-y_{S_2})$. G = 4*np.pi**2 # Gravitational constant S1 = [-2,0] # Coordinates of Star 1 S2 = [2,0] # Coordinates of Star 2 M1 = 2 # Mass of Star 1 (in solar mass) M2 = 1 # Mass of Star 2 (in solar mass) def f(u,t): d1 = np.linalg.norm([u[0] - S1[0],u[2] - S1[1]]) d2 = np.linalg.norm([u[0] - S2[0],u[2] - S2[1]]) dudt = [0,0,0,0] dudt[0] = u[1] dudt[1] = -G*M1*(u[0] - S1[0])/d1**3 - G*M2*(u[0] - S2[0])/d2**3 dudt[2] = u[3] dudt[3] = -G*M1*(u[2] - S1[1])/d1**3 - G*M2*(u[2] - S2[1])/d2**3 return dudt u0 = [0,5,3,0] # Initial conditions of the planet: [xposition,xvelocity,yposition,yvelocity] t = np.linspace(0,30,2000) # Array of time values (in years) u = spi.odeint(f,u0,t) # Solve system: u = [xposition,xvelocity,yposition,yvelocity] plt.plot(u[:,0],u[:,2]) # Plot trajectory of the planet plt.plot(S1[0],S1[1],'ro',markersize=5*M1) # Plot Star 1 as a red star plt.plot(S2[0],S2[1],'ro',markersize=5*M2) # Plot Star 2 as a red star plt.axis('equal') plt.show() Let's put the code above into a function that we can call with different initial conditions to see what kinds of orbits we can create! def euler3body(S1,S2,M1,M2,u0,tf,numpoints=1000): # Define the vector function on the right side of the system of the equations def f(u,t): d1 = np.linalg.norm([u[0]-S1[0],u[2]-S1[1]]) d2 = np.linalg.norm([u[0]-S2[0],u[2]-S2[1]]) dudt = [0,0,0,0] dudt[0] = u[1] dudt[1] = -G*M1*(u[0]-S1[0])/d1**3 - G*M2*(u[0]-S2[0])/d2**3 dudt[2] = u[3] dudt[3] = -G*M1*(u[2]-S1[1])/d1**3 - G*M2*(u[2]-S2[1])/d2**3 return dudt t = np.linspace(0,tf,numpoints) # Array of time values (in years) u = spi.odeint(f,u0,t) # Solve system: u = [xposition,xvelocity,yposition,yvelocity] plt.plot(u[:,0],u[:,2]) # Plot trajectory of the planet plt.plot(S1[0],S1[1],'ro',markersize=5*M1) # Plot Star 1 as a red star plt.plot(S2[0],S2[1],'ro',markersize=5*M2) # Plot Star 2 as a red star plt.axis('equal') plt.show() euler3body([-1,0],[1,0],2,1,[0,5,3,0],30) euler3body([-1,1],[1,-1],2,1,[0,10,0,5],30) euler3body([-2,0],[2,0],1,1,[0,0,0,5],5) euler3body([-2,0],[2,0],1.5,2.5,[0,4.8,3,0],20) euler3body([0,10],[0,-10],4.9,4.9,[0,np.pi,0,np.pi],100)","title":"Euler's 3-Body Problem"},{"location":"differential-equations/systems/#exercises","text":"Under construction","title":"Exercises"},{"location":"differentiation/differentiation/","text":"Numerical Differentiation import numpy as np import matplotlib.pyplot as plt Derivative The derivative of a function $f(x)$ at $x=a$ is the limit $$ f'(a) = \\lim_{h \\to 0} \\frac{f(a+h) - f(a)}{h} $$ Difference Formulas There are 3 main difference formulas for numerically approximating derivatives. The forward difference formula with step size $h$ is $$ f'(a) \\approx \\frac{f(a + h) - f(a)}{h} $$ The backward difference formula with step size $h$ is $$ f'(a) \\approx \\frac{f(a) - f(a - h)}{h} $$ The central difference formula with step size $h$ is the average of the forward and backwards difference formulas $$ f'(a) \\approx \\frac{1}{2} \\left( \\frac{f(a + h) - f(a)}{h} + \\frac{f(a) - f(a - h)}{h} \\right) = \\frac{f(a + h) - f(a - h)}{2h} $$ Implementation Let's write a function called derivative which takes input parameters f , a , method and h (with default values method='central' and h=0.01 ) and returns the corresponding difference formula for $f'(a)$ with step size $h$. def derivative(f,a,method='central',h=0.01): '''Compute the difference formula for f'(a) with step size h. Parameters ---------- f : function Vectorized function of one variable a : number Compute derivative at x = a method : string Difference formula: 'forward', 'backward' or 'central' h : number Step size in difference formula Returns ------- float Difference formula: central: f(a+h) - f(a-h))/2h forward: f(a+h) - f(a))/h backward: f(a) - f(a-h))/h ''' if method == 'central': return (f(a + h) - f(a - h))/(2*h) elif method == 'forward': return (f(a + h) - f(a))/h elif method == 'backward': return (f(a) - f(a - h))/h else: raise ValueError(\"Method must be 'central', 'forward' or 'backward'.\") Let's test our function on some simple functions. For example, we know $$ \\left. \\frac{d}{dx} \\left( \\cos x \\right) \\, \\right|_{x=0} = -\\sin(0) = 0 $$ and we compute derivative(np.cos,0) 0.0 derivative(np.cos,0,method='forward',h=1e-8) 0.0 We also know $$ \\left. \\frac{d}{dx} \\left( e^x \\right) \\, \\right|_{x=0} = e^0 = 1 $$ and we compute derivative(np.exp,0,h=0.0001) 1.0000000016668897 derivative(np.exp,0,method='backward',h=0.0001) 0.9999500016666385 Notice that our function can take an array of inputs for $a$ and return the derivatives for each $a$ value. For example, we can plot the derivative of $\\sin(x)$: x = np.linspace(0,5*np.pi,100) dydx = derivative(np.sin,x) dYdx = np.cos(x) plt.figure(figsize=(12,5)) plt.plot(x,dydx,'r.',label='Central Difference') plt.plot(x,dYdx,'b',label='True Value') plt.title('Central Difference Derivative of y = cos(x)') plt.legend(loc='best') plt.show() Let's compute and plot the derivative of a complicated function $$ y=\\left(\\frac{4x^2+2x+1}{x+2e^x}\\right)^x $$ x = np.linspace(0,6,100) f = lambda x: ((4*x**2 + 2*x + 1)/(x + 2*np.exp(x)))**x y = f(x) dydx = derivative(f,x) plt.figure(figsize=(12,5)) plt.plot(x,y,label='y=f(x)') plt.plot(x,dydx,label=\"Central Difference y=f'(x)\") plt.legend() plt.grid(True) plt.show() Error Formulas Natural questions arise: how good are the approximations given by the forward, backwards and central difference formulas? We derive the error formulas from Taylor's Theorem . Theorem. The degree $n$ Taylor polynomial of $f(x)$ at $x=a$ with remainder term is $$ f(x) = f(a) + f'(a)(x - a) + \\frac{f''(a)}{2}(x-a)^2 + \\cdots + \\frac{f^{(n)}(a)}{n!}(x-a)^n + \\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1} $$ for some value $c$ between $x$ and $a$. Theorem. The forward difference formula error is $$ \\left| \\, \\frac{f(a+h) - f(a)}{h} - f'(a) \\, \\right| \\leq \\frac{hK_2}{2} $$ where $\\left| \\, f''(x) \\, \\right| \\leq K_2$ for all $x \\in [a,a+h]$. The same error fomula holds for the backward difference formula. Proof . Look at the degree 1 Taylor formula: $$ f(x) = f(a) + f'(a)(x-a) + \\frac{f''(c)}{2}(x-a)^{2} $$ Let $x = a+h$ and manipulate the formula \\begin{align} f(a+h) &= f(a) + f'(a)h + \\frac{f''(c)}{2}h^{2} \\\\ f(a+h) - f(a) &= f'(a)h + \\frac{f''(c)}{2}h^{2} \\\\ \\frac{f(a+h) - f(a)}{h} &= f'(a) + \\frac{f''(c)}{2}h \\\\ \\frac{f(a+h) - f(a)}{h} - f'(a) &= \\frac{f''(c)}{2}h \\end{align} Let $K_2$ such that $\\left| \\, f''(x) \\, \\right| \\leq K_2$ for all $x \\in [a,a+h]$ and we see the result. Theorem. The central difference formula error is: $$ \\left| \\frac{f(a+h) - f(a-h)}{2h} - f'(a) \\right| \\leq \\frac{h^2K_3}{6} $$ where $|f'''(x)| \\leq K_3$ for all $x \\in [a-h,a+h]$. Proof . Look at the Taylor polynomial of degree 2: $$ f(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)}{2}(x-a)^2 + \\frac{f'''(c)}{6}(x-a)^{3} $$ Let $x = a + h$ and also $x = a - h$ and write: \\begin{align} f(a+h) &= f(a) + f'(a)h + \\frac{f''(a)}{2}h^2 + \\frac{f'''(c_1)}{6}h^{3} \\\\ f(a-h) &= f(a) - f'(a)h + \\frac{f''(a)}{2}h^2 - \\frac{f'''(c_2)}{6}h^{3} \\\\ f(a+h) - f(a-h) &= 2 f'(a)h + \\frac{f'''(c_1)}{6}h^{3} + \\frac{f'''(c_2)}{6}h^{3} \\\\ \\frac{f(a+h) - f(a-h)}{2h} - f'(a) &= \\frac{f'''(c_1) + f'''(c_2)}{12}h^{2} \\end{align} Notice that $f'''(x)$ is continuous (by assumption) and $(f'''(c_1) + f'''(c_2))/2$ is between $f'''(c_1)$ and $f'''(c_2)$ and so there exists some $c$ between $c_1$ and $c_2$ such that $$ f'''(c) = \\frac{f'''(c_1) + f'''(c_2)}{2} $$ by the Intermediate Value Theorem. Let $K_3$ such that $\\left| \\, f'''(x) \\, \\right| \\leq K_3$ for all $x \\in [a-h,a+h]$ and we see the result. scipy.misc.derivative The SciPy function scipy.misc.derivative computes derivatives using the central difference formula. from scipy.misc import derivative x = np.arange(0,5) derivative(np.exp,x,dx=0.1) array([ 1.0016675 , 2.72281456, 7.40137735, 20.11902956, 54.68919246]) Higher Order Derivatives Under construction Examples Taylor series Let's plot the Taylor polynomial $T_3(x)$ of degree 3 centered at $x=0$ for $f(x) = \\frac{3e^x}{x^2 + x + 1}$ over the interval $x \\in [-3,3]$. First, let's plot the graph $y=f(x)$: x = np.linspace(-3,3,100) f = lambda x: 3*np.exp(x) / (x**2 + x + 1) y = f(x) plt.plot(x,y); plt.show() Let's compute the coefficients $a_n = \\frac{f^{(n)}(0)}{n!}$ for $n=0,1,2,3$: a0 = f(0) a1 = derivative(f,0,dx=0.001,n=1) a2 = derivative(f,0,dx=0.001,n=2) / 2 a3 = derivative(f,0,dx=0.001,n=3,order=5) / 6 # The parameter order specifies the number of points to use # The value order must be odd and at least n + 1 print(a0,a1,a2,a3) 3.0 1.9999983891239026e-06 -1.50000037502096 1.9999920608526622 Finally, let's plot $f(x)$ and $T_3(x)$ together: T3 = a0 + a1*x + a2*x**2 + a3*x**3 plt.plot(x,y,x,T3), plt.xlim([-3,3]), plt.ylim([0,5]); plt.show() Arc length Write a function called arc_length which takes parameters f , a , b and N and returns an approximation of the arc length of $f(x)$ from $a$ to $b$ $$ L \\approx \\int_a^b \\sqrt{ 1 + \\left( f'(x) \\right)^2 } dx $$ The function uses the trapezoid rule ( scipy.integrate.trapz ) to estimate the integral using $N+1$ equally spaced points from $a$ to $b$ (with default value $N=100$), and the central difference formula to approximate $f'(x)$ with step size $h=(b-a)/N$. Note that we can't use the central difference formula at the endpoints because they use $x$ values outside the interval $[a,b]$ and our function may not be defined there. import scipy.integrate as spi def arc_length(f,a,b,N=100): '''Approximate the arc length of y=f(x) from x=a to x=b. Parameters ---------- f : function Vectorized function of one variable a,b : numbers Limits of the interval [a,b] N : number Subintervals in trapezoid method Returns ------- float Trapezoid method approximation of the integral \\int_a^b \\sqrt{1 + (f'(x))^2} dx representing the arc length of y=f(x) from x=a to x=b. ''' x = np.linspace(a,b,N+1) y = f(x) # Compute central difference formula for x_k for 0 < k < N h = (b-a)/N x_interior = x[1:-1] df_interior = (f(x_interior + h) - f(x_interior - h))/(2*h) # Use forward/backward difference formula at the endpoints df_a = (f(a + h) - f(a))/h df_b = (f(b) - f(b - h))/h df = np.hstack([[df_a],df_interior,[df_b]]) # Compute values of the integrand in arc length formula y = np.sqrt(1 + df**2) # Compute the integral L = spi.trapz(y,x) return L Let's test our function with input where we know the exact output. For example, the arc length of $f(x)=x$ from $a=0$ to $b=1$ is $L=\\sqrt{2}$ and we compute f = lambda x: x a = 0; b = 1; arc_length(f,a,b) 1.4142135623730954 and compare with the exact value np.sqrt(2) 1.4142135623730951 The arc length of $f(x)=\\sqrt{1 - x^2}$ from $a=0$ to $b=\\frac{1}{\\sqrt{2}}$ is $L=\\frac{\\pi}{4}$ and we compute f = lambda x: np.sqrt(1 - x**2) a = 0; b = 1/np.sqrt(2); arc_length(f,a,b) 0.7853899139845573 and compare to the exact value np.pi/4 0.7853981633974483 The arc length of $f(x)=\\frac{2x^{3/2}}{3}$ from $a=0$ to $b=1$ is $L = \\frac{2}{3}\\left( 2^{3/2} - 1 \\right)$ and we compute f = lambda x: 2*(x**(3/2))/3 a = 0; b = 1; N = 10**5; arc_length(f,a,b) 1.2189308934829772 and compare to the exact value (2/3)*(2**(3/2) - 1) 1.2189514164974602 Exercises Exercise 1. Use derivative to compute values and then plot the derivative $f'(x)$ of the function $$ f(x) = \\frac{7x^3-5x+1}{2x^4+x^2+1} \\ , \\ x \\in [-5,5] $$ Compute the derivative of $f(x)$ by hand (using the quotient rule), plot the formula for $f'(x)$ and compare to the numerical approximation above. Exercise 2. Plot the Taylor polynomial $T_4(x)$ of degree 4 centered at $x=0$ of the function $$ f(x) = \\cos(x) + \\sin(2x) $$ over the interval $x \\in [-\\pi,\\pi]$.","title":"Numerical Differentiation"},{"location":"differentiation/differentiation/#numerical-differentiation","text":"import numpy as np import matplotlib.pyplot as plt","title":"Numerical Differentiation"},{"location":"differentiation/differentiation/#derivative","text":"The derivative of a function $f(x)$ at $x=a$ is the limit $$ f'(a) = \\lim_{h \\to 0} \\frac{f(a+h) - f(a)}{h} $$","title":"Derivative"},{"location":"differentiation/differentiation/#difference-formulas","text":"There are 3 main difference formulas for numerically approximating derivatives. The forward difference formula with step size $h$ is $$ f'(a) \\approx \\frac{f(a + h) - f(a)}{h} $$ The backward difference formula with step size $h$ is $$ f'(a) \\approx \\frac{f(a) - f(a - h)}{h} $$ The central difference formula with step size $h$ is the average of the forward and backwards difference formulas $$ f'(a) \\approx \\frac{1}{2} \\left( \\frac{f(a + h) - f(a)}{h} + \\frac{f(a) - f(a - h)}{h} \\right) = \\frac{f(a + h) - f(a - h)}{2h} $$","title":"Difference Formulas"},{"location":"differentiation/differentiation/#implementation","text":"Let's write a function called derivative which takes input parameters f , a , method and h (with default values method='central' and h=0.01 ) and returns the corresponding difference formula for $f'(a)$ with step size $h$. def derivative(f,a,method='central',h=0.01): '''Compute the difference formula for f'(a) with step size h. Parameters ---------- f : function Vectorized function of one variable a : number Compute derivative at x = a method : string Difference formula: 'forward', 'backward' or 'central' h : number Step size in difference formula Returns ------- float Difference formula: central: f(a+h) - f(a-h))/2h forward: f(a+h) - f(a))/h backward: f(a) - f(a-h))/h ''' if method == 'central': return (f(a + h) - f(a - h))/(2*h) elif method == 'forward': return (f(a + h) - f(a))/h elif method == 'backward': return (f(a) - f(a - h))/h else: raise ValueError(\"Method must be 'central', 'forward' or 'backward'.\") Let's test our function on some simple functions. For example, we know $$ \\left. \\frac{d}{dx} \\left( \\cos x \\right) \\, \\right|_{x=0} = -\\sin(0) = 0 $$ and we compute derivative(np.cos,0) 0.0 derivative(np.cos,0,method='forward',h=1e-8) 0.0 We also know $$ \\left. \\frac{d}{dx} \\left( e^x \\right) \\, \\right|_{x=0} = e^0 = 1 $$ and we compute derivative(np.exp,0,h=0.0001) 1.0000000016668897 derivative(np.exp,0,method='backward',h=0.0001) 0.9999500016666385 Notice that our function can take an array of inputs for $a$ and return the derivatives for each $a$ value. For example, we can plot the derivative of $\\sin(x)$: x = np.linspace(0,5*np.pi,100) dydx = derivative(np.sin,x) dYdx = np.cos(x) plt.figure(figsize=(12,5)) plt.plot(x,dydx,'r.',label='Central Difference') plt.plot(x,dYdx,'b',label='True Value') plt.title('Central Difference Derivative of y = cos(x)') plt.legend(loc='best') plt.show() Let's compute and plot the derivative of a complicated function $$ y=\\left(\\frac{4x^2+2x+1}{x+2e^x}\\right)^x $$ x = np.linspace(0,6,100) f = lambda x: ((4*x**2 + 2*x + 1)/(x + 2*np.exp(x)))**x y = f(x) dydx = derivative(f,x) plt.figure(figsize=(12,5)) plt.plot(x,y,label='y=f(x)') plt.plot(x,dydx,label=\"Central Difference y=f'(x)\") plt.legend() plt.grid(True) plt.show()","title":"Implementation"},{"location":"differentiation/differentiation/#error-formulas","text":"Natural questions arise: how good are the approximations given by the forward, backwards and central difference formulas? We derive the error formulas from Taylor's Theorem . Theorem. The degree $n$ Taylor polynomial of $f(x)$ at $x=a$ with remainder term is $$ f(x) = f(a) + f'(a)(x - a) + \\frac{f''(a)}{2}(x-a)^2 + \\cdots + \\frac{f^{(n)}(a)}{n!}(x-a)^n + \\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1} $$ for some value $c$ between $x$ and $a$. Theorem. The forward difference formula error is $$ \\left| \\, \\frac{f(a+h) - f(a)}{h} - f'(a) \\, \\right| \\leq \\frac{hK_2}{2} $$ where $\\left| \\, f''(x) \\, \\right| \\leq K_2$ for all $x \\in [a,a+h]$. The same error fomula holds for the backward difference formula. Proof . Look at the degree 1 Taylor formula: $$ f(x) = f(a) + f'(a)(x-a) + \\frac{f''(c)}{2}(x-a)^{2} $$ Let $x = a+h$ and manipulate the formula \\begin{align} f(a+h) &= f(a) + f'(a)h + \\frac{f''(c)}{2}h^{2} \\\\ f(a+h) - f(a) &= f'(a)h + \\frac{f''(c)}{2}h^{2} \\\\ \\frac{f(a+h) - f(a)}{h} &= f'(a) + \\frac{f''(c)}{2}h \\\\ \\frac{f(a+h) - f(a)}{h} - f'(a) &= \\frac{f''(c)}{2}h \\end{align} Let $K_2$ such that $\\left| \\, f''(x) \\, \\right| \\leq K_2$ for all $x \\in [a,a+h]$ and we see the result. Theorem. The central difference formula error is: $$ \\left| \\frac{f(a+h) - f(a-h)}{2h} - f'(a) \\right| \\leq \\frac{h^2K_3}{6} $$ where $|f'''(x)| \\leq K_3$ for all $x \\in [a-h,a+h]$. Proof . Look at the Taylor polynomial of degree 2: $$ f(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)}{2}(x-a)^2 + \\frac{f'''(c)}{6}(x-a)^{3} $$ Let $x = a + h$ and also $x = a - h$ and write: \\begin{align} f(a+h) &= f(a) + f'(a)h + \\frac{f''(a)}{2}h^2 + \\frac{f'''(c_1)}{6}h^{3} \\\\ f(a-h) &= f(a) - f'(a)h + \\frac{f''(a)}{2}h^2 - \\frac{f'''(c_2)}{6}h^{3} \\\\ f(a+h) - f(a-h) &= 2 f'(a)h + \\frac{f'''(c_1)}{6}h^{3} + \\frac{f'''(c_2)}{6}h^{3} \\\\ \\frac{f(a+h) - f(a-h)}{2h} - f'(a) &= \\frac{f'''(c_1) + f'''(c_2)}{12}h^{2} \\end{align} Notice that $f'''(x)$ is continuous (by assumption) and $(f'''(c_1) + f'''(c_2))/2$ is between $f'''(c_1)$ and $f'''(c_2)$ and so there exists some $c$ between $c_1$ and $c_2$ such that $$ f'''(c) = \\frac{f'''(c_1) + f'''(c_2)}{2} $$ by the Intermediate Value Theorem. Let $K_3$ such that $\\left| \\, f'''(x) \\, \\right| \\leq K_3$ for all $x \\in [a-h,a+h]$ and we see the result.","title":"Error Formulas"},{"location":"differentiation/differentiation/#scipymiscderivative","text":"The SciPy function scipy.misc.derivative computes derivatives using the central difference formula. from scipy.misc import derivative x = np.arange(0,5) derivative(np.exp,x,dx=0.1) array([ 1.0016675 , 2.72281456, 7.40137735, 20.11902956, 54.68919246])","title":"scipy.misc.derivative"},{"location":"differentiation/differentiation/#higher-order-derivatives","text":"Under construction","title":"Higher Order Derivatives"},{"location":"differentiation/differentiation/#examples","text":"","title":"Examples"},{"location":"differentiation/differentiation/#taylor-series","text":"Let's plot the Taylor polynomial $T_3(x)$ of degree 3 centered at $x=0$ for $f(x) = \\frac{3e^x}{x^2 + x + 1}$ over the interval $x \\in [-3,3]$. First, let's plot the graph $y=f(x)$: x = np.linspace(-3,3,100) f = lambda x: 3*np.exp(x) / (x**2 + x + 1) y = f(x) plt.plot(x,y); plt.show() Let's compute the coefficients $a_n = \\frac{f^{(n)}(0)}{n!}$ for $n=0,1,2,3$: a0 = f(0) a1 = derivative(f,0,dx=0.001,n=1) a2 = derivative(f,0,dx=0.001,n=2) / 2 a3 = derivative(f,0,dx=0.001,n=3,order=5) / 6 # The parameter order specifies the number of points to use # The value order must be odd and at least n + 1 print(a0,a1,a2,a3) 3.0 1.9999983891239026e-06 -1.50000037502096 1.9999920608526622 Finally, let's plot $f(x)$ and $T_3(x)$ together: T3 = a0 + a1*x + a2*x**2 + a3*x**3 plt.plot(x,y,x,T3), plt.xlim([-3,3]), plt.ylim([0,5]); plt.show()","title":"Taylor series"},{"location":"differentiation/differentiation/#arc-length","text":"Write a function called arc_length which takes parameters f , a , b and N and returns an approximation of the arc length of $f(x)$ from $a$ to $b$ $$ L \\approx \\int_a^b \\sqrt{ 1 + \\left( f'(x) \\right)^2 } dx $$ The function uses the trapezoid rule ( scipy.integrate.trapz ) to estimate the integral using $N+1$ equally spaced points from $a$ to $b$ (with default value $N=100$), and the central difference formula to approximate $f'(x)$ with step size $h=(b-a)/N$. Note that we can't use the central difference formula at the endpoints because they use $x$ values outside the interval $[a,b]$ and our function may not be defined there. import scipy.integrate as spi def arc_length(f,a,b,N=100): '''Approximate the arc length of y=f(x) from x=a to x=b. Parameters ---------- f : function Vectorized function of one variable a,b : numbers Limits of the interval [a,b] N : number Subintervals in trapezoid method Returns ------- float Trapezoid method approximation of the integral \\int_a^b \\sqrt{1 + (f'(x))^2} dx representing the arc length of y=f(x) from x=a to x=b. ''' x = np.linspace(a,b,N+1) y = f(x) # Compute central difference formula for x_k for 0 < k < N h = (b-a)/N x_interior = x[1:-1] df_interior = (f(x_interior + h) - f(x_interior - h))/(2*h) # Use forward/backward difference formula at the endpoints df_a = (f(a + h) - f(a))/h df_b = (f(b) - f(b - h))/h df = np.hstack([[df_a],df_interior,[df_b]]) # Compute values of the integrand in arc length formula y = np.sqrt(1 + df**2) # Compute the integral L = spi.trapz(y,x) return L Let's test our function with input where we know the exact output. For example, the arc length of $f(x)=x$ from $a=0$ to $b=1$ is $L=\\sqrt{2}$ and we compute f = lambda x: x a = 0; b = 1; arc_length(f,a,b) 1.4142135623730954 and compare with the exact value np.sqrt(2) 1.4142135623730951 The arc length of $f(x)=\\sqrt{1 - x^2}$ from $a=0$ to $b=\\frac{1}{\\sqrt{2}}$ is $L=\\frac{\\pi}{4}$ and we compute f = lambda x: np.sqrt(1 - x**2) a = 0; b = 1/np.sqrt(2); arc_length(f,a,b) 0.7853899139845573 and compare to the exact value np.pi/4 0.7853981633974483 The arc length of $f(x)=\\frac{2x^{3/2}}{3}$ from $a=0$ to $b=1$ is $L = \\frac{2}{3}\\left( 2^{3/2} - 1 \\right)$ and we compute f = lambda x: 2*(x**(3/2))/3 a = 0; b = 1; N = 10**5; arc_length(f,a,b) 1.2189308934829772 and compare to the exact value (2/3)*(2**(3/2) - 1) 1.2189514164974602","title":"Arc length"},{"location":"differentiation/differentiation/#exercises","text":"Exercise 1. Use derivative to compute values and then plot the derivative $f'(x)$ of the function $$ f(x) = \\frac{7x^3-5x+1}{2x^4+x^2+1} \\ , \\ x \\in [-5,5] $$ Compute the derivative of $f(x)$ by hand (using the quotient rule), plot the formula for $f'(x)$ and compare to the numerical approximation above. Exercise 2. Plot the Taylor polynomial $T_4(x)$ of degree 4 centered at $x=0$ of the function $$ f(x) = \\cos(x) + \\sin(2x) $$ over the interval $x \\in [-\\pi,\\pi]$.","title":"Exercises"},{"location":"integration/integrals/","text":"Definite Integrals The definite integral of a function $f(x)$ over an interval $[a,b]$ is the limit $$ \\int_a^b f(x) \\, dx = \\lim_{N \\to \\infty} \\sum_{i=1}^N f(x_i^ * ) (x_i - x_{i-1}) \\ \\ , \\ x_i^* \\in [x_{i-1},x_i] $$ where, for each $N$, $$ x_0 = a < x_1 < \\cdots < x_N = b $$ is a partition of $[a,b]$ with $N$ subintervals and the values $x_i^ * \\in [x_{i-1},x_i]$ chosen in each subinterval is arbitrary. The definite integral represents the (net) area under the curve of the graph of $y=f(x)$ on the interval $[a,b]$. $$ \\int_a^b f(x) \\, dx = \\text{(net) area under the curve } y = f(x) \\text{ on } [a,b] $$ The term \"net\" means that area above the $x$-axis is positive and the area under the $x$-axis counts as negative area. For example, we can visualize the integral: $$ \\int_{\\pi/2}^{3\\pi/2} \\left( \\sin(0.2 x) + \\sin(2x) + 1 \\right) dx $$ import numpy as np import matplotlib.pyplot as plt f = lambda x: np.sin(0.2*x) + np.sin(2*x) + 1 x = np.linspace(0,2*np.pi,100) y = f(x) plt.plot(x,y) X = np.linspace(np.pi/2,3*np.pi/2,100) Y = f(X) plt.fill_between(X,Y) plt.xticks([np.pi/2,3*np.pi/2],['$\\pi/2$','$3\\pi/2$']); plt.yticks([]); plt.xlim([0,2*np.pi]); plt.ylim([0,3]); plt.show() In our introductory calculus courses, we focus on integrals which we can solve exactly by the Fundamental Theorem of Calculus such as $$ \\int_0^{\\pi/2} \\cos(x) \\, dx = \\sin(\\pi/2) - \\sin(0) = 1 $$ However, most definite integrals are impossible to solve exactly. For example, the famous error function in probability $$ \\mathrm{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_0^x e^{-t^2} dt $$ is a definite integral for which there is no explicit formula. The idea behind numerical integration is to use simple geometric shapes to approximate the area under the curve $y=f(x)$ to estimate definite integrals. In this section, we explore the simplest methods of numerical integration: Riemann sums, the trapezoid rule and Simpson's rule.","title":"Definite Integrals"},{"location":"integration/integrals/#definite-integrals","text":"The definite integral of a function $f(x)$ over an interval $[a,b]$ is the limit $$ \\int_a^b f(x) \\, dx = \\lim_{N \\to \\infty} \\sum_{i=1}^N f(x_i^ * ) (x_i - x_{i-1}) \\ \\ , \\ x_i^* \\in [x_{i-1},x_i] $$ where, for each $N$, $$ x_0 = a < x_1 < \\cdots < x_N = b $$ is a partition of $[a,b]$ with $N$ subintervals and the values $x_i^ * \\in [x_{i-1},x_i]$ chosen in each subinterval is arbitrary. The definite integral represents the (net) area under the curve of the graph of $y=f(x)$ on the interval $[a,b]$. $$ \\int_a^b f(x) \\, dx = \\text{(net) area under the curve } y = f(x) \\text{ on } [a,b] $$ The term \"net\" means that area above the $x$-axis is positive and the area under the $x$-axis counts as negative area. For example, we can visualize the integral: $$ \\int_{\\pi/2}^{3\\pi/2} \\left( \\sin(0.2 x) + \\sin(2x) + 1 \\right) dx $$ import numpy as np import matplotlib.pyplot as plt f = lambda x: np.sin(0.2*x) + np.sin(2*x) + 1 x = np.linspace(0,2*np.pi,100) y = f(x) plt.plot(x,y) X = np.linspace(np.pi/2,3*np.pi/2,100) Y = f(X) plt.fill_between(X,Y) plt.xticks([np.pi/2,3*np.pi/2],['$\\pi/2$','$3\\pi/2$']); plt.yticks([]); plt.xlim([0,2*np.pi]); plt.ylim([0,3]); plt.show() In our introductory calculus courses, we focus on integrals which we can solve exactly by the Fundamental Theorem of Calculus such as $$ \\int_0^{\\pi/2} \\cos(x) \\, dx = \\sin(\\pi/2) - \\sin(0) = 1 $$ However, most definite integrals are impossible to solve exactly. For example, the famous error function in probability $$ \\mathrm{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_0^x e^{-t^2} dt $$ is a definite integral for which there is no explicit formula. The idea behind numerical integration is to use simple geometric shapes to approximate the area under the curve $y=f(x)$ to estimate definite integrals. In this section, we explore the simplest methods of numerical integration: Riemann sums, the trapezoid rule and Simpson's rule.","title":"Definite Integrals"},{"location":"integration/riemann-sums/","text":"Riemann Sums import numpy as np import matplotlib.pyplot as plt Definition A Riemann sum of a function $f(x)$ over a partition $$ x_0 = a < x_1 < \\cdots < x_{N-1} < x_N = b $$ is a sum of the form $$ \\sum_{i=1}^N f(x_i^ * ) (x_i - x_{i-1}) \\ \\ , \\ x_i^* \\in [x_{i-1},x_i] $$ where each value $x_i^* \\in [x_{i-1},x_i]$ in each subinterval is arbitrary. Riemann sums are important because they provide an easy way to approximate a definite integral $$ \\int_a^b f(x) \\, dx \\approx \\sum_{i=1}^N f(x_i^ * ) (x_i - x_{i-1}) \\ \\ , \\ x_i^* \\in [x_{i-1},x_i] $$ Notice that the product $f(x_i^ * ) (x_i - x_{i-1})$ for each $i$ is the area of a rectangle of height $f(x_i^ * )$ and width $x_i - x_{i-1}$. We can think of a Riemann sum as the area of $N$ rectangles with heights determined by the graph of $y=f(x)$. The value $x_i^*$ chosen in each subinterval is arbitrary however there are certain obvious choices: A left Riemann sum is when each $x_i^* = x_{i-1}$ is the left endpoint of the subinterval $[x_{i-1},x_i]$ A right Riemann sum is when each $x_i^* = x_i$ is the right endpoint of the subinterval $[x_{i-1},x_i]$ A midpoint Riemann sum is when each $x_i^* = (x_{i-1} + x_i)/2$ is the midpoint of the subinterval $[x_{i-1},x_i]$ Let's visualize rectangles in the left, right and midpoint Riemann sums for the function $$ f(x) = \\frac{1}{1 + x^2} $$ over the interval $[0,5]$ with a partition of size $N=10$. f = lambda x : 1/(1+x**2) a = 0; b = 5; N = 10 n = 10 # Use n*N+1 points to plot the function smoothly x = np.linspace(a,b,N+1) y = f(x) X = np.linspace(a,b,n*N+1) Y = f(X) plt.figure(figsize=(15,5)) plt.subplot(1,3,1) plt.plot(X,Y,'b') x_left = x[:-1] # Left endpoints y_left = y[:-1] plt.plot(x_left,y_left,'b.',markersize=10) plt.bar(x_left,y_left,width=(b-a)/N,alpha=0.2,align='edge',edgecolor='b') plt.title('Left Riemann Sum, N = {}'.format(N)) plt.subplot(1,3,2) plt.plot(X,Y,'b') x_mid = (x[:-1] + x[1:])/2 # Midpoints y_mid = f(x_mid) plt.plot(x_mid,y_mid,'b.',markersize=10) plt.bar(x_mid,y_mid,width=(b-a)/N,alpha=0.2,edgecolor='b') plt.title('Midpoint Riemann Sum, N = {}'.format(N)) plt.subplot(1,3,3) plt.plot(X,Y,'b') x_right = x[1:] # Left endpoints y_right = y[1:] plt.plot(x_right,y_right,'b.',markersize=10) plt.bar(x_right,y_right,width=-(b-a)/N,alpha=0.2,align='edge',edgecolor='b') plt.title('Right Riemann Sum, N = {}'.format(N)) plt.show() Notice that when the function $f(x)$ is decreasing on $[a,b]$ the left endpoints give an overestimate of the integral $\\int_a^b f(x) dx$ and right endpoints give an underestimate. The opposite is true is when the function is increasing. Let's compute the value of each of the Riemann sums: dx = (b-a)/N x_left = np.linspace(a,b-dx,N) x_midpoint = np.linspace(dx/2,b - dx/2,N) x_right = np.linspace(dx,b,N) print(\"Partition with\",N,\"subintervals.\") left_riemann_sum = np.sum(f(x_left) * dx) print(\"Left Riemann Sum:\",left_riemann_sum) midpoint_riemann_sum = np.sum(f(x_midpoint) * dx) print(\"Midpoint Riemann Sum:\",midpoint_riemann_sum) right_riemann_sum = np.sum(f(x_right) * dx) print(\"Right Riemann Sum:\",right_riemann_sum) Partition with 10 subintervals. Left Riemann Sum: 1.613488696614725 Midpoint Riemann Sum: 1.373543428316664 Right Riemann Sum: 1.1327194658454942 We know the exact value $$ \\int_0^5 \\frac{1}{1 + x^2} dx = \\arctan(5) $$ and we can compare the Riemann sums to the value I = np.arctan(5) print(I) 1.373400766945016 print(\"Left Riemann Sum Error:\",np.abs(left_riemann_sum - I)) print(\"Midpoint Riemann Sum:\",np.abs(midpoint_riemann_sum - I)) print(\"Right Riemann Sum:\",np.abs(right_riemann_sum - I)) Left Riemann Sum Error: 0.24008792966970915 Midpoint Riemann Sum: 0.00014266137164820059 Right Riemann Sum: 0.24068130109952168 Error Formulas A Riemann sum is an approximation of a definite integral. A natural question arises: how good of an approximation is a Riemann sum? Theorem. Let $L_N(f)$ denote the left Riemann sum $$ L_N(f) = \\sum_{i=1}^N f(x_{i-1} ) \\Delta x $$ where $\\Delta x = (b-a)/N$ and $x_i = a + i \\Delta x$. The error bound is $$ E_N^{L}(f) = \\left| \\ \\int_a^b f(x) \\ dx - L_N(f) \\ \\right| \\leq \\frac{(b-a)^2}{2 N} K_1 $$ where $\\left| \\, f'(x) \\, \\right| \\leq K_1$ for all $x \\in [a,b]$. Theorem. Let $R_N(f)$ denote the right Riemann sum $$ R_N(f) = \\sum_{i=1}^N f(x_{i} ) \\Delta x $$ where $\\Delta x = (b-a)/N$ and $x_i = a + i \\Delta x$. The error bound is $$ E_N^{R}(f) = \\left| \\ \\int_a^b f(x) \\ dx - R_N(f) \\ \\right| \\leq \\frac{(b-a)^2}{2 N} K_1 $$ where $\\left| \\, f'(x) \\, \\right| \\leq K_1$ for all $x \\in [a,b]$. Theorem. Let $M_N(f)$ denote the midpoint Riemann sum $$ M_N(f) = \\sum_{i=1}^N f(x_i^* ) \\Delta x $$ where $\\Delta x = (b-a)/N$ and $x_i^* = (x_{i-1} + x_i)/2$ for $x_i = a + i \\Delta x$. The error bound is $$ E_N^{M}(f) = \\left| \\ \\int_a^b f(x) \\ dx - M_N(f) \\ \\right| \\leq \\frac{(b-a)^3}{24 N^2} K_2 $$ where $\\left| \\, f''(x) \\, \\right| \\leq K_2$ for all $x \\in [a,b]$. There are several points to notice: Left and right Riemann sums have the same error bound which depends on the first derivative $f'(x)$. Midpoint Riemann sum error bound depends on the second derivative $f''(x)$. We expect the midpoint Riemann sum to give a better approximation as $N \\to \\infty$ since its error bound is inversely proportional to $N^2$ but left/right Riemann sum error bound is inversely proportional only to $N$. Implementation Let's write a function called riemann_sum which takes 5 input parameters f , a , b , N and method and returns the Riemann sum $$ \\sum_{i=1}^N f(x_i^*) \\Delta x $$ where $\\Delta x = (b-a)/N$ and $x_i = a + i\\Delta x$ defines a partition with $N$ subintervals of equal length , and method determines whether we use left endpoints, right endpoints or midpoints (with midpoints as the default method). def riemann_sum(f,a,b,N,method='midpoint'): '''Compute the Riemann sum of f(x) over the interval [a,b]. Parameters ---------- f : function Vectorized function of one variable a , b : numbers Endpoints of the interval [a,b] N : integer Number of subintervals of equal length in the partition of [a,b] method : string Determines the kind of Riemann sum: right : Riemann sum using right endpoints left : Riemann sum using left endpoints midpoint (default) : Riemann sum using midpoints Returns ------- float Approximation of the integral given by the Riemann sum. ''' dx = (b - a)/N x = np.linspace(a,b,N+1) if method == 'left': x_left = x[:-1] return np.sum(f(x_left)*dx) elif method == 'right': x_right = x[1:] return np.sum(f(x_right)*dx) elif method == 'midpoint': x_mid = (x[:-1] + x[1:])/2 return np.sum(f(x_mid)*dx) else: raise ValueError(\"Method must be 'left', 'right' or 'midpoint'.\") Let's test our function with inputs where we know exactly what the output should be. For example, we know $$ \\int_0^{\\pi/2} \\sin(x) \\, dx = 1 $$ and, since $\\sin(x)$ is increasing on $[0,\\pi/2]$, we know that left endpoints will give an under-estimate, and right endpoints will give an over-estimate. riemann_sum(np.sin,0,np.pi/2,100) 1.0000102809119054 riemann_sum(np.sin,0,np.pi/2,100,'right') 1.007833419873582 riemann_sum(np.sin,0,np.pi/2,100,'left') 0.992125456605633 We also know that $\\int_0^1 x \\, dx = 1/2$ and midpoint should give the result exactly for any $N$: riemann_sum(lambda x : x,0,1,1) 0.5 Examples Approximate Pi Find a value $N$ which guarantees the right Riemann sum of $f(x)=\\frac{4}{1 + x^2}$ over $[0,1]$ is within $10^{-5}$ of the exact value $$ \\int_0^1 \\frac{4}{1 + x^2} dx = \\pi $$ Compute $$ f'(x) = -\\frac{8x}{(1+x^2)^2} $$ Use brute force optimization to find a bound on $\\left| f'(x) \\right|$ on $[0,1]$: x = np.linspace(0,1,1000) y = np.abs(-8*x/(1 + x**2)**2) np.max(y) 2.5980759093919907 Therefore, $\\left| f'(x) \\right| \\leq 2.6$ for $x \\in [0,1]$. Use the error bound $$ \\frac{(b-a)^2}{2 N} K_1 \\leq 10^{-5} \\ \\Rightarrow \\ \\frac{1.3}{N} \\leq 10^{-5} \\ \\Rightarrow \\ 130000 \\leq N $$ Let's compute the right Riemann sum for $N=130000$: approximation = riemann_sum(lambda x : 4/(1 + x**2),0,1,130000,method='right') print(approximation) 3.1415849612722386 Verify the accuracy of the approximation np.abs(approximation - np.pi) < 10**(-5) True Approximate ln(2) Find a value $N$ which guarantees the midpoint Riemann sum of $f(x)=\\frac{1}{x}$ over $[1,2]$ is within $10^{-8}$ of the exact value $$ \\int_1^2 \\frac{1}{x} dx = \\ln(2) $$ Compute $$ f''(x) = \\frac{2}{x^3} $$ Since $f''(x)$ is decreasing for all $x>0$ we have $\\left| \\, f''(x) \\, \\right| \\leq 2$ for all $x \\in [1,2]$. Use the error bound: $$ \\frac{(b-a)^3}{24 N^2} K_2 \\leq 10^{-8} \\ \\Rightarrow \\ \\frac{1}{12 N^2} \\leq 10^{-8} \\ \\Rightarrow \\frac{10^4}{\\sqrt{12}} \\leq N $$ 10**4 / np.sqrt(12) 2886.751345948129 Therefore a partition of size $N=2887$ guarantees the desired accuracy: approximation = riemann_sum(lambda x : 1/x,1,2,2887,method='midpoint') print(approximation) 0.6931471768105913 Verify the accuracy of the approximation: np.abs(approximation - np.log(2)) < 10**(-8) True Exercises Exercise 1. Consider the integral $$ \\int_1^2 \\frac{dx}{1+x^3} $$ Without plotting the functions $f(x)$, $f'(x)$ or $f''(x)$, find a value $N$ such that $E_N^R(f) \\leq 10^{-5}$ given $$ f(x) = \\frac{1}{1 + x^3} \\ , \\ f'(x) = -\\frac{3 x^{2}}{\\left(x^{3} + 1\\right)^{2}} \\ , \\ f''(x) = \\frac{6 x \\left(2 x^{3} - 1\\right)}{\\left(x + 1\\right)^{3} \\left(x^{2} - x + 1\\right)^{3}} \\\\ $$ Exercise 2. Plot the function $f''(x)$ from the previous question on the interval $[1,2]$ and find a value $N$ such that $E_N^M(f) \\leq 10^{-5}$ for the integral in the previous question. Exercise 3. Let $f(x) = x^x$ and note that $$ f'(x) = x^{x} \\left(\\log{\\left(x \\right)} + 1\\right) \\ , \\ f''(x) = x^{x} \\left(\\log{\\left(x \\right)} + 1\\right)^{2} + x^{x-1} $$ Plot the function $f''(x)$ and use that information to compute $T_N(f)$ for the integral $$ \\int_1^2 x^x \\, dx $$ such that $E_N^T(f) \\leq 10^{-3}$.","title":"Riemann Sums"},{"location":"integration/riemann-sums/#riemann-sums","text":"import numpy as np import matplotlib.pyplot as plt","title":"Riemann Sums"},{"location":"integration/riemann-sums/#definition","text":"A Riemann sum of a function $f(x)$ over a partition $$ x_0 = a < x_1 < \\cdots < x_{N-1} < x_N = b $$ is a sum of the form $$ \\sum_{i=1}^N f(x_i^ * ) (x_i - x_{i-1}) \\ \\ , \\ x_i^* \\in [x_{i-1},x_i] $$ where each value $x_i^* \\in [x_{i-1},x_i]$ in each subinterval is arbitrary. Riemann sums are important because they provide an easy way to approximate a definite integral $$ \\int_a^b f(x) \\, dx \\approx \\sum_{i=1}^N f(x_i^ * ) (x_i - x_{i-1}) \\ \\ , \\ x_i^* \\in [x_{i-1},x_i] $$ Notice that the product $f(x_i^ * ) (x_i - x_{i-1})$ for each $i$ is the area of a rectangle of height $f(x_i^ * )$ and width $x_i - x_{i-1}$. We can think of a Riemann sum as the area of $N$ rectangles with heights determined by the graph of $y=f(x)$. The value $x_i^*$ chosen in each subinterval is arbitrary however there are certain obvious choices: A left Riemann sum is when each $x_i^* = x_{i-1}$ is the left endpoint of the subinterval $[x_{i-1},x_i]$ A right Riemann sum is when each $x_i^* = x_i$ is the right endpoint of the subinterval $[x_{i-1},x_i]$ A midpoint Riemann sum is when each $x_i^* = (x_{i-1} + x_i)/2$ is the midpoint of the subinterval $[x_{i-1},x_i]$ Let's visualize rectangles in the left, right and midpoint Riemann sums for the function $$ f(x) = \\frac{1}{1 + x^2} $$ over the interval $[0,5]$ with a partition of size $N=10$. f = lambda x : 1/(1+x**2) a = 0; b = 5; N = 10 n = 10 # Use n*N+1 points to plot the function smoothly x = np.linspace(a,b,N+1) y = f(x) X = np.linspace(a,b,n*N+1) Y = f(X) plt.figure(figsize=(15,5)) plt.subplot(1,3,1) plt.plot(X,Y,'b') x_left = x[:-1] # Left endpoints y_left = y[:-1] plt.plot(x_left,y_left,'b.',markersize=10) plt.bar(x_left,y_left,width=(b-a)/N,alpha=0.2,align='edge',edgecolor='b') plt.title('Left Riemann Sum, N = {}'.format(N)) plt.subplot(1,3,2) plt.plot(X,Y,'b') x_mid = (x[:-1] + x[1:])/2 # Midpoints y_mid = f(x_mid) plt.plot(x_mid,y_mid,'b.',markersize=10) plt.bar(x_mid,y_mid,width=(b-a)/N,alpha=0.2,edgecolor='b') plt.title('Midpoint Riemann Sum, N = {}'.format(N)) plt.subplot(1,3,3) plt.plot(X,Y,'b') x_right = x[1:] # Left endpoints y_right = y[1:] plt.plot(x_right,y_right,'b.',markersize=10) plt.bar(x_right,y_right,width=-(b-a)/N,alpha=0.2,align='edge',edgecolor='b') plt.title('Right Riemann Sum, N = {}'.format(N)) plt.show() Notice that when the function $f(x)$ is decreasing on $[a,b]$ the left endpoints give an overestimate of the integral $\\int_a^b f(x) dx$ and right endpoints give an underestimate. The opposite is true is when the function is increasing. Let's compute the value of each of the Riemann sums: dx = (b-a)/N x_left = np.linspace(a,b-dx,N) x_midpoint = np.linspace(dx/2,b - dx/2,N) x_right = np.linspace(dx,b,N) print(\"Partition with\",N,\"subintervals.\") left_riemann_sum = np.sum(f(x_left) * dx) print(\"Left Riemann Sum:\",left_riemann_sum) midpoint_riemann_sum = np.sum(f(x_midpoint) * dx) print(\"Midpoint Riemann Sum:\",midpoint_riemann_sum) right_riemann_sum = np.sum(f(x_right) * dx) print(\"Right Riemann Sum:\",right_riemann_sum) Partition with 10 subintervals. Left Riemann Sum: 1.613488696614725 Midpoint Riemann Sum: 1.373543428316664 Right Riemann Sum: 1.1327194658454942 We know the exact value $$ \\int_0^5 \\frac{1}{1 + x^2} dx = \\arctan(5) $$ and we can compare the Riemann sums to the value I = np.arctan(5) print(I) 1.373400766945016 print(\"Left Riemann Sum Error:\",np.abs(left_riemann_sum - I)) print(\"Midpoint Riemann Sum:\",np.abs(midpoint_riemann_sum - I)) print(\"Right Riemann Sum:\",np.abs(right_riemann_sum - I)) Left Riemann Sum Error: 0.24008792966970915 Midpoint Riemann Sum: 0.00014266137164820059 Right Riemann Sum: 0.24068130109952168","title":"Definition"},{"location":"integration/riemann-sums/#error-formulas","text":"A Riemann sum is an approximation of a definite integral. A natural question arises: how good of an approximation is a Riemann sum? Theorem. Let $L_N(f)$ denote the left Riemann sum $$ L_N(f) = \\sum_{i=1}^N f(x_{i-1} ) \\Delta x $$ where $\\Delta x = (b-a)/N$ and $x_i = a + i \\Delta x$. The error bound is $$ E_N^{L}(f) = \\left| \\ \\int_a^b f(x) \\ dx - L_N(f) \\ \\right| \\leq \\frac{(b-a)^2}{2 N} K_1 $$ where $\\left| \\, f'(x) \\, \\right| \\leq K_1$ for all $x \\in [a,b]$. Theorem. Let $R_N(f)$ denote the right Riemann sum $$ R_N(f) = \\sum_{i=1}^N f(x_{i} ) \\Delta x $$ where $\\Delta x = (b-a)/N$ and $x_i = a + i \\Delta x$. The error bound is $$ E_N^{R}(f) = \\left| \\ \\int_a^b f(x) \\ dx - R_N(f) \\ \\right| \\leq \\frac{(b-a)^2}{2 N} K_1 $$ where $\\left| \\, f'(x) \\, \\right| \\leq K_1$ for all $x \\in [a,b]$. Theorem. Let $M_N(f)$ denote the midpoint Riemann sum $$ M_N(f) = \\sum_{i=1}^N f(x_i^* ) \\Delta x $$ where $\\Delta x = (b-a)/N$ and $x_i^* = (x_{i-1} + x_i)/2$ for $x_i = a + i \\Delta x$. The error bound is $$ E_N^{M}(f) = \\left| \\ \\int_a^b f(x) \\ dx - M_N(f) \\ \\right| \\leq \\frac{(b-a)^3}{24 N^2} K_2 $$ where $\\left| \\, f''(x) \\, \\right| \\leq K_2$ for all $x \\in [a,b]$. There are several points to notice: Left and right Riemann sums have the same error bound which depends on the first derivative $f'(x)$. Midpoint Riemann sum error bound depends on the second derivative $f''(x)$. We expect the midpoint Riemann sum to give a better approximation as $N \\to \\infty$ since its error bound is inversely proportional to $N^2$ but left/right Riemann sum error bound is inversely proportional only to $N$.","title":"Error Formulas"},{"location":"integration/riemann-sums/#implementation","text":"Let's write a function called riemann_sum which takes 5 input parameters f , a , b , N and method and returns the Riemann sum $$ \\sum_{i=1}^N f(x_i^*) \\Delta x $$ where $\\Delta x = (b-a)/N$ and $x_i = a + i\\Delta x$ defines a partition with $N$ subintervals of equal length , and method determines whether we use left endpoints, right endpoints or midpoints (with midpoints as the default method). def riemann_sum(f,a,b,N,method='midpoint'): '''Compute the Riemann sum of f(x) over the interval [a,b]. Parameters ---------- f : function Vectorized function of one variable a , b : numbers Endpoints of the interval [a,b] N : integer Number of subintervals of equal length in the partition of [a,b] method : string Determines the kind of Riemann sum: right : Riemann sum using right endpoints left : Riemann sum using left endpoints midpoint (default) : Riemann sum using midpoints Returns ------- float Approximation of the integral given by the Riemann sum. ''' dx = (b - a)/N x = np.linspace(a,b,N+1) if method == 'left': x_left = x[:-1] return np.sum(f(x_left)*dx) elif method == 'right': x_right = x[1:] return np.sum(f(x_right)*dx) elif method == 'midpoint': x_mid = (x[:-1] + x[1:])/2 return np.sum(f(x_mid)*dx) else: raise ValueError(\"Method must be 'left', 'right' or 'midpoint'.\") Let's test our function with inputs where we know exactly what the output should be. For example, we know $$ \\int_0^{\\pi/2} \\sin(x) \\, dx = 1 $$ and, since $\\sin(x)$ is increasing on $[0,\\pi/2]$, we know that left endpoints will give an under-estimate, and right endpoints will give an over-estimate. riemann_sum(np.sin,0,np.pi/2,100) 1.0000102809119054 riemann_sum(np.sin,0,np.pi/2,100,'right') 1.007833419873582 riemann_sum(np.sin,0,np.pi/2,100,'left') 0.992125456605633 We also know that $\\int_0^1 x \\, dx = 1/2$ and midpoint should give the result exactly for any $N$: riemann_sum(lambda x : x,0,1,1) 0.5","title":"Implementation"},{"location":"integration/riemann-sums/#examples","text":"","title":"Examples"},{"location":"integration/riemann-sums/#approximate-pi","text":"Find a value $N$ which guarantees the right Riemann sum of $f(x)=\\frac{4}{1 + x^2}$ over $[0,1]$ is within $10^{-5}$ of the exact value $$ \\int_0^1 \\frac{4}{1 + x^2} dx = \\pi $$ Compute $$ f'(x) = -\\frac{8x}{(1+x^2)^2} $$ Use brute force optimization to find a bound on $\\left| f'(x) \\right|$ on $[0,1]$: x = np.linspace(0,1,1000) y = np.abs(-8*x/(1 + x**2)**2) np.max(y) 2.5980759093919907 Therefore, $\\left| f'(x) \\right| \\leq 2.6$ for $x \\in [0,1]$. Use the error bound $$ \\frac{(b-a)^2}{2 N} K_1 \\leq 10^{-5} \\ \\Rightarrow \\ \\frac{1.3}{N} \\leq 10^{-5} \\ \\Rightarrow \\ 130000 \\leq N $$ Let's compute the right Riemann sum for $N=130000$: approximation = riemann_sum(lambda x : 4/(1 + x**2),0,1,130000,method='right') print(approximation) 3.1415849612722386 Verify the accuracy of the approximation np.abs(approximation - np.pi) < 10**(-5) True","title":"Approximate Pi"},{"location":"integration/riemann-sums/#approximate-ln2","text":"Find a value $N$ which guarantees the midpoint Riemann sum of $f(x)=\\frac{1}{x}$ over $[1,2]$ is within $10^{-8}$ of the exact value $$ \\int_1^2 \\frac{1}{x} dx = \\ln(2) $$ Compute $$ f''(x) = \\frac{2}{x^3} $$ Since $f''(x)$ is decreasing for all $x>0$ we have $\\left| \\, f''(x) \\, \\right| \\leq 2$ for all $x \\in [1,2]$. Use the error bound: $$ \\frac{(b-a)^3}{24 N^2} K_2 \\leq 10^{-8} \\ \\Rightarrow \\ \\frac{1}{12 N^2} \\leq 10^{-8} \\ \\Rightarrow \\frac{10^4}{\\sqrt{12}} \\leq N $$ 10**4 / np.sqrt(12) 2886.751345948129 Therefore a partition of size $N=2887$ guarantees the desired accuracy: approximation = riemann_sum(lambda x : 1/x,1,2,2887,method='midpoint') print(approximation) 0.6931471768105913 Verify the accuracy of the approximation: np.abs(approximation - np.log(2)) < 10**(-8) True","title":"Approximate ln(2)"},{"location":"integration/riemann-sums/#exercises","text":"Exercise 1. Consider the integral $$ \\int_1^2 \\frac{dx}{1+x^3} $$ Without plotting the functions $f(x)$, $f'(x)$ or $f''(x)$, find a value $N$ such that $E_N^R(f) \\leq 10^{-5}$ given $$ f(x) = \\frac{1}{1 + x^3} \\ , \\ f'(x) = -\\frac{3 x^{2}}{\\left(x^{3} + 1\\right)^{2}} \\ , \\ f''(x) = \\frac{6 x \\left(2 x^{3} - 1\\right)}{\\left(x + 1\\right)^{3} \\left(x^{2} - x + 1\\right)^{3}} \\\\ $$ Exercise 2. Plot the function $f''(x)$ from the previous question on the interval $[1,2]$ and find a value $N$ such that $E_N^M(f) \\leq 10^{-5}$ for the integral in the previous question. Exercise 3. Let $f(x) = x^x$ and note that $$ f'(x) = x^{x} \\left(\\log{\\left(x \\right)} + 1\\right) \\ , \\ f''(x) = x^{x} \\left(\\log{\\left(x \\right)} + 1\\right)^{2} + x^{x-1} $$ Plot the function $f''(x)$ and use that information to compute $T_N(f)$ for the integral $$ \\int_1^2 x^x \\, dx $$ such that $E_N^T(f) \\leq 10^{-3}$.","title":"Exercises"},{"location":"integration/simpsons-rule/","text":"Simpson's Rule import numpy as np import matplotlib.pyplot as plt Definition Simpson's rule uses a quadratic polynomial on each subinterval of a partition to approximate the function $f(x)$ and to compute the definite integral. This is an improvement over the trapezoid rule which approximates $f(x)$ by a straight line on each subinterval of a partition. The formula for Simpson's rule is $$ S_N(f) = \\frac{\\Delta x}{3} \\sum_{i=1}^{N/2} \\left( f(x_{2i-2}) + 4 f(x_{2i-1}) + f(x_{2i}) \\right) $$ where $N$ is an even number of subintervals of $[a,b]$, $\\Delta x = (b - a)/N$ and $x_i = a + i \\Delta x$. Error Formula We have seen that the error in a Riemann sum is inversely proportional to the size of the partition $N$ and the trapezoid rule is inversely proportional to $N^2$. The error formula in the theorem below shows that Simpson's rule is even better as the error is inversely proportional to $N^4$. Theorem Let $S_N(f)$ denote Simpson's rule $$ S_N(f) = \\frac{\\Delta x}{3} \\sum_{i=1}^{N/2} \\left( f(x_{2i-2}) + 4 f(x_{2i-1}) + f(x_{2i}) \\right) $$ where $N$ is an even number of subintervals of $[a,b]$, $\\Delta x = (b - a)/N$ and $x_i = a + i \\Delta x$. The error bound is $$ E_N^S(f) = \\left| \\ \\int_a^b f(x) \\, dx - S_N(f) \\ \\right| \\leq \\frac{(b-a)^5}{180N^4} K_4 $$ where $\\left| \\ f^{(4)}(x) \\ \\right| \\leq K_4$ for all $x \\in [a,b]$. Implementation Let's write a function called simps which takes input parameters $f$, $a$, $b$ and $N$ and returns the approximation $S_N(f)$. Furthermore, let's assign a default value $N=50$. def simps(f,a,b,N=50): '''Approximate the integral of f(x) from a to b by Simpson's rule. Simpson's rule approximates the integral \\int_a^b f(x) dx by the sum: (dx/3) \\sum_{k=1}^{N/2} (f(x_{2i-2} + 4f(x_{2i-1}) + f(x_{2i})) where x_i = a + i*dx and dx = (b - a)/N. Parameters ---------- f : function Vectorized function of a single variable a , b : numbers Interval of integration [a,b] N : (even) integer Number of subintervals of [a,b] Returns ------- float Approximation of the integral of f(x) from a to b using Simpson's rule with N subintervals of equal length. Examples -------- >>> simps(lambda x : 3*x**2,0,1,10) 1.0 ''' if N % 2 == 1: raise ValueError(\"N must be an even integer.\") dx = (b-a)/N x = np.linspace(a,b,N+1) y = f(x) S = dx/3 * np.sum(y[0:-1:2] + 4*y[1::2] + y[2::2]) return S Let's test our function on integrals for which we know the exact value. For example, we know $$ \\int_0^1 3x^2 dx = 1 $$ simps(lambda x : 3*x**2,0,1,10) 1.0 Test our function again with the integral $$ \\int_0^{\\pi/2} \\sin(x) dx = 1 $$ simps(np.sin,0,np.pi/2,100) 1.000000000338236 scipy.integrate.simps The SciPy subpackage scipy.integrate contains several functions for approximating definite integrals and numerically solving differential equations. Let's import the subpackage under the name spi . import scipy.integrate as spi The function scipy.integrate.simps computes the approximation of a definite integral by Simpson's rule. Consulting the documentation, we see that all we need to do it supply arrays of $x$ and $y$ values for the integrand and scipy.integrate.simps returns the approximation of the integral using Simpson's rule. Examples Approximate ln(2) Find a value $N$ which guarantees that Simpson's rule approximation $S_N(f)$ of the integral $$ \\int_1^2 \\frac{1}{x} dx $$ satisfies $E_N^S(f) \\leq 0.0001$. Compute $$ f^{(4)}(x) = \\frac{24}{x^5} $$ therefore $\\left| \\, f^{(4)}(x) \\, \\right| \\leq 24$ for all $x \\in [1,2]$ and so $$ \\frac{1}{180N^4} 24 \\leq 0.0001 \\ \\Rightarrow \\ \\frac{20000}{15N^4} \\leq 1 \\ \\Rightarrow \\ \\left( \\frac{20000}{15} \\right)^{1/4} \\leq N $$ Compute (20000/15)**0.25 6.042750794713537 Compute Simpson's rule with $N=8$ (the smallest even integer greater than 6.04) approximation = simps(lambda x : 1/x,1,2,8) print(approximation) 0.6931545306545306 We could also use the function scipy.integrate.simps to compute the exact same result N = 8; a = 1; b = 2; x = np.linspace(a,b,N+1) y = 1/x approximation = spi.simps(y,x) print(approximation) 0.6931545306545306 Verify that $E_N^S(f) \\leq 0.0001$ np.abs(np.log(2) - approximation) <= 0.0001 True Exercises Under construction","title":"Simpson's Rule"},{"location":"integration/simpsons-rule/#simpsons-rule","text":"import numpy as np import matplotlib.pyplot as plt","title":"Simpson's Rule"},{"location":"integration/simpsons-rule/#definition","text":"Simpson's rule uses a quadratic polynomial on each subinterval of a partition to approximate the function $f(x)$ and to compute the definite integral. This is an improvement over the trapezoid rule which approximates $f(x)$ by a straight line on each subinterval of a partition. The formula for Simpson's rule is $$ S_N(f) = \\frac{\\Delta x}{3} \\sum_{i=1}^{N/2} \\left( f(x_{2i-2}) + 4 f(x_{2i-1}) + f(x_{2i}) \\right) $$ where $N$ is an even number of subintervals of $[a,b]$, $\\Delta x = (b - a)/N$ and $x_i = a + i \\Delta x$.","title":"Definition"},{"location":"integration/simpsons-rule/#error-formula","text":"We have seen that the error in a Riemann sum is inversely proportional to the size of the partition $N$ and the trapezoid rule is inversely proportional to $N^2$. The error formula in the theorem below shows that Simpson's rule is even better as the error is inversely proportional to $N^4$. Theorem Let $S_N(f)$ denote Simpson's rule $$ S_N(f) = \\frac{\\Delta x}{3} \\sum_{i=1}^{N/2} \\left( f(x_{2i-2}) + 4 f(x_{2i-1}) + f(x_{2i}) \\right) $$ where $N$ is an even number of subintervals of $[a,b]$, $\\Delta x = (b - a)/N$ and $x_i = a + i \\Delta x$. The error bound is $$ E_N^S(f) = \\left| \\ \\int_a^b f(x) \\, dx - S_N(f) \\ \\right| \\leq \\frac{(b-a)^5}{180N^4} K_4 $$ where $\\left| \\ f^{(4)}(x) \\ \\right| \\leq K_4$ for all $x \\in [a,b]$.","title":"Error Formula"},{"location":"integration/simpsons-rule/#implementation","text":"Let's write a function called simps which takes input parameters $f$, $a$, $b$ and $N$ and returns the approximation $S_N(f)$. Furthermore, let's assign a default value $N=50$. def simps(f,a,b,N=50): '''Approximate the integral of f(x) from a to b by Simpson's rule. Simpson's rule approximates the integral \\int_a^b f(x) dx by the sum: (dx/3) \\sum_{k=1}^{N/2} (f(x_{2i-2} + 4f(x_{2i-1}) + f(x_{2i})) where x_i = a + i*dx and dx = (b - a)/N. Parameters ---------- f : function Vectorized function of a single variable a , b : numbers Interval of integration [a,b] N : (even) integer Number of subintervals of [a,b] Returns ------- float Approximation of the integral of f(x) from a to b using Simpson's rule with N subintervals of equal length. Examples -------- >>> simps(lambda x : 3*x**2,0,1,10) 1.0 ''' if N % 2 == 1: raise ValueError(\"N must be an even integer.\") dx = (b-a)/N x = np.linspace(a,b,N+1) y = f(x) S = dx/3 * np.sum(y[0:-1:2] + 4*y[1::2] + y[2::2]) return S Let's test our function on integrals for which we know the exact value. For example, we know $$ \\int_0^1 3x^2 dx = 1 $$ simps(lambda x : 3*x**2,0,1,10) 1.0 Test our function again with the integral $$ \\int_0^{\\pi/2} \\sin(x) dx = 1 $$ simps(np.sin,0,np.pi/2,100) 1.000000000338236","title":"Implementation"},{"location":"integration/simpsons-rule/#scipyintegratesimps","text":"The SciPy subpackage scipy.integrate contains several functions for approximating definite integrals and numerically solving differential equations. Let's import the subpackage under the name spi . import scipy.integrate as spi The function scipy.integrate.simps computes the approximation of a definite integral by Simpson's rule. Consulting the documentation, we see that all we need to do it supply arrays of $x$ and $y$ values for the integrand and scipy.integrate.simps returns the approximation of the integral using Simpson's rule.","title":"scipy.integrate.simps"},{"location":"integration/simpsons-rule/#examples","text":"","title":"Examples"},{"location":"integration/simpsons-rule/#approximate-ln2","text":"Find a value $N$ which guarantees that Simpson's rule approximation $S_N(f)$ of the integral $$ \\int_1^2 \\frac{1}{x} dx $$ satisfies $E_N^S(f) \\leq 0.0001$. Compute $$ f^{(4)}(x) = \\frac{24}{x^5} $$ therefore $\\left| \\, f^{(4)}(x) \\, \\right| \\leq 24$ for all $x \\in [1,2]$ and so $$ \\frac{1}{180N^4} 24 \\leq 0.0001 \\ \\Rightarrow \\ \\frac{20000}{15N^4} \\leq 1 \\ \\Rightarrow \\ \\left( \\frac{20000}{15} \\right)^{1/4} \\leq N $$ Compute (20000/15)**0.25 6.042750794713537 Compute Simpson's rule with $N=8$ (the smallest even integer greater than 6.04) approximation = simps(lambda x : 1/x,1,2,8) print(approximation) 0.6931545306545306 We could also use the function scipy.integrate.simps to compute the exact same result N = 8; a = 1; b = 2; x = np.linspace(a,b,N+1) y = 1/x approximation = spi.simps(y,x) print(approximation) 0.6931545306545306 Verify that $E_N^S(f) \\leq 0.0001$ np.abs(np.log(2) - approximation) <= 0.0001 True","title":"Approximate ln(2)"},{"location":"integration/simpsons-rule/#exercises","text":"Under construction","title":"Exercises"},{"location":"integration/trapezoid-rule/","text":"Trapezoid Rule import numpy as np import matplotlib.pyplot as plt Trapezoids The definite integral of $f(x)$ is equal to the (net) area under the curve $y=f(x)$ over the interval $[a,b]$. Riemann sums approximate definite integrals by using sums of rectangles to approximate the area. The trapezoid rule gives a better approximation of a definite integral by summing the areas of the trapezoids connecting the points $$ (x_{i-1},0), (x_i,0), (x_{i-1},f(x_{i-1})), (x_i,f(x_i)) $$ for each subinterval $[x_{i-1},x_i]$ of a partition. Note that the area of each trapezoid is the sum of a rectangle and a triangle $$ (x_i - x_{i-1}) f(x_{i-1}) + \\frac{1}{2}(x_i - x_{i-1}) (f(x_i) - f(x_{i-1})) = \\frac{1}{2}(f(x_i) + f(x_{i-1}))(x_i - x_{i-1}) $$ For example, we can use a single trapezoid to approximate: $$ \\int_0^1 e^{-x^2} \\, dx $$ First, let's plot the curve $y = e^{-x^2}$ and the trapezoid on the interval $[0,1]$: x = np.linspace(-0.5,1.5,100) y = np.exp(-x**2) plt.plot(x,y) x0 = 0; x1 = 1; y0 = np.exp(-x0**2); y1 = np.exp(-x1**2); plt.fill_between([x0,x1],[y0,y1]) plt.xlim([-0.5,1.5]); plt.ylim([0,1.5]); plt.show() Approximate the integral by the area of the trapezoid: A = 0.5*(y1 + y0)*(x1 - x0) print(\"Trapezoid area:\", A) Trapezoid area: 0.6839397205857212 Definition The trapezoid rule for $N$ subintervals of $[a,b]$ of equal length is $$ T_N(f) = \\frac{\\Delta x}{2} \\sum_{i=1}^N (f(x_i) + f(x_{i-1})) $$ where $\\Delta x = (b - a)/N$ is the length of the subintervals and $x_i = a + i \\Delta x$. Notice that the trapezoid is the average of the left and right Riemann sums $$ T_N(f) = \\frac{\\Delta x}{2} \\sum_{i=1}^N (f(x_i) + f(x_{i-1})) = \\frac{1}{2} \\left( \\sum_{i=1}^N f(x_i) \\Delta x + \\sum_{i=1}^N f(x_{i-1}) \\Delta x \\right) $$ Error Formula When computing integrals numerically, it is essential to know how good our approximations are. Notice in the theorem below that the error formula is inversely proportional to $N^2$. This means that the error decreases much faster with larger $N$ compared to Riemann sums. Theorem. Let $T_N(f)$ denote the trapezoid rule $$ T_N(f) = \\frac{\\Delta x}{2} \\sum_{i=1}^N (f(x_i) + f(x_{i-1})) $$ where $\\Delta x = (b-a)/N$ and $x_i = a + i \\Delta x$. The error bound is $$ E_N^T(f) = \\left| \\ \\int_a^b f(x) \\ dx - T_N(f) \\ \\right| \\leq \\frac{(b-a)^3}{12 N^2} K_2 $$ where $\\left| \\ f''(x) \\, \\right| \\leq K_2$ for all $x \\in [a,b]$. Implementation Let's write a function called trapz which takes input parameters $f$, $a$, $b$ and $N$ and returns the approximation $T_N(f)$. Furthermore, let's assign default value $N=50$. def trapz(f,a,b,N=50): '''Approximate the integral of f(x) from a to b by the trapezoid rule. The trapezoid rule approximates the integral \\int_a^b f(x) dx by the sum: (dx/2) \\sum_{k=1}^N (f(x_k) + f(x_{k-1})) where x_k = a + k*dx and dx = (b - a)/N. Parameters ---------- f : function Vectorized function of a single variable a , b : numbers Interval of integration [a,b] N : integer Number of subintervals of [a,b] Returns ------- float Approximation of the integral of f(x) from a to b using the trapezoid rule with N subintervals of equal length. Examples -------- >>> trapz(np.sin,0,np.pi/2,1000) 0.9999997943832332 ''' x = np.linspace(a,b,N+1) # N+1 points make N subintervals y = f(x) y_right = y[1:] # right endpoints y_left = y[:-1] # left endpoints dx = (b - a)/N T = (dx/2) * np.sum(y_right + y_left) return T Let's test our function on an integral where we know the answer $$ \\int_0^{\\pi/2} \\sin x \\ dx = 1 $$ trapz(np.sin,0,np.pi/2,1000) 0.9999997943832332 Let's test our function again: $$ \\int_0^1 3 x^2 \\ dx = 1 $$ trapz(lambda x : 3*x**2,0,1,10000) 1.0000000050000002 And once more: $$ \\int_0^1 x \\ dx = \\frac{1}{2} $$ trapz(lambda x : x,0,1,1) 0.5 scipy.integrate.trapz The SciPy subpackage scipy.integrate contains several functions for approximating definite integrals and numerically solving differential equations. Let's import the subpackage under the name spi . import scipy.integrate as spi The function scipy.integrate.trapz computes the approximation of a definite by the trapezoid rule. Consulting the documentation, we see that all we need to do it supply arrays of $x$ and $y$ values for the integrand and scipy.integrate.trapz returns the approximation of the integral using the trapezoid rule. The number of points we give to scipy.integrate.trapz is up to us but we have to remember that more points gives a better approximation but it takes more time to compute! Examples Arctangent Let's plot the trapezoids for $\\displaystyle f(x)=\\frac{1}{1 + x^2}$ on $[0,5]$ with $N=10$. f = lambda x : 1/(1 + x**2) a = 0; b = 5; N = 10 # x and y values for the trapezoid rule x = np.linspace(a,b,N+1) y = f(x) # X and Y values for plotting y=f(x) X = np.linspace(a,b,100) Y = f(X) plt.plot(X,Y) for i in range(N): xs = [x[i],x[i],x[i+1],x[i+1]] ys = [0,f(x[i]),f(x[i+1]),0] plt.fill(xs,ys,'b',edgecolor='b',alpha=0.2) plt.title('Trapezoid Rule, N = {}'.format(N)) plt.show() Let's compute the sum of areas of the trapezoids: T = trapz(f,a,b,N) print(T) 1.3731040812301096 We know the exact value $$ \\int_0^5 \\frac{1}{1 + x^2} dx = \\arctan(5) $$ and we can compare the trapezoid rule to the value I = np.arctan(5) print(I) 1.373400766945016 print(\"Trapezoid Rule Error:\",np.abs(I - T)) Trapezoid Rule Error: 0.00029668571490626405 Approximate ln(2) Find a value $N$ which guarantees that the trapezoid rule approximation $T_N(f)$ of the integral $$ \\int_1^2 \\frac{1}{x} \\, dx = \\ln(2) $$ satisfies $E_N^T(f) \\leq 10^{-8}$. For $f(x) = \\frac{1}{x}$, we compute $f''(x) = \\frac{2}{x^3} \\leq 2$ for all $x \\in [1,2]$ therefore the error formula implies $$ \\left| \\, \\int_1^2 \\frac{1}{x} \\, dx - T_N(f) \\, \\right| \\leq \\frac{2}{12N^2} $$ Then $E_N^T \\leq 10^{-8}$ is guaranteed if $\\frac{1}{6N^2} \\leq 10^{-8}$ which implies $$ \\frac{10^4}{\\sqrt{6}} \\leq N $$ 10**4/np.sqrt(6) 4082.4829046386303 We need 4083 subintervals to guarantee $E_N^T(f) \\leq 10^{-8}$. Compute the approximation using our own implementation of the trapezoid rule: approximation = trapz(lambda x : 1/x,1,2,4083) print(approximation) 0.6931471843089954 We could also use scipy.integrate.trapz to get the exact same result: N = 4083 x = np.linspace(1,2,N+1) y = 1/x approximation = spi.trapz(y,x) print(approximation) 0.6931471843089955 Let's verify that this is within $10^{-6}$: np.abs(approximation - np.log(2)) < 10**(-8) True Success! However, a natural question arises: what is the actual smallest $N$ such that the trapezoid rule gives the estimate of $\\ln (2)$ to within $10^{-8}$? for n in range(1,4083): approx = trapz(lambda x : 1/x,1,2,n) if np.abs(approx - np.log(2)) < 10e-8: print(\"Accuracy achieved at N =\",n) break Accuracy achieved at N = 791 Fresnel Integral Fresnel integrals are examples of nonelementary integrals : antiderivatives which cannot be written in terms of elementary functions . There are two types of Fresnel integrals: $$ S(t) = \\int_0^t \\sin(x^2) dx \\ \\ \\text{and} \\ \\ C(t) = \\int_0^t \\cos(x^2) dx $$ Use the trapezoid rule to approximate the Fresnel integral $$ S(1) = \\int_0^1 \\sin(x^2) dx $$ such that the error is less than $10^{-5}$. Compute the derivatives of the integrand $$ f(x) = \\sin(x^2) \\ \\ , \\ \\ f'(x) = 2x\\cos(x^2) $$ $$ f''(x) = 2\\cos(x^2) - 4x^2\\sin(x^2) \\ \\ , \\ \\ f'''(x) = -12x\\sin(x^2) - 8x^3\\cos(x^2) $$ Since $f'''(x) \\leq 0$ for $x \\in [0,1]$, we see that $f''(x)$ is decreasing on $[0,1]$. Values of $f''(x)$ at the endpoints of the interval are x = 0 2*np.cos(x**2) - 4*x**2*np.sin(x**2) 2.0 x = 1 2*np.cos(x**2) - 4*x**2*np.sin(x**2) -2.2852793274953065 Therefore $\\left| \\, f''(x) \\, \\right| \\leq 2.2852793274953065$ for $x \\in [0,1]$. Use the error bound formula to find a good choice for $N$ $$ \\frac{(b-a)^3}{12 N^2} K_2 \\leq 10^{-5} \\Rightarrow \\sqrt{\\frac{10^5(2.2852793274953065)}{12}} \\leq N $$ np.sqrt(10**5 * 2.2852793274953065 / 12) 137.9999796949051 Let's compute the integral using the trapezoid rule with $N=138$ subintervals x = np.linspace(0,1,139) y = np.sin(x**2) I = spi.trapz(y,x) print(I) 0.31027303032220394 Therefore the Fresnel integral $S(1)$ is approximately $$ S(1) = \\int_0^1 \\sin(x^2) \\, dx \\approx 0.310273030322 $$ with error less than $10^{-5}$. Logarithmic Integral The Eulerian logarithmic integral is another nonelementary integral $$ \\mathrm{Li}(t) = \\int_2^t \\frac{1}{\\ln x} dx $$ Let's compute $Li(10)$ such that the error is less than $10^{-4}$. Compute derivatives of the integrand $$ f(x) = \\frac{1}{\\ln x} \\ \\ , \\ \\ f'(x) = -\\frac{1}{x(\\ln x)^2} \\ \\ , \\ \\ f''(x) = \\frac{\\ln x + 2 }{x^2(\\ln x)^3} $$ Plot $f''(x)$ on the interval $[2,10]$. a = 2 b = 10 x = np.linspace(a,b,100) y = (np.log(x) + 2) / (x**2 * np.log(x)**3) plt.plot(x,y) plt.show() Clearly $f''(x)$ is decreasing on $[2,10]$ (and bounded below by 0) therefore the absolute maximum occurs at the left endpoint: $$ \\left| \\, f''(x) \\, \\right| \\leq \\frac{\\ln (2) + 2}{4 \\ln (2)^3} $$ for $x \\in [2,10]$ and we compute K2 = (np.log(2) + 2)/(4*np.log(2)**3) print(K2) 2.021732598829855 Use the error formula: $$ \\frac{(b-a)^3}{12 N^2} K_2 \\leq 10^{-4} \\Rightarrow \\frac{8^3}{12 N^2} 2.021732598829855 \\leq 10^{-4} \\Rightarrow \\sqrt{ \\frac{8^3 10^4}{12} 2.021732598829855} \\leq N $$ np.sqrt(8**3 * 10**4 * 2.021732598829855 / 12) 928.7657986995814 Compute the trapzoid rule with $N=929$ N = 929 x = np.linspace(a,b,N+1) y = 1/np.log(x) I = spi.trapz(y,x) print(I) 5.120442039184057 Therefore the Eulerian logarithmic integral is $$ \\mathrm{Li}(10) = \\int_2^{10} \\frac{1}{\\ln x} dx \\approx 5.121065367200469 $$ such that the error is less than $10^{-4}$. Exercises Exercise 1. Let $f(x) = x^x$ and note that $$ f'(x) = x^{x} \\left(\\log{\\left(x \\right)} + 1\\right) \\ , \\ f''(x) = x^{x} \\left(\\log{\\left(x \\right)} + 1\\right)^{2} + x^{x-1} $$ Plot the function $f''(x)$ and use that information to compute $T_N(f)$ for the integral $$ \\int_1^2 x^x \\, dx $$ such that $E_N^T(f) \\leq 10^{-3}$. Exercise 2. Consider the integral $$ \\int_0^1 \\ln(1+x^2) \\, dx $$ and note that $$ f(x) = \\ln(1 + x^2) \\hspace{1in} f'(x) = \\frac{2x}{1 + x^2} $$ $$ f''(x) = 2 \\left( \\frac{1 - x^2}{1 + x^2} \\right) \\hspace{1in} f'''(x) = 4x \\frac{x^2 - 3}{(x^2 + 1)^3} $$ Without plotting the functions $f(x)$, $f'(x)$, $f''(x)$ or $f'''(x)$, find a value $N$ such that $E_N^T(f) \\leq 10^{-6}$.","title":"Trapezoid Rule"},{"location":"integration/trapezoid-rule/#trapezoid-rule","text":"import numpy as np import matplotlib.pyplot as plt","title":"Trapezoid Rule"},{"location":"integration/trapezoid-rule/#trapezoids","text":"The definite integral of $f(x)$ is equal to the (net) area under the curve $y=f(x)$ over the interval $[a,b]$. Riemann sums approximate definite integrals by using sums of rectangles to approximate the area. The trapezoid rule gives a better approximation of a definite integral by summing the areas of the trapezoids connecting the points $$ (x_{i-1},0), (x_i,0), (x_{i-1},f(x_{i-1})), (x_i,f(x_i)) $$ for each subinterval $[x_{i-1},x_i]$ of a partition. Note that the area of each trapezoid is the sum of a rectangle and a triangle $$ (x_i - x_{i-1}) f(x_{i-1}) + \\frac{1}{2}(x_i - x_{i-1}) (f(x_i) - f(x_{i-1})) = \\frac{1}{2}(f(x_i) + f(x_{i-1}))(x_i - x_{i-1}) $$ For example, we can use a single trapezoid to approximate: $$ \\int_0^1 e^{-x^2} \\, dx $$ First, let's plot the curve $y = e^{-x^2}$ and the trapezoid on the interval $[0,1]$: x = np.linspace(-0.5,1.5,100) y = np.exp(-x**2) plt.plot(x,y) x0 = 0; x1 = 1; y0 = np.exp(-x0**2); y1 = np.exp(-x1**2); plt.fill_between([x0,x1],[y0,y1]) plt.xlim([-0.5,1.5]); plt.ylim([0,1.5]); plt.show() Approximate the integral by the area of the trapezoid: A = 0.5*(y1 + y0)*(x1 - x0) print(\"Trapezoid area:\", A) Trapezoid area: 0.6839397205857212","title":"Trapezoids"},{"location":"integration/trapezoid-rule/#definition","text":"The trapezoid rule for $N$ subintervals of $[a,b]$ of equal length is $$ T_N(f) = \\frac{\\Delta x}{2} \\sum_{i=1}^N (f(x_i) + f(x_{i-1})) $$ where $\\Delta x = (b - a)/N$ is the length of the subintervals and $x_i = a + i \\Delta x$. Notice that the trapezoid is the average of the left and right Riemann sums $$ T_N(f) = \\frac{\\Delta x}{2} \\sum_{i=1}^N (f(x_i) + f(x_{i-1})) = \\frac{1}{2} \\left( \\sum_{i=1}^N f(x_i) \\Delta x + \\sum_{i=1}^N f(x_{i-1}) \\Delta x \\right) $$","title":"Definition"},{"location":"integration/trapezoid-rule/#error-formula","text":"When computing integrals numerically, it is essential to know how good our approximations are. Notice in the theorem below that the error formula is inversely proportional to $N^2$. This means that the error decreases much faster with larger $N$ compared to Riemann sums. Theorem. Let $T_N(f)$ denote the trapezoid rule $$ T_N(f) = \\frac{\\Delta x}{2} \\sum_{i=1}^N (f(x_i) + f(x_{i-1})) $$ where $\\Delta x = (b-a)/N$ and $x_i = a + i \\Delta x$. The error bound is $$ E_N^T(f) = \\left| \\ \\int_a^b f(x) \\ dx - T_N(f) \\ \\right| \\leq \\frac{(b-a)^3}{12 N^2} K_2 $$ where $\\left| \\ f''(x) \\, \\right| \\leq K_2$ for all $x \\in [a,b]$.","title":"Error Formula"},{"location":"integration/trapezoid-rule/#implementation","text":"Let's write a function called trapz which takes input parameters $f$, $a$, $b$ and $N$ and returns the approximation $T_N(f)$. Furthermore, let's assign default value $N=50$. def trapz(f,a,b,N=50): '''Approximate the integral of f(x) from a to b by the trapezoid rule. The trapezoid rule approximates the integral \\int_a^b f(x) dx by the sum: (dx/2) \\sum_{k=1}^N (f(x_k) + f(x_{k-1})) where x_k = a + k*dx and dx = (b - a)/N. Parameters ---------- f : function Vectorized function of a single variable a , b : numbers Interval of integration [a,b] N : integer Number of subintervals of [a,b] Returns ------- float Approximation of the integral of f(x) from a to b using the trapezoid rule with N subintervals of equal length. Examples -------- >>> trapz(np.sin,0,np.pi/2,1000) 0.9999997943832332 ''' x = np.linspace(a,b,N+1) # N+1 points make N subintervals y = f(x) y_right = y[1:] # right endpoints y_left = y[:-1] # left endpoints dx = (b - a)/N T = (dx/2) * np.sum(y_right + y_left) return T Let's test our function on an integral where we know the answer $$ \\int_0^{\\pi/2} \\sin x \\ dx = 1 $$ trapz(np.sin,0,np.pi/2,1000) 0.9999997943832332 Let's test our function again: $$ \\int_0^1 3 x^2 \\ dx = 1 $$ trapz(lambda x : 3*x**2,0,1,10000) 1.0000000050000002 And once more: $$ \\int_0^1 x \\ dx = \\frac{1}{2} $$ trapz(lambda x : x,0,1,1) 0.5","title":"Implementation"},{"location":"integration/trapezoid-rule/#scipyintegratetrapz","text":"The SciPy subpackage scipy.integrate contains several functions for approximating definite integrals and numerically solving differential equations. Let's import the subpackage under the name spi . import scipy.integrate as spi The function scipy.integrate.trapz computes the approximation of a definite by the trapezoid rule. Consulting the documentation, we see that all we need to do it supply arrays of $x$ and $y$ values for the integrand and scipy.integrate.trapz returns the approximation of the integral using the trapezoid rule. The number of points we give to scipy.integrate.trapz is up to us but we have to remember that more points gives a better approximation but it takes more time to compute!","title":"scipy.integrate.trapz"},{"location":"integration/trapezoid-rule/#examples","text":"","title":"Examples"},{"location":"integration/trapezoid-rule/#arctangent","text":"Let's plot the trapezoids for $\\displaystyle f(x)=\\frac{1}{1 + x^2}$ on $[0,5]$ with $N=10$. f = lambda x : 1/(1 + x**2) a = 0; b = 5; N = 10 # x and y values for the trapezoid rule x = np.linspace(a,b,N+1) y = f(x) # X and Y values for plotting y=f(x) X = np.linspace(a,b,100) Y = f(X) plt.plot(X,Y) for i in range(N): xs = [x[i],x[i],x[i+1],x[i+1]] ys = [0,f(x[i]),f(x[i+1]),0] plt.fill(xs,ys,'b',edgecolor='b',alpha=0.2) plt.title('Trapezoid Rule, N = {}'.format(N)) plt.show() Let's compute the sum of areas of the trapezoids: T = trapz(f,a,b,N) print(T) 1.3731040812301096 We know the exact value $$ \\int_0^5 \\frac{1}{1 + x^2} dx = \\arctan(5) $$ and we can compare the trapezoid rule to the value I = np.arctan(5) print(I) 1.373400766945016 print(\"Trapezoid Rule Error:\",np.abs(I - T)) Trapezoid Rule Error: 0.00029668571490626405","title":"Arctangent"},{"location":"integration/trapezoid-rule/#approximate-ln2","text":"Find a value $N$ which guarantees that the trapezoid rule approximation $T_N(f)$ of the integral $$ \\int_1^2 \\frac{1}{x} \\, dx = \\ln(2) $$ satisfies $E_N^T(f) \\leq 10^{-8}$. For $f(x) = \\frac{1}{x}$, we compute $f''(x) = \\frac{2}{x^3} \\leq 2$ for all $x \\in [1,2]$ therefore the error formula implies $$ \\left| \\, \\int_1^2 \\frac{1}{x} \\, dx - T_N(f) \\, \\right| \\leq \\frac{2}{12N^2} $$ Then $E_N^T \\leq 10^{-8}$ is guaranteed if $\\frac{1}{6N^2} \\leq 10^{-8}$ which implies $$ \\frac{10^4}{\\sqrt{6}} \\leq N $$ 10**4/np.sqrt(6) 4082.4829046386303 We need 4083 subintervals to guarantee $E_N^T(f) \\leq 10^{-8}$. Compute the approximation using our own implementation of the trapezoid rule: approximation = trapz(lambda x : 1/x,1,2,4083) print(approximation) 0.6931471843089954 We could also use scipy.integrate.trapz to get the exact same result: N = 4083 x = np.linspace(1,2,N+1) y = 1/x approximation = spi.trapz(y,x) print(approximation) 0.6931471843089955 Let's verify that this is within $10^{-6}$: np.abs(approximation - np.log(2)) < 10**(-8) True Success! However, a natural question arises: what is the actual smallest $N$ such that the trapezoid rule gives the estimate of $\\ln (2)$ to within $10^{-8}$? for n in range(1,4083): approx = trapz(lambda x : 1/x,1,2,n) if np.abs(approx - np.log(2)) < 10e-8: print(\"Accuracy achieved at N =\",n) break Accuracy achieved at N = 791","title":"Approximate ln(2)"},{"location":"integration/trapezoid-rule/#fresnel-integral","text":"Fresnel integrals are examples of nonelementary integrals : antiderivatives which cannot be written in terms of elementary functions . There are two types of Fresnel integrals: $$ S(t) = \\int_0^t \\sin(x^2) dx \\ \\ \\text{and} \\ \\ C(t) = \\int_0^t \\cos(x^2) dx $$ Use the trapezoid rule to approximate the Fresnel integral $$ S(1) = \\int_0^1 \\sin(x^2) dx $$ such that the error is less than $10^{-5}$. Compute the derivatives of the integrand $$ f(x) = \\sin(x^2) \\ \\ , \\ \\ f'(x) = 2x\\cos(x^2) $$ $$ f''(x) = 2\\cos(x^2) - 4x^2\\sin(x^2) \\ \\ , \\ \\ f'''(x) = -12x\\sin(x^2) - 8x^3\\cos(x^2) $$ Since $f'''(x) \\leq 0$ for $x \\in [0,1]$, we see that $f''(x)$ is decreasing on $[0,1]$. Values of $f''(x)$ at the endpoints of the interval are x = 0 2*np.cos(x**2) - 4*x**2*np.sin(x**2) 2.0 x = 1 2*np.cos(x**2) - 4*x**2*np.sin(x**2) -2.2852793274953065 Therefore $\\left| \\, f''(x) \\, \\right| \\leq 2.2852793274953065$ for $x \\in [0,1]$. Use the error bound formula to find a good choice for $N$ $$ \\frac{(b-a)^3}{12 N^2} K_2 \\leq 10^{-5} \\Rightarrow \\sqrt{\\frac{10^5(2.2852793274953065)}{12}} \\leq N $$ np.sqrt(10**5 * 2.2852793274953065 / 12) 137.9999796949051 Let's compute the integral using the trapezoid rule with $N=138$ subintervals x = np.linspace(0,1,139) y = np.sin(x**2) I = spi.trapz(y,x) print(I) 0.31027303032220394 Therefore the Fresnel integral $S(1)$ is approximately $$ S(1) = \\int_0^1 \\sin(x^2) \\, dx \\approx 0.310273030322 $$ with error less than $10^{-5}$.","title":"Fresnel Integral"},{"location":"integration/trapezoid-rule/#logarithmic-integral","text":"The Eulerian logarithmic integral is another nonelementary integral $$ \\mathrm{Li}(t) = \\int_2^t \\frac{1}{\\ln x} dx $$ Let's compute $Li(10)$ such that the error is less than $10^{-4}$. Compute derivatives of the integrand $$ f(x) = \\frac{1}{\\ln x} \\ \\ , \\ \\ f'(x) = -\\frac{1}{x(\\ln x)^2} \\ \\ , \\ \\ f''(x) = \\frac{\\ln x + 2 }{x^2(\\ln x)^3} $$ Plot $f''(x)$ on the interval $[2,10]$. a = 2 b = 10 x = np.linspace(a,b,100) y = (np.log(x) + 2) / (x**2 * np.log(x)**3) plt.plot(x,y) plt.show() Clearly $f''(x)$ is decreasing on $[2,10]$ (and bounded below by 0) therefore the absolute maximum occurs at the left endpoint: $$ \\left| \\, f''(x) \\, \\right| \\leq \\frac{\\ln (2) + 2}{4 \\ln (2)^3} $$ for $x \\in [2,10]$ and we compute K2 = (np.log(2) + 2)/(4*np.log(2)**3) print(K2) 2.021732598829855 Use the error formula: $$ \\frac{(b-a)^3}{12 N^2} K_2 \\leq 10^{-4} \\Rightarrow \\frac{8^3}{12 N^2} 2.021732598829855 \\leq 10^{-4} \\Rightarrow \\sqrt{ \\frac{8^3 10^4}{12} 2.021732598829855} \\leq N $$ np.sqrt(8**3 * 10**4 * 2.021732598829855 / 12) 928.7657986995814 Compute the trapzoid rule with $N=929$ N = 929 x = np.linspace(a,b,N+1) y = 1/np.log(x) I = spi.trapz(y,x) print(I) 5.120442039184057 Therefore the Eulerian logarithmic integral is $$ \\mathrm{Li}(10) = \\int_2^{10} \\frac{1}{\\ln x} dx \\approx 5.121065367200469 $$ such that the error is less than $10^{-4}$.","title":"Logarithmic Integral"},{"location":"integration/trapezoid-rule/#exercises","text":"Exercise 1. Let $f(x) = x^x$ and note that $$ f'(x) = x^{x} \\left(\\log{\\left(x \\right)} + 1\\right) \\ , \\ f''(x) = x^{x} \\left(\\log{\\left(x \\right)} + 1\\right)^{2} + x^{x-1} $$ Plot the function $f''(x)$ and use that information to compute $T_N(f)$ for the integral $$ \\int_1^2 x^x \\, dx $$ such that $E_N^T(f) \\leq 10^{-3}$. Exercise 2. Consider the integral $$ \\int_0^1 \\ln(1+x^2) \\, dx $$ and note that $$ f(x) = \\ln(1 + x^2) \\hspace{1in} f'(x) = \\frac{2x}{1 + x^2} $$ $$ f''(x) = 2 \\left( \\frac{1 - x^2}{1 + x^2} \\right) \\hspace{1in} f'''(x) = 4x \\frac{x^2 - 3}{(x^2 + 1)^3} $$ Without plotting the functions $f(x)$, $f'(x)$, $f''(x)$ or $f'''(x)$, find a value $N$ such that $E_N^T(f) \\leq 10^{-6}$.","title":"Exercises"},{"location":"jupyter/latex/","text":"LaTeX LaTeX is a programming environment for producing scientific documents. Jupyter notebook recognizes LaTeX code written in markdown cells and renders the mathematical symbols in the browser using the MathJax JavaScript library. Mathematics Inline and Display Enclose LaTeX code in dollar signs $ ... $ to display math inline. For example, the code $\\int_a^b f(x) = F(b) - F(a)$ renders inline as $ \\int_a^b f(x) dx = F(b) - F(a) $. Enclose LaTeX code in double dollar signs $$ ... $$ to display expressions in a centered paragraph. For example: $$f'(a) = \\lim_{x \\to a} \\frac{f(x) - f(a)}{x-a}$$ renders as $$f'(a) = \\lim_{x \\to a} \\frac{f(x) - f(a)}{x-a}$$ See the LaTeX WikiBook for more information (especially the section on mathematics ). Common Symbols Below we give a list of commonly used mathematical symbols. Most other symbols can be inferred from these examples. See the LaTeX WikiBook (Mathematics) and the Detexify App to find any symbol you can think of! Syntax Output $x_n$ $x_n$ $x^2$ $x^2$ $\\infty$ $\\infty$ $\\frac{a}{b}$ $\\frac{a}{b}$ $\\partial$ $\\partial$ $\\alpha$ $\\alpha$ $\\beta$ $\\beta$ $\\gamma$ $\\gamma$ $\\Gamma$ $\\Gamma$ $\\Delta$ $\\Delta$ $\\sin$ $\\sin$ $\\cos$ $\\cos$ $\\tan$ $\\tan$ $\\sum_{n=0}^{\\infty}$ $\\sum_{n=0}^{\\infty}$ $\\prod_{n=0}^{\\infty}$ $\\prod_{n=0}^{\\infty}$ $\\int_a^b$ $\\int_a^b$ $\\lim_{x \\to a}$ $\\lim_{x \\to a}$ $\\mathrm{Hom}$ $\\mathrm{Hom}$ $\\mathbf{v}$ $\\mathbf{v}$ $\\mathbb{Z}$ $\\mathbb{Z}$ $\\mathscr{L}$ $\\mathscr{L}$ $\\mathfrak{g}$ $\\mathfrak{g}$ $\\dots$ $\\dots$ $\\vdots$ $\\vdots$ $\\ddots$ $\\ddots$ Matrices and Brackets Create a matrix without brackets: $$\\begin{matrix} a & b \\\\ c & d \\end{matrix}$$ $$ \\begin{matrix} a & b \\\\ c & d \\end{matrix} $$ Create a matrix with round brackets: $$\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$$ $$ \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} $$ Create a matrix with square brackets: $$\\begin{bmatrix} 1 & 2 & 1 \\\\ 3 & 0 & 1 \\\\ 0 & 2 & 4 \\end{bmatrix}$$ $$ \\begin{bmatrix} 1 & 2 & 1 \\\\ 3 & 0 & 1 \\\\ 0 & 2 & 4 \\end{bmatrix} $$ Use \\left and \\right to enclose any expression in brackets: $$\\left( \\frac{p}{q} \\right)$$ $$\\left( \\frac{p}{q} \\right)$$ Examples Derivative The derivative $f'(a)$ of the function $f(x)$ at the point $x=a$ is the limit: $$f'(a) = \\lim_{x \\to a} \\frac{f(x) - f(a)}{x - a}$$ $$f'(a) = \\lim_{x \\to a} \\frac{f(x) - f(a)}{x - a}$$ Continuity A function $f(x)$ is continuous at a point $x=a$ if: $$\\lim_{x \\to a^-} f(x) = f(a) = \\lim_{x \\to a^+} f(x)$$ $$\\lim_{x \\to a^-} f(x) = f(a) = \\lim_{x \\to a^+} f(x)$$ MacLaurin Series The MacLaurin series for $e^x$ is: $$e^x = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!}$$ $$e^x = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!}$$ Jacobian Matrix The Jacobian matrix of the function $\\mathbf{f}(x_1, \\dots, x_n)$ is: $$ \\mathbf{J} = \\frac{d \\mathbf{f}}{d \\mathbf{x}} = \\left[ \\frac{\\partial \\mathbf{f}}{\\partial x_1} \\cdots \\frac{\\partial \\mathbf{f}}{\\partial x_n} \\right] = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$ $$ \\mathbf{J} = \\frac{d \\mathbf{f}}{d \\mathbf{x}} = \\left[ \\frac{\\partial \\mathbf{f}}{\\partial x_1} \\cdots \\frac{\\partial \\mathbf{f}}{\\partial x_n} \\right] = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$ Exercises Exercise 1 . Write LaTeX code to display the angle sum identity $$\\cos(\\alpha \\pm \\beta) = \\cos \\alpha \\cos \\beta \\mp \\sin \\alpha \\sin \\beta$$ Exercise 2. Write LaTeX code to display the indefinite integral $$\\int \\frac{1}{1 + x^2} \\, dx = \\arctan x + C$$ Exercise 3. Write LaTeX code to display the Navier-Stokes Equation for Incompressible Flow $$\\frac{\\partial \\mathbf{u}}{\\partial t} + (\\mathbf{u} \\cdot \\nabla) \\mathbf{u} - \\nu \\nabla^2 \\mathbf{u} = - \\nabla w + \\mathbf{g}$$ Exercise 4. Write LaTeX code to display Green's Theorem $$\\oint_C (L dx + M dy) = \\iint_D \\left( \\frac{\\partial M}{\\partial x} - \\frac{\\partial L}{\\partial y} \\right) dx \\, dy$$ Exercise 5. Write LaTeX code to display the Prime Number Theorem $$\\lim_{x \\to \\infty} \\frac{\\pi(x)}{ \\frac{x}{\\log(x)}} = 1$$ Exercise 6. Write LaTeX code to display the general formula for Taylor series $$\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n!} (x-a)^n$$ Exercise 7. Write LaTeX code to display Stokes' Theorem $$\\int_{\\partial \\Omega} \\omega = \\int_{\\Omega} d \\omega$$ Exercise 8. Write LaTeX code to display the adjoint property of the tensor product $$\\mathrm{Hom}(U \\otimes V,W) \\cong \\mathrm{Hom}(U, \\mathrm{Hom}(V,W))$$ Exercise 9. Write LaTeX code to display the definition of the Laplace transform $$\\mathscr{L} \\{ f(t) \\} = F(s) = \\int_0^{\\infty} f(t) e^{-st} dt$$ Exercise 10. Write LaTeX code to display the inverse matrix formula $$\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}$$ Exercise 11. Write LaTeX code to display the infinite product formula $$\\sin x = x \\prod_{n=1}^{\\infty} \\left( 1 - \\frac{x^2}{\\pi^2 n^2} \\right)$$ Exercise 12. Pick your favourite math course and write the notes from your last class in LaTeX.","title":"LaTeX"},{"location":"jupyter/latex/#latex","text":"LaTeX is a programming environment for producing scientific documents. Jupyter notebook recognizes LaTeX code written in markdown cells and renders the mathematical symbols in the browser using the MathJax JavaScript library.","title":"LaTeX"},{"location":"jupyter/latex/#mathematics-inline-and-display","text":"Enclose LaTeX code in dollar signs $ ... $ to display math inline. For example, the code $\\int_a^b f(x) = F(b) - F(a)$ renders inline as $ \\int_a^b f(x) dx = F(b) - F(a) $. Enclose LaTeX code in double dollar signs $$ ... $$ to display expressions in a centered paragraph. For example: $$f'(a) = \\lim_{x \\to a} \\frac{f(x) - f(a)}{x-a}$$ renders as $$f'(a) = \\lim_{x \\to a} \\frac{f(x) - f(a)}{x-a}$$ See the LaTeX WikiBook for more information (especially the section on mathematics ).","title":"Mathematics Inline and Display"},{"location":"jupyter/latex/#common-symbols","text":"Below we give a list of commonly used mathematical symbols. Most other symbols can be inferred from these examples. See the LaTeX WikiBook (Mathematics) and the Detexify App to find any symbol you can think of! Syntax Output $x_n$ $x_n$ $x^2$ $x^2$ $\\infty$ $\\infty$ $\\frac{a}{b}$ $\\frac{a}{b}$ $\\partial$ $\\partial$ $\\alpha$ $\\alpha$ $\\beta$ $\\beta$ $\\gamma$ $\\gamma$ $\\Gamma$ $\\Gamma$ $\\Delta$ $\\Delta$ $\\sin$ $\\sin$ $\\cos$ $\\cos$ $\\tan$ $\\tan$ $\\sum_{n=0}^{\\infty}$ $\\sum_{n=0}^{\\infty}$ $\\prod_{n=0}^{\\infty}$ $\\prod_{n=0}^{\\infty}$ $\\int_a^b$ $\\int_a^b$ $\\lim_{x \\to a}$ $\\lim_{x \\to a}$ $\\mathrm{Hom}$ $\\mathrm{Hom}$ $\\mathbf{v}$ $\\mathbf{v}$ $\\mathbb{Z}$ $\\mathbb{Z}$ $\\mathscr{L}$ $\\mathscr{L}$ $\\mathfrak{g}$ $\\mathfrak{g}$ $\\dots$ $\\dots$ $\\vdots$ $\\vdots$ $\\ddots$ $\\ddots$","title":"Common Symbols"},{"location":"jupyter/latex/#matrices-and-brackets","text":"Create a matrix without brackets: $$\\begin{matrix} a & b \\\\ c & d \\end{matrix}$$ $$ \\begin{matrix} a & b \\\\ c & d \\end{matrix} $$ Create a matrix with round brackets: $$\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$$ $$ \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} $$ Create a matrix with square brackets: $$\\begin{bmatrix} 1 & 2 & 1 \\\\ 3 & 0 & 1 \\\\ 0 & 2 & 4 \\end{bmatrix}$$ $$ \\begin{bmatrix} 1 & 2 & 1 \\\\ 3 & 0 & 1 \\\\ 0 & 2 & 4 \\end{bmatrix} $$ Use \\left and \\right to enclose any expression in brackets: $$\\left( \\frac{p}{q} \\right)$$ $$\\left( \\frac{p}{q} \\right)$$","title":"Matrices and Brackets"},{"location":"jupyter/latex/#examples","text":"","title":"Examples"},{"location":"jupyter/latex/#derivative","text":"The derivative $f'(a)$ of the function $f(x)$ at the point $x=a$ is the limit: $$f'(a) = \\lim_{x \\to a} \\frac{f(x) - f(a)}{x - a}$$ $$f'(a) = \\lim_{x \\to a} \\frac{f(x) - f(a)}{x - a}$$","title":"Derivative"},{"location":"jupyter/latex/#continuity","text":"A function $f(x)$ is continuous at a point $x=a$ if: $$\\lim_{x \\to a^-} f(x) = f(a) = \\lim_{x \\to a^+} f(x)$$ $$\\lim_{x \\to a^-} f(x) = f(a) = \\lim_{x \\to a^+} f(x)$$","title":"Continuity"},{"location":"jupyter/latex/#maclaurin-series","text":"The MacLaurin series for $e^x$ is: $$e^x = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!}$$ $$e^x = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!}$$","title":"MacLaurin Series"},{"location":"jupyter/latex/#jacobian-matrix","text":"The Jacobian matrix of the function $\\mathbf{f}(x_1, \\dots, x_n)$ is: $$ \\mathbf{J} = \\frac{d \\mathbf{f}}{d \\mathbf{x}} = \\left[ \\frac{\\partial \\mathbf{f}}{\\partial x_1} \\cdots \\frac{\\partial \\mathbf{f}}{\\partial x_n} \\right] = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$ $$ \\mathbf{J} = \\frac{d \\mathbf{f}}{d \\mathbf{x}} = \\left[ \\frac{\\partial \\mathbf{f}}{\\partial x_1} \\cdots \\frac{\\partial \\mathbf{f}}{\\partial x_n} \\right] = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$","title":"Jacobian Matrix"},{"location":"jupyter/latex/#exercises","text":"Exercise 1 . Write LaTeX code to display the angle sum identity $$\\cos(\\alpha \\pm \\beta) = \\cos \\alpha \\cos \\beta \\mp \\sin \\alpha \\sin \\beta$$ Exercise 2. Write LaTeX code to display the indefinite integral $$\\int \\frac{1}{1 + x^2} \\, dx = \\arctan x + C$$ Exercise 3. Write LaTeX code to display the Navier-Stokes Equation for Incompressible Flow $$\\frac{\\partial \\mathbf{u}}{\\partial t} + (\\mathbf{u} \\cdot \\nabla) \\mathbf{u} - \\nu \\nabla^2 \\mathbf{u} = - \\nabla w + \\mathbf{g}$$ Exercise 4. Write LaTeX code to display Green's Theorem $$\\oint_C (L dx + M dy) = \\iint_D \\left( \\frac{\\partial M}{\\partial x} - \\frac{\\partial L}{\\partial y} \\right) dx \\, dy$$ Exercise 5. Write LaTeX code to display the Prime Number Theorem $$\\lim_{x \\to \\infty} \\frac{\\pi(x)}{ \\frac{x}{\\log(x)}} = 1$$ Exercise 6. Write LaTeX code to display the general formula for Taylor series $$\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n!} (x-a)^n$$ Exercise 7. Write LaTeX code to display Stokes' Theorem $$\\int_{\\partial \\Omega} \\omega = \\int_{\\Omega} d \\omega$$ Exercise 8. Write LaTeX code to display the adjoint property of the tensor product $$\\mathrm{Hom}(U \\otimes V,W) \\cong \\mathrm{Hom}(U, \\mathrm{Hom}(V,W))$$ Exercise 9. Write LaTeX code to display the definition of the Laplace transform $$\\mathscr{L} \\{ f(t) \\} = F(s) = \\int_0^{\\infty} f(t) e^{-st} dt$$ Exercise 10. Write LaTeX code to display the inverse matrix formula $$\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}$$ Exercise 11. Write LaTeX code to display the infinite product formula $$\\sin x = x \\prod_{n=1}^{\\infty} \\left( 1 - \\frac{x^2}{\\pi^2 n^2} \\right)$$ Exercise 12. Pick your favourite math course and write the notes from your last class in LaTeX.","title":"Exercises"},{"location":"jupyter/markdown/","text":"Markdown Markdown is a simple text-to-HTML markup language written in plain text. Jupyter notebook recognizes markdown and renders markdown code as HTML. See Markdown (by John Gruber) and GitHub Markdown Help for more information. Text Output Syntax emphasis *emphasis* strong **strong** code `code` Headings Output Syntax Heading 1 # Heading 1 Heading 2 ## Heading 2 Heading 3 ### Heading 3 Heading 4 #### Heading 4 Heading 5 ##### Heading 5 Heading 6 ###### Heading 6 Lists Create an ordered list using numbers: 1. Number theory 2. Algebra 3. Partial differential equations 4. Probability Number theory Algebra Partial differential equations Probability Create an unordered list using an asterisk * for each item: * Number theory * Algebra * Partial differential equations * Probability Number theory Algebra Partial differential equations Probability Use indentation to create nested lists: 1. Mathematics * Calculus * Linear Algebra * Probability 2. Physics * Classical Mechanics * Relativity * Thermodynamics 3. Biology * Diffusion and Osmosis * Homeostasis * Immunology Mathematics Calculus Linear Algebra Probability Physics Classical Mechanics Relativity Thermodynamics Biology Diffusion and Osmosis Homeostasis Immunology Links Create a link with the syntax [description](url) . For example: [UBC Math](http://www.math.ubc.ca) creates the link UBC Math . Images Include an image using the syntax ![description](url) . For example: ![Jupyter logo](https://jupyter.org/assets/try/jupyter.png) displays the image Tables Create a table by separating entries by pipe characters |: | Python Operator | Description | | :---: | :---: | | `+` | addition | | `-` | subtraction | | `*` | multiplication | | `/` | division | | `**` | power | Python Operator Description + addition - subtraction * multiplication / division ** power The syntax :---: specifies the alignment (centered in this case) of the columns. See more about GitHub flavoured markdown . Exercises Exercise 1. Create a numbered list of the top 5 websites you visit most often and include a link for each site. Exercise 2. Write a short biography of your favourite mathematician, provide a link to their Wikipedia page and include an image (with a link and description of the source). Exercise 3. Create a table of all the courses that you have taken in university. Include the columns: course number, course title, year (that you took the class), and instructor name.","title":"Markdown"},{"location":"jupyter/markdown/#markdown","text":"Markdown is a simple text-to-HTML markup language written in plain text. Jupyter notebook recognizes markdown and renders markdown code as HTML. See Markdown (by John Gruber) and GitHub Markdown Help for more information.","title":"Markdown"},{"location":"jupyter/markdown/#text","text":"Output Syntax emphasis *emphasis* strong **strong** code `code`","title":"Text"},{"location":"jupyter/markdown/#headings","text":"Output Syntax","title":"Headings"},{"location":"jupyter/markdown/#lists","text":"Create an ordered list using numbers: 1. Number theory 2. Algebra 3. Partial differential equations 4. Probability Number theory Algebra Partial differential equations Probability Create an unordered list using an asterisk * for each item: * Number theory * Algebra * Partial differential equations * Probability Number theory Algebra Partial differential equations Probability Use indentation to create nested lists: 1. Mathematics * Calculus * Linear Algebra * Probability 2. Physics * Classical Mechanics * Relativity * Thermodynamics 3. Biology * Diffusion and Osmosis * Homeostasis * Immunology Mathematics Calculus Linear Algebra Probability Physics Classical Mechanics Relativity Thermodynamics Biology Diffusion and Osmosis Homeostasis Immunology","title":"Lists"},{"location":"jupyter/markdown/#links","text":"Create a link with the syntax [description](url) . For example: [UBC Math](http://www.math.ubc.ca) creates the link UBC Math .","title":"Links"},{"location":"jupyter/markdown/#images","text":"Include an image using the syntax ![description](url) . For example: ![Jupyter logo](https://jupyter.org/assets/try/jupyter.png) displays the image","title":"Images"},{"location":"jupyter/markdown/#tables","text":"Create a table by separating entries by pipe characters |: | Python Operator | Description | | :---: | :---: | | `+` | addition | | `-` | subtraction | | `*` | multiplication | | `/` | division | | `**` | power | Python Operator Description + addition - subtraction * multiplication / division ** power The syntax :---: specifies the alignment (centered in this case) of the columns. See more about GitHub flavoured markdown .","title":"Tables"},{"location":"jupyter/markdown/#exercises","text":"Exercise 1. Create a numbered list of the top 5 websites you visit most often and include a link for each site. Exercise 2. Write a short biography of your favourite mathematician, provide a link to their Wikipedia page and include an image (with a link and description of the source). Exercise 3. Create a table of all the courses that you have taken in university. Include the columns: course number, course title, year (that you took the class), and instructor name.","title":"Exercises"},{"location":"jupyter/notebook/","text":"Jupyter Notebook Jupyter Notebook is a web application for creating and sharing documents that contain live code, equations, visualizations and explanatory text. Cells There are two main types of cells: code cells and markdown cells. Hit SHIFT+ENTER to execute the contents of a cell. Markdown cells contain: markdown HTML LaTeX plain text images videos Anything that a browser can understand For more information about markdown see Markdown Basics on GitHub and Markdown Syntax . Python code is written in code cells. Hit SHIFT+ENTER to execute the code. Output is displayed below the code cell: # Python code to display the first 10 square numbers for n in range(1,11): print(n**2) 1 4 9 16 25 36 49 64 81 100 Modes There are two modes: edit mode and command mode. Press ESC to enter command mode and ENTER for edit mode. Edit mode is for writing text and code in the cell. Command mode is for notebook editing commands such as cut cell, paste cell, and insert cell above. Keyboard Shortcuts The toolbar has buttons for common actions however you can increase the speed of your workflow by memorizing the following keyboard shortcuts in command mode: Command Mode Action Shortcut insert empty cell above a insert empty cell below b copy cell c cut cell x paste cell below v switch to code cell y switch to markdown cell m save and checkpoint s execute cell SHIFT+ENTER See Help in the toolbar of the Jupyter notebook to see the list of keyboard shortcuts.","title":"Jupyter Notebook"},{"location":"jupyter/notebook/#jupyter-notebook","text":"Jupyter Notebook is a web application for creating and sharing documents that contain live code, equations, visualizations and explanatory text.","title":"Jupyter Notebook"},{"location":"jupyter/notebook/#cells","text":"There are two main types of cells: code cells and markdown cells. Hit SHIFT+ENTER to execute the contents of a cell. Markdown cells contain: markdown HTML LaTeX plain text images videos Anything that a browser can understand For more information about markdown see Markdown Basics on GitHub and Markdown Syntax . Python code is written in code cells. Hit SHIFT+ENTER to execute the code. Output is displayed below the code cell: # Python code to display the first 10 square numbers for n in range(1,11): print(n**2) 1 4 9 16 25 36 49 64 81 100","title":"Cells"},{"location":"jupyter/notebook/#modes","text":"There are two modes: edit mode and command mode. Press ESC to enter command mode and ENTER for edit mode. Edit mode is for writing text and code in the cell. Command mode is for notebook editing commands such as cut cell, paste cell, and insert cell above.","title":"Modes"},{"location":"jupyter/notebook/#keyboard-shortcuts","text":"The toolbar has buttons for common actions however you can increase the speed of your workflow by memorizing the following keyboard shortcuts in command mode: Command Mode Action Shortcut insert empty cell above a insert empty cell below b copy cell c cut cell x paste cell below v switch to code cell y switch to markdown cell m save and checkpoint s execute cell SHIFT+ENTER See Help in the toolbar of the Jupyter notebook to see the list of keyboard shortcuts.","title":"Keyboard Shortcuts"},{"location":"linear-algebra/applications/","text":"Applications import numpy as np import matplotlib.pyplot as plt import scipy.linalg as la Polynomial Interpolation Polynomial interpolation finds the unique polynomial of degree $n$ which passes through $n+1$ points in the $xy$-plane. For example, two points in the $xy$-plane determine a line and three points determine a parabola. Formulation Suppose we have $n + 1$ points in the $xy$-plane $$ (x_0,y_0),(x_1,y_1),\\dots,(x_n,y_n) $$ such that all the $x$ values are distinct ($x_i \\not= x_j$ for $i \\not= j$). The general form of a degree $n$ polynomial is $$ p(x) = a_0 + a_1 x + a_2x^2 + \\cdots + a_n x^n $$ If $p(x)$ is the unique degree $n$ polynomial which interpolates all the points, then the coefficients $a_0$, $a_1$, $\\dots$, $a_n$ satisfy the following equations: \\begin{align} a_0 + a_1x_0 + a_2x_0^2 + \\cdots + a_n x_0^n &= y_0 \\\\ a_0 + a_1x_1 + a_2x_1^2 + \\cdots + a_n x_1^n &= y_1 \\\\ & \\ \\ \\vdots \\\\ a_0 + a_1x_n + a_2x_n^2 + \\cdots + a_n x_n^n &= y_n \\end{align} Therefore the vector of coefficients $$ \\mathbf{a} = \\begin{bmatrix} a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_n \\end{bmatrix} $$ is the unique the solution of the linear system of equations $$ X \\mathbf{a}=\\mathbf{y} $$ where $X$ is the Vandermonde matrix and $\\mathbf{y}$ is the vector of $y$ values $$ X = \\begin{bmatrix} 1 & x_0 & x_0^2 & \\dots & x_0^n \\\\ 1 & x_1 & x_1^2 & \\dots & x_1^n \\\\ & \\vdots & & & \\vdots \\\\ 1 & x_n & x_n^2 & \\dots & x_n^n \\\\ \\end{bmatrix} \\ \\ \\mathrm{and} \\ \\ \\mathbf{y} = \\begin{bmatrix} y_0 \\\\ y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} $$ Examples Simple Parabola Let's do a simple example. We know that $y=x^2$ is the unique degree 2 polynomial that interpolates the points $(-1,1)$, $(0,0)$ and $(1,1)$. Let's compute the polynomial interpolation of these points and verify the expected result $a_0=0$, $a_1=0$ and $a_2=1$. Create the Vandermonde matrix $X$ with the array of $x$ values: x = np.array([-1,0,1]) X = np.column_stack([[1,1,1],x,x**2]) print(X) [[ 1 -1 1] [ 1 0 0] [ 1 1 1]] Create the vector $\\mathbf{y}$ of $y$ values: y = np.array([1,0,1]).reshape(3,1) print(y) [[1] [0] [1]] We expect the solution $\\mathbf{a} = [0,0,1]^T$: a = la.solve(X,y) print(a) [[0.] [0.] [1.]] Success! Another Parabola The polynomial interpolation of 3 points $(x_0,y_0)$, $(x_1,y_1)$ and $(x_2,y_2)$ is the parabola $p(x) = a_0 + a_1x + a_2x^2$ such that the coefficients satisfy \\begin{align} a_0 + a_1x_0 + a_2x_0^2 = y_0 \\\\ a_0 + a_1x_1 + a_2x_1^2 = y_1 \\\\ a_0 + a_1x_2 + a_2x_2^2 = y_2 \\end{align} Let's find the polynomial interpolation of the points $(0,6)$, $(3,1)$ and $(8,2)$. Create the Vandermonde matrix $X$: x = np.array([0,3,8]) X = np.column_stack([[1,1,1],x,x**2]) print(X) [[ 1 0 0] [ 1 3 9] [ 1 8 64]] And the vector of $y$ values: y = np.array([6,1,2]).reshape(3,1) print(y) [[6] [1] [2]] Compute the vector $\\mathbf{a}$ of coefficients: a = la.solve(X,y) print(a) [[ 6. ] [-2.36666667] [ 0.23333333]] And plot the result: xs = np.linspace(0,8,20) ys = a[0] + a[1]*xs + a[2]*xs**2 plt.plot(xs,ys,x,y,'b.',ms=20) plt.show() Over Fitting 10 Random Points Now let's interpolate points with $x_i=i$, $i=0,\\dots,9$, and 10 random integers sampled from $[0,10)$ as $y$ values: N = 10 x = np.arange(0,N) y = np.random.randint(0,10,N) plt.plot(x,y,'r.') plt.show() Create the Vandermonde matrix and verify the first 5 rows and columns: X = np.column_stack([x**k for k in range(0,N)]) print(X[:5,:5]) [[ 1 0 0 0 0] [ 1 1 1 1 1] [ 1 2 4 8 16] [ 1 3 9 27 81] [ 1 4 16 64 256]] We could also use the NumPy function numpy.vander . We specify the option increasing=True so that powers of $x_i$ increase left-to-right: X = np.vander(x,increasing=True) print(X[:5,:5]) [[ 1 0 0 0 0] [ 1 1 1 1 1] [ 1 2 4 8 16] [ 1 3 9 27 81] [ 1 4 16 64 256]] Solve the linear system: a = la.solve(X,y) Plot the interpolation: xs = np.linspace(0,N-1,200) ys = sum([a[k]*xs**k for k in range(0,N)]) plt.plot(x,y,'r.',xs,ys) plt.show() Success! But notice how unstable the curve is. That's why it better to use a cubic spline to interpolate a large number of points. However real-life data is usually very noisy and interpolation is not the best tool to fit a line to data. Instead we would want to take a polynomial with smaller degree (like a line) and fit it as best we can without interpolating the points. Least Squares Linear Regression Suppose we have $n+1$ points $$ (x_0,y_0) , (x_1,y_1) , \\dots , (x_n,y_n) $$ in the $xy$-plane and we want to fit a line $$ y=a_0 + a_1x $$ that \"best fits\" the data. There are different ways to quantify what \"best fit\" means but the most common method is called least squares linear regression . In least squares linear regression, we want to minimize the sum of squared errors $$ SSE = \\sum_i (y_i - (a_0 + a_1 x_i))^2 $$ Formulation If we form matrices $$ X = \\begin{bmatrix} 1 & x_0 \\\\ 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\ , \\ \\ \\mathbf{y} = \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\ , \\ \\ \\mathbf{a} = \\begin{bmatrix} a_0 \\\\ a_1 \\end{bmatrix} $$ then the sum of squared errors can be expressed as $$ SSE = \\Vert \\mathbf{y} - X \\mathbf{a} \\Vert^2 $$ Theorem. (Least Squares Linear Regression) Consider $n+1$ points $$ (x_0,y_0) , (x_1,y_1) , \\dots , (x_n,y_n) $$ in the $xy$-plane. The coefficients $\\mathbf{a} = [a_0,a_1]^T$ which minimize the sum of squared errors $$ SSE = \\sum_i (y_i - (a_0 + a_1 x_i))^2 $$ is the unique solution of the system $$ \\left( X^T X \\right) \\mathbf{a} = X^T \\mathbf{y} $$ Sketch of Proof. The product $X\\mathbf{a}$ is in the column space of $X$. The line connecting $\\mathbf{y}$ to the nearest point in the column space of $X$ is perpendicluar to the column space of $X$. Therefore $$ X^T \\left( \\mathbf{y} - X \\mathbf{a} \\right) = \\mathbf{0} $$ and so $$ \\left( X^T X \\right) \\mathbf{a} = X^T \\mathbf{y} $$ Examples Fake Noisy Linear Data Let's do an example with some fake data. Let's build a set of random points based on the model $$ y = a_0 + a_1x + \\epsilon $$ for some arbitrary choice of $a_0$ and $a_1$. The factor $\\epsilon$ represents some random noise which we model using the normal distribution . We can generate random numbers sampled from the standard normal distribution using the NumPy function numpy.random.rand . The goal is to demonstrate that we can use linear regression to retrieve the coefficeints $a_0$ and $a_1$ from the linear regression calculation. a0 = 2 a1 = 3 N = 100 x = np.random.rand(100) noise = 0.1*np.random.randn(100) y = a0 + a1*x + noise plt.scatter(x,y); plt.show() Let's use linear regression to retrieve the coefficients $a_0$ and $a_1$. Construct the matrix $X$: X = np.column_stack([np.ones(N),x]) print(X.shape) (100, 2) Let's look at the first 5 rows of $X$ to see that it is in the correct form: X[:5,:] array([[1. , 0.92365627], [1. , 0.78757973], [1. , 0.51506055], [1. , 0.51540875], [1. , 0.86563343]]) Use scipy.linalg.solve to solve $\\left(X^T X\\right)\\mathbf{a} = \\left(X^T\\right)\\mathbf{y}$ for $\\mathbf{a}$: a = la.solve(X.T @ X, X.T @ y) print(a) [2.02783873 2.95308228] We have retrieved the coefficients of the model almost exactly! Let's plot the random data points with the linear regression we just computed. xs = np.linspace(0,1,10) ys = a[0] + a[1]*xs plt.plot(xs,ys,'r',linewidth=4) plt.scatter(x,y); plt.show() Real Kobe Bryant Data Let's work with some real data. Kobe Bryant retired in 2016 with 33643 total points which is the third highest total points in NBA history . How many more years would Kobe Bryant have to had played to pass Kareem Abdul-Jabbar's record 38387 points? Kobe Bryant's peak was the 2005-2006 NBA season. Let's look at Kobe Bryant's total games played and points per game from 2006 to 2016. years = np.array([2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]) games = [80,77,82,82,73,82,58,78,6,35,66] points = np.array([35.4,31.6,28.3,26.8,27,25.3,27.9,27.3,13.8,22.3,17.6]) fig = plt.figure(figsize=(12,10)) axs = fig.subplots(2,1,sharex=True) axs[0].plot(years,points,'b.',ms=15) axs[0].set_title('Kobe Bryant, Points per Game') axs[0].set_ylim([0,40]) axs[0].grid(True) axs[1].bar(years,games) axs[1].set_title('Kobe Bryant, Games Played') axs[1].set_ylim([0,100]) axs[1].grid(True) plt.show() Kobe was injured for most of the 2013-2014 NBA season and played only 6 games. This is an outlier and so we can drop this data point: years = np.array([2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2015, 2016]) games = np.array([80,77,82,82,73,82,58,78,35,66]) points = np.array([35.4,31.6,28.3,26.8,27,25.3,27.9,27.3,22.3,17.6]) Let's compute the average games played per season over this period: avg_games_per_year = np.mean(games) print(avg_games_per_year) 71.3 Compute the linear model for points per game: X = np.column_stack([np.ones(len(years)),years]) a = la.solve(X.T @ X, X.T @ points) model = a[0] + a[1]*years plt.plot(years,model,years,points,'b.',ms=15) plt.title('Kobe Bryant, Points per Game') plt.ylim([0,40]) plt.grid(True) plt.show() Now we can extrapolate to future years and multiply points per games by games per season and compute the cumulative sum to see Kobe's total points: future_years = np.array([2017,2018,2019,2020,2021]) future_points = (a[0] + a[1]*future_years)*avg_games_per_year total_points = 33643 + np.cumsum(future_points) kareem = 38387*np.ones(len(future_years)) plt.plot(future_years,total_points,future_years,kareem) plt.grid(True) plt.xticks(future_years) plt.title('Kobe Bryant Total Points Prediction') plt.show() Only 4 more years! Polynomial Regression Formulation The same idea works for fitting a degree $d$ polynomial model $$ y = a_0 + a_1x + a_2x^2 + \\cdots + a_dx^d $$ to a set of $n+1$ data points $$ (x_0,y_0), (x_1,y_1), \\dots , (x_n,y_n) $$ We form the matrices as before but now the Vandermonde matrix $X$ has $d+1$ columns $$ X = \\begin{bmatrix} 1 & x_0 & x_0^2 & \\cdots & x_0^d \\\\ 1 & x_1 & x_1^2 & \\cdots & x_1^d \\\\ & \\vdots & & & \\vdots \\\\ 1 & x_n & x_n^2 & \\cdots & x_n^d \\end{bmatrix} \\ , \\ \\ \\mathbf{y} = \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\ , \\ \\ \\mathbf{a} = \\begin{bmatrix} a_0 \\\\ a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_d \\end{bmatrix} $$ The coefficients $\\mathbf{a} = [a_0,a_1,a_2,\\dots,a_d]^T$ which minimize the sum of squared errors $SSE$ is the unique solution of the linear system $$ \\left( X^T X \\right) \\mathbf{a} = \\left( X^T \\right) \\mathbf{y} $$ Example Fake Noisy Quadratic Data Let's build some fake data using a quadratic model $y = a_0 + a_1x + a_2x^2 + \\epsilon$ and use linear regression to retrieve the coefficients $a_0$, $a_1$ and $a_2$. a0 = 3 a1 = 5 a2 = 8 N = 1000 x = 2*np.random.rand(N) - 1 # Random numbers in the interval (-1,1) noise = np.random.randn(N) y = a0 + a1*x + a2*x**2 + noise plt.scatter(x,y,alpha=0.5,lw=0); plt.show() Construct the matrix $X$: X = np.column_stack([np.ones(N),x,x**2]) Use scipy.linalg.solve to solve $\\left( X^T X \\right) \\mathbf{a} = \\left( X^T \\right) \\mathbf{y}$: a = la.solve((X.T @ X),X.T @ y) Plot the result: xs = np.linspace(-1,1,20) ys = a[0] + a[1]*xs + a[2]*xs**2 plt.plot(xs,ys,'r',linewidth=4) plt.scatter(x,y,alpha=0.5,lw=0) plt.show() Graph Theory A graph is a set of vertices and a set of edges connecting some of the vertices. We will consider simple, undirected, connected graphs: a graph is simple if there are no loops or multiple edges between vertices a graph is undirected if the edges do not have an orientation a graph is connected if each vertex is connected to every other vertex in the graph by a path We can visualize a graph as a set of vertices and edges and answer questions about the graph just by looking at it. However this becomes much more difficult with a large graphs such as a social network graph . Instead, we construct matrices from the graph such as the adjacency matrix and the Laplacian matrix and study their properties. Spectral graph theory is the study of the eigenvalues of the adjacency matrix (and other associated matrices) and the relationships to the structure of $G$. NetworkX Let's use the Python package NetworkX to construct and visualize some simple graphs. import networkx as nx Adjacency Matrix The adjacency matrix $A_G$ of a graph $G$ with $n$ vertices is the square matrix of size $n$ such that $A_{i,j} = 1$ if vertices $i$ and $j$ are connected by an edge, and $A_{i,j} = 0$ otherwise. We can use networkx to create the adjacency matrix of a graph $G$. The function nx.adjacency_matrix returns a sparse matrix and we convert it to a regular NumPy array using the todense method. For example, plot the complete graph with 5 vertices and compute the adjacency matrix: G = nx.complete_graph(5) nx.draw(G,with_labels=True) A = nx.adjacency_matrix(G).todense() print(A) [[0 1 1 1 1] [1 0 1 1 1] [1 1 0 1 1] [1 1 1 0 1] [1 1 1 1 0]] Length of the Shortest Path The length of the shortest path between vertices in a simple, undirected graph $G$ can be easily computed from the adjacency matrix $A_G$. In particular, the length of shortest path from vertex $i$ to vertex $j$ ($i\\not=j$) is the smallest positive integer $k$ such that $A^k_{i,j} \\not= 0$. Plot the dodecahedral graph : G = nx.dodecahedral_graph() nx.draw(G,with_labels=True) A = nx.adjacency_matrix(G).todense() print(A) [[0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1] [1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0] [0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1] [0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0] [0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0] [0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0] [0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0] [1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0] [0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0] [0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0] [0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0] [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1] [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]] With this labelling, let's find the length of the shortest path from vertex $0$ to $15$: i = 0 j = 15 k = 1 Ak = A while Ak[i,j] == 0: Ak = Ak @ A k = k + 1 print('Length of the shortest path is',k) Length of the shortest path is 5 Triangles in a Graph A simple result in spectral graph theory is the number of triangles in a graph $T(G)$ is given by: $$ T(G) = \\frac{1}{6} ( \\lambda_1^3 + \\lambda_2^3 + \\cdots + \\lambda_n^3) $$ where $\\lambda_1 \\leq \\lambda_2 \\leq \\cdots \\leq \\lambda_n$ are the eigenvalues of the adjacency matrix. Let's verify this for the simplest case, the complete graph on 3 vertices: C3 = nx.complete_graph(3) nx.draw(C3,with_labels=True) A3 = nx.adjacency_matrix(C3).todense() eigvals, eigvecs = la.eig(A3) int(np.round(np.sum(eigvals.real**3)/6,0)) 1 Let's compute the number of triangles in the complete graph 7 vertices: C7 = nx.complete_graph(7) nx.draw(C7,with_labels=True) A7 = nx.adjacency_matrix(C7).todense() eigvals, eigvecs = la.eig(A7) int(np.round(np.sum(eigvals.real**3)/6,0)) 35 There are 35 triangles in the complete graph with 7 vertices! Let's write a function called triangles which takes a square matrix M and return the sum $$ \\frac{1}{6} ( \\lambda_1^3 + \\lambda_2^3 + \\cdots + \\lambda_n^3) $$ where $\\lambda_i$ are the eigenvalues of the symmetric matrix $A = (M + M^T)/2$. Note that $M = A$ if $M$ is symmetric. The return value is the number of triangles in the graph $G$ if the input $M$ is the adjacency matrix. def triangles(M): A = (M + M.T)/2 eigvals, eigvecs = la.eig(A) eigvals = eigvals.real return int(np.round(np.sum(eigvals**3)/6,0)) Next, let's try a Turan graph . G = nx.turan_graph(10,5) nx.draw(G,with_labels=True) A = nx.adjacency_matrix(G).todense() print(A) [[0 0 1 1 1 1 1 1 1 1] [0 0 1 1 1 1 1 1 1 1] [1 1 0 0 1 1 1 1 1 1] [1 1 0 0 1 1 1 1 1 1] [1 1 1 1 0 0 1 1 1 1] [1 1 1 1 0 0 1 1 1 1] [1 1 1 1 1 1 0 0 1 1] [1 1 1 1 1 1 0 0 1 1] [1 1 1 1 1 1 1 1 0 0] [1 1 1 1 1 1 1 1 0 0]] Find the number of triangles: triangles(A) 80 Finally, let's compute the number of triangles in the dodecahedral graph: G = nx.dodecahedral_graph() nx.draw(G,with_labels=True) A = nx.adjacency_matrix(G).todense() print(A) [[0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1] [1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0] [0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1] [0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0] [0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0] [0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0] [0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0] [1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0] [0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0] [0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0] [0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0] [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1] [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]] np.round(triangles(A),2) 0 Exercises Under Construction","title":"Applications"},{"location":"linear-algebra/applications/#applications","text":"import numpy as np import matplotlib.pyplot as plt import scipy.linalg as la","title":"Applications"},{"location":"linear-algebra/applications/#polynomial-interpolation","text":"Polynomial interpolation finds the unique polynomial of degree $n$ which passes through $n+1$ points in the $xy$-plane. For example, two points in the $xy$-plane determine a line and three points determine a parabola.","title":"Polynomial Interpolation"},{"location":"linear-algebra/applications/#formulation","text":"Suppose we have $n + 1$ points in the $xy$-plane $$ (x_0,y_0),(x_1,y_1),\\dots,(x_n,y_n) $$ such that all the $x$ values are distinct ($x_i \\not= x_j$ for $i \\not= j$). The general form of a degree $n$ polynomial is $$ p(x) = a_0 + a_1 x + a_2x^2 + \\cdots + a_n x^n $$ If $p(x)$ is the unique degree $n$ polynomial which interpolates all the points, then the coefficients $a_0$, $a_1$, $\\dots$, $a_n$ satisfy the following equations: \\begin{align} a_0 + a_1x_0 + a_2x_0^2 + \\cdots + a_n x_0^n &= y_0 \\\\ a_0 + a_1x_1 + a_2x_1^2 + \\cdots + a_n x_1^n &= y_1 \\\\ & \\ \\ \\vdots \\\\ a_0 + a_1x_n + a_2x_n^2 + \\cdots + a_n x_n^n &= y_n \\end{align} Therefore the vector of coefficients $$ \\mathbf{a} = \\begin{bmatrix} a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_n \\end{bmatrix} $$ is the unique the solution of the linear system of equations $$ X \\mathbf{a}=\\mathbf{y} $$ where $X$ is the Vandermonde matrix and $\\mathbf{y}$ is the vector of $y$ values $$ X = \\begin{bmatrix} 1 & x_0 & x_0^2 & \\dots & x_0^n \\\\ 1 & x_1 & x_1^2 & \\dots & x_1^n \\\\ & \\vdots & & & \\vdots \\\\ 1 & x_n & x_n^2 & \\dots & x_n^n \\\\ \\end{bmatrix} \\ \\ \\mathrm{and} \\ \\ \\mathbf{y} = \\begin{bmatrix} y_0 \\\\ y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} $$","title":"Formulation"},{"location":"linear-algebra/applications/#examples","text":"Simple Parabola Let's do a simple example. We know that $y=x^2$ is the unique degree 2 polynomial that interpolates the points $(-1,1)$, $(0,0)$ and $(1,1)$. Let's compute the polynomial interpolation of these points and verify the expected result $a_0=0$, $a_1=0$ and $a_2=1$. Create the Vandermonde matrix $X$ with the array of $x$ values: x = np.array([-1,0,1]) X = np.column_stack([[1,1,1],x,x**2]) print(X) [[ 1 -1 1] [ 1 0 0] [ 1 1 1]] Create the vector $\\mathbf{y}$ of $y$ values: y = np.array([1,0,1]).reshape(3,1) print(y) [[1] [0] [1]] We expect the solution $\\mathbf{a} = [0,0,1]^T$: a = la.solve(X,y) print(a) [[0.] [0.] [1.]] Success! Another Parabola The polynomial interpolation of 3 points $(x_0,y_0)$, $(x_1,y_1)$ and $(x_2,y_2)$ is the parabola $p(x) = a_0 + a_1x + a_2x^2$ such that the coefficients satisfy \\begin{align} a_0 + a_1x_0 + a_2x_0^2 = y_0 \\\\ a_0 + a_1x_1 + a_2x_1^2 = y_1 \\\\ a_0 + a_1x_2 + a_2x_2^2 = y_2 \\end{align} Let's find the polynomial interpolation of the points $(0,6)$, $(3,1)$ and $(8,2)$. Create the Vandermonde matrix $X$: x = np.array([0,3,8]) X = np.column_stack([[1,1,1],x,x**2]) print(X) [[ 1 0 0] [ 1 3 9] [ 1 8 64]] And the vector of $y$ values: y = np.array([6,1,2]).reshape(3,1) print(y) [[6] [1] [2]] Compute the vector $\\mathbf{a}$ of coefficients: a = la.solve(X,y) print(a) [[ 6. ] [-2.36666667] [ 0.23333333]] And plot the result: xs = np.linspace(0,8,20) ys = a[0] + a[1]*xs + a[2]*xs**2 plt.plot(xs,ys,x,y,'b.',ms=20) plt.show() Over Fitting 10 Random Points Now let's interpolate points with $x_i=i$, $i=0,\\dots,9$, and 10 random integers sampled from $[0,10)$ as $y$ values: N = 10 x = np.arange(0,N) y = np.random.randint(0,10,N) plt.plot(x,y,'r.') plt.show() Create the Vandermonde matrix and verify the first 5 rows and columns: X = np.column_stack([x**k for k in range(0,N)]) print(X[:5,:5]) [[ 1 0 0 0 0] [ 1 1 1 1 1] [ 1 2 4 8 16] [ 1 3 9 27 81] [ 1 4 16 64 256]] We could also use the NumPy function numpy.vander . We specify the option increasing=True so that powers of $x_i$ increase left-to-right: X = np.vander(x,increasing=True) print(X[:5,:5]) [[ 1 0 0 0 0] [ 1 1 1 1 1] [ 1 2 4 8 16] [ 1 3 9 27 81] [ 1 4 16 64 256]] Solve the linear system: a = la.solve(X,y) Plot the interpolation: xs = np.linspace(0,N-1,200) ys = sum([a[k]*xs**k for k in range(0,N)]) plt.plot(x,y,'r.',xs,ys) plt.show() Success! But notice how unstable the curve is. That's why it better to use a cubic spline to interpolate a large number of points. However real-life data is usually very noisy and interpolation is not the best tool to fit a line to data. Instead we would want to take a polynomial with smaller degree (like a line) and fit it as best we can without interpolating the points.","title":"Examples"},{"location":"linear-algebra/applications/#least-squares-linear-regression","text":"Suppose we have $n+1$ points $$ (x_0,y_0) , (x_1,y_1) , \\dots , (x_n,y_n) $$ in the $xy$-plane and we want to fit a line $$ y=a_0 + a_1x $$ that \"best fits\" the data. There are different ways to quantify what \"best fit\" means but the most common method is called least squares linear regression . In least squares linear regression, we want to minimize the sum of squared errors $$ SSE = \\sum_i (y_i - (a_0 + a_1 x_i))^2 $$","title":"Least Squares Linear Regression"},{"location":"linear-algebra/applications/#formulation_1","text":"If we form matrices $$ X = \\begin{bmatrix} 1 & x_0 \\\\ 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\ , \\ \\ \\mathbf{y} = \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\ , \\ \\ \\mathbf{a} = \\begin{bmatrix} a_0 \\\\ a_1 \\end{bmatrix} $$ then the sum of squared errors can be expressed as $$ SSE = \\Vert \\mathbf{y} - X \\mathbf{a} \\Vert^2 $$ Theorem. (Least Squares Linear Regression) Consider $n+1$ points $$ (x_0,y_0) , (x_1,y_1) , \\dots , (x_n,y_n) $$ in the $xy$-plane. The coefficients $\\mathbf{a} = [a_0,a_1]^T$ which minimize the sum of squared errors $$ SSE = \\sum_i (y_i - (a_0 + a_1 x_i))^2 $$ is the unique solution of the system $$ \\left( X^T X \\right) \\mathbf{a} = X^T \\mathbf{y} $$ Sketch of Proof. The product $X\\mathbf{a}$ is in the column space of $X$. The line connecting $\\mathbf{y}$ to the nearest point in the column space of $X$ is perpendicluar to the column space of $X$. Therefore $$ X^T \\left( \\mathbf{y} - X \\mathbf{a} \\right) = \\mathbf{0} $$ and so $$ \\left( X^T X \\right) \\mathbf{a} = X^T \\mathbf{y} $$","title":"Formulation"},{"location":"linear-algebra/applications/#examples_1","text":"Fake Noisy Linear Data Let's do an example with some fake data. Let's build a set of random points based on the model $$ y = a_0 + a_1x + \\epsilon $$ for some arbitrary choice of $a_0$ and $a_1$. The factor $\\epsilon$ represents some random noise which we model using the normal distribution . We can generate random numbers sampled from the standard normal distribution using the NumPy function numpy.random.rand . The goal is to demonstrate that we can use linear regression to retrieve the coefficeints $a_0$ and $a_1$ from the linear regression calculation. a0 = 2 a1 = 3 N = 100 x = np.random.rand(100) noise = 0.1*np.random.randn(100) y = a0 + a1*x + noise plt.scatter(x,y); plt.show() Let's use linear regression to retrieve the coefficients $a_0$ and $a_1$. Construct the matrix $X$: X = np.column_stack([np.ones(N),x]) print(X.shape) (100, 2) Let's look at the first 5 rows of $X$ to see that it is in the correct form: X[:5,:] array([[1. , 0.92365627], [1. , 0.78757973], [1. , 0.51506055], [1. , 0.51540875], [1. , 0.86563343]]) Use scipy.linalg.solve to solve $\\left(X^T X\\right)\\mathbf{a} = \\left(X^T\\right)\\mathbf{y}$ for $\\mathbf{a}$: a = la.solve(X.T @ X, X.T @ y) print(a) [2.02783873 2.95308228] We have retrieved the coefficients of the model almost exactly! Let's plot the random data points with the linear regression we just computed. xs = np.linspace(0,1,10) ys = a[0] + a[1]*xs plt.plot(xs,ys,'r',linewidth=4) plt.scatter(x,y); plt.show() Real Kobe Bryant Data Let's work with some real data. Kobe Bryant retired in 2016 with 33643 total points which is the third highest total points in NBA history . How many more years would Kobe Bryant have to had played to pass Kareem Abdul-Jabbar's record 38387 points? Kobe Bryant's peak was the 2005-2006 NBA season. Let's look at Kobe Bryant's total games played and points per game from 2006 to 2016. years = np.array([2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]) games = [80,77,82,82,73,82,58,78,6,35,66] points = np.array([35.4,31.6,28.3,26.8,27,25.3,27.9,27.3,13.8,22.3,17.6]) fig = plt.figure(figsize=(12,10)) axs = fig.subplots(2,1,sharex=True) axs[0].plot(years,points,'b.',ms=15) axs[0].set_title('Kobe Bryant, Points per Game') axs[0].set_ylim([0,40]) axs[0].grid(True) axs[1].bar(years,games) axs[1].set_title('Kobe Bryant, Games Played') axs[1].set_ylim([0,100]) axs[1].grid(True) plt.show() Kobe was injured for most of the 2013-2014 NBA season and played only 6 games. This is an outlier and so we can drop this data point: years = np.array([2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2015, 2016]) games = np.array([80,77,82,82,73,82,58,78,35,66]) points = np.array([35.4,31.6,28.3,26.8,27,25.3,27.9,27.3,22.3,17.6]) Let's compute the average games played per season over this period: avg_games_per_year = np.mean(games) print(avg_games_per_year) 71.3 Compute the linear model for points per game: X = np.column_stack([np.ones(len(years)),years]) a = la.solve(X.T @ X, X.T @ points) model = a[0] + a[1]*years plt.plot(years,model,years,points,'b.',ms=15) plt.title('Kobe Bryant, Points per Game') plt.ylim([0,40]) plt.grid(True) plt.show() Now we can extrapolate to future years and multiply points per games by games per season and compute the cumulative sum to see Kobe's total points: future_years = np.array([2017,2018,2019,2020,2021]) future_points = (a[0] + a[1]*future_years)*avg_games_per_year total_points = 33643 + np.cumsum(future_points) kareem = 38387*np.ones(len(future_years)) plt.plot(future_years,total_points,future_years,kareem) plt.grid(True) plt.xticks(future_years) plt.title('Kobe Bryant Total Points Prediction') plt.show() Only 4 more years!","title":"Examples"},{"location":"linear-algebra/applications/#polynomial-regression","text":"","title":"Polynomial Regression"},{"location":"linear-algebra/applications/#formulation_2","text":"The same idea works for fitting a degree $d$ polynomial model $$ y = a_0 + a_1x + a_2x^2 + \\cdots + a_dx^d $$ to a set of $n+1$ data points $$ (x_0,y_0), (x_1,y_1), \\dots , (x_n,y_n) $$ We form the matrices as before but now the Vandermonde matrix $X$ has $d+1$ columns $$ X = \\begin{bmatrix} 1 & x_0 & x_0^2 & \\cdots & x_0^d \\\\ 1 & x_1 & x_1^2 & \\cdots & x_1^d \\\\ & \\vdots & & & \\vdots \\\\ 1 & x_n & x_n^2 & \\cdots & x_n^d \\end{bmatrix} \\ , \\ \\ \\mathbf{y} = \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\ , \\ \\ \\mathbf{a} = \\begin{bmatrix} a_0 \\\\ a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_d \\end{bmatrix} $$ The coefficients $\\mathbf{a} = [a_0,a_1,a_2,\\dots,a_d]^T$ which minimize the sum of squared errors $SSE$ is the unique solution of the linear system $$ \\left( X^T X \\right) \\mathbf{a} = \\left( X^T \\right) \\mathbf{y} $$","title":"Formulation"},{"location":"linear-algebra/applications/#example","text":"Fake Noisy Quadratic Data Let's build some fake data using a quadratic model $y = a_0 + a_1x + a_2x^2 + \\epsilon$ and use linear regression to retrieve the coefficients $a_0$, $a_1$ and $a_2$. a0 = 3 a1 = 5 a2 = 8 N = 1000 x = 2*np.random.rand(N) - 1 # Random numbers in the interval (-1,1) noise = np.random.randn(N) y = a0 + a1*x + a2*x**2 + noise plt.scatter(x,y,alpha=0.5,lw=0); plt.show() Construct the matrix $X$: X = np.column_stack([np.ones(N),x,x**2]) Use scipy.linalg.solve to solve $\\left( X^T X \\right) \\mathbf{a} = \\left( X^T \\right) \\mathbf{y}$: a = la.solve((X.T @ X),X.T @ y) Plot the result: xs = np.linspace(-1,1,20) ys = a[0] + a[1]*xs + a[2]*xs**2 plt.plot(xs,ys,'r',linewidth=4) plt.scatter(x,y,alpha=0.5,lw=0) plt.show()","title":"Example"},{"location":"linear-algebra/applications/#graph-theory","text":"A graph is a set of vertices and a set of edges connecting some of the vertices. We will consider simple, undirected, connected graphs: a graph is simple if there are no loops or multiple edges between vertices a graph is undirected if the edges do not have an orientation a graph is connected if each vertex is connected to every other vertex in the graph by a path We can visualize a graph as a set of vertices and edges and answer questions about the graph just by looking at it. However this becomes much more difficult with a large graphs such as a social network graph . Instead, we construct matrices from the graph such as the adjacency matrix and the Laplacian matrix and study their properties. Spectral graph theory is the study of the eigenvalues of the adjacency matrix (and other associated matrices) and the relationships to the structure of $G$.","title":"Graph Theory"},{"location":"linear-algebra/applications/#networkx","text":"Let's use the Python package NetworkX to construct and visualize some simple graphs. import networkx as nx","title":"NetworkX"},{"location":"linear-algebra/applications/#adjacency-matrix","text":"The adjacency matrix $A_G$ of a graph $G$ with $n$ vertices is the square matrix of size $n$ such that $A_{i,j} = 1$ if vertices $i$ and $j$ are connected by an edge, and $A_{i,j} = 0$ otherwise. We can use networkx to create the adjacency matrix of a graph $G$. The function nx.adjacency_matrix returns a sparse matrix and we convert it to a regular NumPy array using the todense method. For example, plot the complete graph with 5 vertices and compute the adjacency matrix: G = nx.complete_graph(5) nx.draw(G,with_labels=True) A = nx.adjacency_matrix(G).todense() print(A) [[0 1 1 1 1] [1 0 1 1 1] [1 1 0 1 1] [1 1 1 0 1] [1 1 1 1 0]]","title":"Adjacency Matrix"},{"location":"linear-algebra/applications/#length-of-the-shortest-path","text":"The length of the shortest path between vertices in a simple, undirected graph $G$ can be easily computed from the adjacency matrix $A_G$. In particular, the length of shortest path from vertex $i$ to vertex $j$ ($i\\not=j$) is the smallest positive integer $k$ such that $A^k_{i,j} \\not= 0$. Plot the dodecahedral graph : G = nx.dodecahedral_graph() nx.draw(G,with_labels=True) A = nx.adjacency_matrix(G).todense() print(A) [[0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1] [1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0] [0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1] [0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0] [0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0] [0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0] [0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0] [1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0] [0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0] [0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0] [0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0] [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1] [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]] With this labelling, let's find the length of the shortest path from vertex $0$ to $15$: i = 0 j = 15 k = 1 Ak = A while Ak[i,j] == 0: Ak = Ak @ A k = k + 1 print('Length of the shortest path is',k) Length of the shortest path is 5","title":"Length of the Shortest Path"},{"location":"linear-algebra/applications/#triangles-in-a-graph","text":"A simple result in spectral graph theory is the number of triangles in a graph $T(G)$ is given by: $$ T(G) = \\frac{1}{6} ( \\lambda_1^3 + \\lambda_2^3 + \\cdots + \\lambda_n^3) $$ where $\\lambda_1 \\leq \\lambda_2 \\leq \\cdots \\leq \\lambda_n$ are the eigenvalues of the adjacency matrix. Let's verify this for the simplest case, the complete graph on 3 vertices: C3 = nx.complete_graph(3) nx.draw(C3,with_labels=True) A3 = nx.adjacency_matrix(C3).todense() eigvals, eigvecs = la.eig(A3) int(np.round(np.sum(eigvals.real**3)/6,0)) 1 Let's compute the number of triangles in the complete graph 7 vertices: C7 = nx.complete_graph(7) nx.draw(C7,with_labels=True) A7 = nx.adjacency_matrix(C7).todense() eigvals, eigvecs = la.eig(A7) int(np.round(np.sum(eigvals.real**3)/6,0)) 35 There are 35 triangles in the complete graph with 7 vertices! Let's write a function called triangles which takes a square matrix M and return the sum $$ \\frac{1}{6} ( \\lambda_1^3 + \\lambda_2^3 + \\cdots + \\lambda_n^3) $$ where $\\lambda_i$ are the eigenvalues of the symmetric matrix $A = (M + M^T)/2$. Note that $M = A$ if $M$ is symmetric. The return value is the number of triangles in the graph $G$ if the input $M$ is the adjacency matrix. def triangles(M): A = (M + M.T)/2 eigvals, eigvecs = la.eig(A) eigvals = eigvals.real return int(np.round(np.sum(eigvals**3)/6,0)) Next, let's try a Turan graph . G = nx.turan_graph(10,5) nx.draw(G,with_labels=True) A = nx.adjacency_matrix(G).todense() print(A) [[0 0 1 1 1 1 1 1 1 1] [0 0 1 1 1 1 1 1 1 1] [1 1 0 0 1 1 1 1 1 1] [1 1 0 0 1 1 1 1 1 1] [1 1 1 1 0 0 1 1 1 1] [1 1 1 1 0 0 1 1 1 1] [1 1 1 1 1 1 0 0 1 1] [1 1 1 1 1 1 0 0 1 1] [1 1 1 1 1 1 1 1 0 0] [1 1 1 1 1 1 1 1 0 0]] Find the number of triangles: triangles(A) 80 Finally, let's compute the number of triangles in the dodecahedral graph: G = nx.dodecahedral_graph() nx.draw(G,with_labels=True) A = nx.adjacency_matrix(G).todense() print(A) [[0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1] [1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0] [0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1] [0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0] [0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0] [0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0] [0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0] [1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0] [0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0] [0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0] [0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0] [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1] [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]] np.round(triangles(A),2) 0","title":"Triangles in a Graph"},{"location":"linear-algebra/applications/#exercises","text":"Under Construction","title":"Exercises"},{"location":"linear-algebra/eigenvalues-eigenvectors/","text":"Eigenvalues and Eigenvectors import numpy as np import matplotlib.pyplot as plt import scipy.linalg as la Definition Let $A$ be a square matrix. A non-zero vector $\\mathbf{v}$ is an eigenvector for $A$ with eigenvalue $\\lambda$ if $$ A\\mathbf{v} = \\lambda \\mathbf{v} $$ Rearranging the equation, we see that $\\mathbf{v}$ is a solution of the homogeneous system of equations $$ \\left( A - \\lambda I \\right) \\mathbf{v} = \\mathbf{0} $$ where $I$ is the identity matrix of size $n$. Non-trivial solutions exist only if the matrix $A - \\lambda I$ is singular which means $\\mathrm{det}(A - \\lambda I) = 0$. Therefore eigenvalues of $A$ are roots of the characteristic polynomial $$ p(\\lambda) = \\mathrm{det}(A - \\lambda I) $$ scipy.linalg.eig The function scipy.linalg.eig computes eigenvalues and eigenvectors of a square matrix $A$. Let's consider a simple example with a diagonal matrix: A = np.array([[1,0],[0,-2]]) print(A) [[ 1 0] [ 0 -2]] The function la.eig returns a tuple (eigvals,eigvecs) where eigvals is a 1D NumPy array of complex numbers giving the eigenvalues of $A$, and eigvecs is a 2D NumPy array with the corresponding eigenvectors in the columns: results = la.eig(A) The eigenvalues of $A$ are: print(results[0]) [ 1.+0.j -2.+0.j] The corresponding eigenvectors are: print(results[1]) [[1. 0.] [0. 1.]] We can unpack the tuple : eigvals, eigvecs = la.eig(A) print(eigvals) [ 1.+0.j -2.+0.j] print(eigvecs) [[1. 0.] [0. 1.]] If we know that the eigenvalues are real numbers (ie. if $A$ is symmetric), then we can use the NumPy array method .real to convert the array of eigenvalues to real numbers: eigvals = eigvals.real print(eigvals) [ 1. -2.] Notice that the position of an eigenvalue in the array eigvals correspond to the column in eigvecs with its eigenvector: lambda1 = eigvals[1] print(lambda1) -2.0 v1 = eigvecs[:,1].reshape(2,1) print(v1) [[0.] [1.]] A @ v1 array([[ 0.], [-2.]]) lambda1 * v1 array([[-0.], [-2.]]) Examples Symmetric Matrices The eigenvalues of a symmetric matrix are always real and the eigenvectors are always orthogonal! Let's verify these facts with some random matrices: n = 4 P = np.random.randint(0,10,(n,n)) print(P) [[7 0 6 2] [9 5 1 3] [0 2 2 5] [6 8 8 6]] Create the symmetric matrix $S = P P^T$: S = P @ P.T print(S) [[ 89 75 22 102] [ 75 116 27 120] [ 22 27 33 62] [102 120 62 200]] Let's unpack the eigenvalues and eigenvectors of $S$: evals, evecs = la.eig(S) print(evals) [361.75382302+0.j 42.74593101+0.j 26.33718907+0.j 7.16305691+0.j] The eigenvalues all have zero imaginary part and so they are indeed real numbers: evals = evals.real print(evals) [361.75382302 42.74593101 26.33718907 7.16305691] The corresponding eigenvectors of $A$ are: print(evecs) [[-0.42552429 -0.42476765 0.76464379 -0.23199439] [-0.50507589 -0.54267519 -0.64193252 -0.19576676] [-0.20612674 0.54869183 -0.05515612 -0.80833585] [-0.72203822 0.4733005 0.01415338 0.50442752]] Let's check that the eigenvectors are orthogonal to each other: v1 = evecs[:,0] # First column is the first eigenvector print(v1) [-0.42552429 -0.50507589 -0.20612674 -0.72203822] v2 = evecs[:,1] # Second column is the second eigenvector print(v2) [-0.42476765 -0.54267519 0.54869183 0.4733005 ] v1 @ v2 -1.1102230246251565e-16 The dot product of eigenvectors $\\mathbf{v}_1$ and $\\mathbf{v}_2$ is zero (the number above is very close to zero and is due to rounding errors in the computations) and so they are orthogonal! Diagonalization A square matrix $M$ is diagonalizable if it is similar to a diagonal matrix. In other words, $M$ is diagonalizable if there exists an invertible matrix $P$ such that $D = P^{-1}MP$ is a diagonal matrix. A beautiful result in linear algebra is that a square matrix $M$ of size $n$ is diagonalizable if and only if $M$ has $n$ independent eigevectors. Furthermore, $M = PDP^{-1}$ where the columns of $P$ are the eigenvectors of $M$ and $D$ has corresponding eigenvalues along the diagonal. Let's use this to construct a matrix with given eigenvalues $\\lambda_1 = 3, \\lambda_2 = 1$, and eigenvectors $v_1 = [1,1]^T, v_2 = [1,-1]^T$. P = np.array([[1,1],[1,-1]]) print(P) [[ 1 1] [ 1 -1]] D = np.diag((3,1)) print(D) [[3 0] [0 1]] M = P @ D @ la.inv(P) print(M) [[2. 1.] [1. 2.]] Let's verify that the eigenvalues of $M$ are 3 and 1: evals, evecs = la.eig(M) print(evals) [3.+0.j 1.+0.j] Verify the eigenvectors: print(evecs) [[ 0.70710678 -0.70710678] [ 0.70710678 0.70710678]] Matrix Powers Let $M$ be a square matrix. Computing powers of $M$ by matrix multiplication $$ M^k = \\underbrace{M M \\cdots M}_k $$ is computationally expensive. Instead, let's use diagonalization to compute $M^k$ more efficiently $$ M^k = \\left( P D P^{-1} \\right)^k = \\underbrace{P D P^{-1} P D P^{-1} \\cdots P D P^{-1}}_k = P D^k P^{-1} $$ Let's compute $M^{20}$ both ways and compare execution time. Pinv = la.inv(P) k = 20 %%timeit result = M.copy() for _ in range(1,k): result = result @ M 42.1 \u00b5s \u00b1 11.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each) Let's use diagonalization to do the same computation. %%timeit P @ D**k @ Pinv 6.42 \u00b5s \u00b1 1.36 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each) Diagonalization computes $M^{k}$ much faster! Exercises Under construction","title":"Eigenvalues and Eigenvectors"},{"location":"linear-algebra/eigenvalues-eigenvectors/#eigenvalues-and-eigenvectors","text":"import numpy as np import matplotlib.pyplot as plt import scipy.linalg as la","title":"Eigenvalues and Eigenvectors"},{"location":"linear-algebra/eigenvalues-eigenvectors/#definition","text":"Let $A$ be a square matrix. A non-zero vector $\\mathbf{v}$ is an eigenvector for $A$ with eigenvalue $\\lambda$ if $$ A\\mathbf{v} = \\lambda \\mathbf{v} $$ Rearranging the equation, we see that $\\mathbf{v}$ is a solution of the homogeneous system of equations $$ \\left( A - \\lambda I \\right) \\mathbf{v} = \\mathbf{0} $$ where $I$ is the identity matrix of size $n$. Non-trivial solutions exist only if the matrix $A - \\lambda I$ is singular which means $\\mathrm{det}(A - \\lambda I) = 0$. Therefore eigenvalues of $A$ are roots of the characteristic polynomial $$ p(\\lambda) = \\mathrm{det}(A - \\lambda I) $$","title":"Definition"},{"location":"linear-algebra/eigenvalues-eigenvectors/#scipylinalgeig","text":"The function scipy.linalg.eig computes eigenvalues and eigenvectors of a square matrix $A$. Let's consider a simple example with a diagonal matrix: A = np.array([[1,0],[0,-2]]) print(A) [[ 1 0] [ 0 -2]] The function la.eig returns a tuple (eigvals,eigvecs) where eigvals is a 1D NumPy array of complex numbers giving the eigenvalues of $A$, and eigvecs is a 2D NumPy array with the corresponding eigenvectors in the columns: results = la.eig(A) The eigenvalues of $A$ are: print(results[0]) [ 1.+0.j -2.+0.j] The corresponding eigenvectors are: print(results[1]) [[1. 0.] [0. 1.]] We can unpack the tuple : eigvals, eigvecs = la.eig(A) print(eigvals) [ 1.+0.j -2.+0.j] print(eigvecs) [[1. 0.] [0. 1.]] If we know that the eigenvalues are real numbers (ie. if $A$ is symmetric), then we can use the NumPy array method .real to convert the array of eigenvalues to real numbers: eigvals = eigvals.real print(eigvals) [ 1. -2.] Notice that the position of an eigenvalue in the array eigvals correspond to the column in eigvecs with its eigenvector: lambda1 = eigvals[1] print(lambda1) -2.0 v1 = eigvecs[:,1].reshape(2,1) print(v1) [[0.] [1.]] A @ v1 array([[ 0.], [-2.]]) lambda1 * v1 array([[-0.], [-2.]])","title":"scipy.linalg.eig"},{"location":"linear-algebra/eigenvalues-eigenvectors/#examples","text":"","title":"Examples"},{"location":"linear-algebra/eigenvalues-eigenvectors/#symmetric-matrices","text":"The eigenvalues of a symmetric matrix are always real and the eigenvectors are always orthogonal! Let's verify these facts with some random matrices: n = 4 P = np.random.randint(0,10,(n,n)) print(P) [[7 0 6 2] [9 5 1 3] [0 2 2 5] [6 8 8 6]] Create the symmetric matrix $S = P P^T$: S = P @ P.T print(S) [[ 89 75 22 102] [ 75 116 27 120] [ 22 27 33 62] [102 120 62 200]] Let's unpack the eigenvalues and eigenvectors of $S$: evals, evecs = la.eig(S) print(evals) [361.75382302+0.j 42.74593101+0.j 26.33718907+0.j 7.16305691+0.j] The eigenvalues all have zero imaginary part and so they are indeed real numbers: evals = evals.real print(evals) [361.75382302 42.74593101 26.33718907 7.16305691] The corresponding eigenvectors of $A$ are: print(evecs) [[-0.42552429 -0.42476765 0.76464379 -0.23199439] [-0.50507589 -0.54267519 -0.64193252 -0.19576676] [-0.20612674 0.54869183 -0.05515612 -0.80833585] [-0.72203822 0.4733005 0.01415338 0.50442752]] Let's check that the eigenvectors are orthogonal to each other: v1 = evecs[:,0] # First column is the first eigenvector print(v1) [-0.42552429 -0.50507589 -0.20612674 -0.72203822] v2 = evecs[:,1] # Second column is the second eigenvector print(v2) [-0.42476765 -0.54267519 0.54869183 0.4733005 ] v1 @ v2 -1.1102230246251565e-16 The dot product of eigenvectors $\\mathbf{v}_1$ and $\\mathbf{v}_2$ is zero (the number above is very close to zero and is due to rounding errors in the computations) and so they are orthogonal!","title":"Symmetric Matrices"},{"location":"linear-algebra/eigenvalues-eigenvectors/#diagonalization","text":"A square matrix $M$ is diagonalizable if it is similar to a diagonal matrix. In other words, $M$ is diagonalizable if there exists an invertible matrix $P$ such that $D = P^{-1}MP$ is a diagonal matrix. A beautiful result in linear algebra is that a square matrix $M$ of size $n$ is diagonalizable if and only if $M$ has $n$ independent eigevectors. Furthermore, $M = PDP^{-1}$ where the columns of $P$ are the eigenvectors of $M$ and $D$ has corresponding eigenvalues along the diagonal. Let's use this to construct a matrix with given eigenvalues $\\lambda_1 = 3, \\lambda_2 = 1$, and eigenvectors $v_1 = [1,1]^T, v_2 = [1,-1]^T$. P = np.array([[1,1],[1,-1]]) print(P) [[ 1 1] [ 1 -1]] D = np.diag((3,1)) print(D) [[3 0] [0 1]] M = P @ D @ la.inv(P) print(M) [[2. 1.] [1. 2.]] Let's verify that the eigenvalues of $M$ are 3 and 1: evals, evecs = la.eig(M) print(evals) [3.+0.j 1.+0.j] Verify the eigenvectors: print(evecs) [[ 0.70710678 -0.70710678] [ 0.70710678 0.70710678]]","title":"Diagonalization"},{"location":"linear-algebra/eigenvalues-eigenvectors/#matrix-powers","text":"Let $M$ be a square matrix. Computing powers of $M$ by matrix multiplication $$ M^k = \\underbrace{M M \\cdots M}_k $$ is computationally expensive. Instead, let's use diagonalization to compute $M^k$ more efficiently $$ M^k = \\left( P D P^{-1} \\right)^k = \\underbrace{P D P^{-1} P D P^{-1} \\cdots P D P^{-1}}_k = P D^k P^{-1} $$ Let's compute $M^{20}$ both ways and compare execution time. Pinv = la.inv(P) k = 20 %%timeit result = M.copy() for _ in range(1,k): result = result @ M 42.1 \u00b5s \u00b1 11.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each) Let's use diagonalization to do the same computation. %%timeit P @ D**k @ Pinv 6.42 \u00b5s \u00b1 1.36 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each) Diagonalization computes $M^{k}$ much faster!","title":"Matrix Powers"},{"location":"linear-algebra/eigenvalues-eigenvectors/#exercises","text":"Under construction","title":"Exercises"},{"location":"linear-algebra/linear-algebra-scipy/","text":"Linear Algebra with SciPy The main Python package for linear algebra is the SciPy subpackage scipy.linalg which builds on NumPy. Let's import both packages: import numpy as np import scipy.linalg as la NumPy Arrays Let's begin with a quick review of NumPy arrays . We can think of a 1D NumPy array as a list of numbers. We can think of a 2D NumPy array as a matrix. And we can think of a 3D array as a cube of numbers. When we select a row or column from a 2D NumPy array, the result is a 1D NumPy array (called a slice ). This is different from MATLAB where when you select a column from a matrix it's returned as a column vector which is a 2D MATLAB matrix. It can get a bit confusing and so we need to keep track of the shape, size and dimension of our NumPy arrays. Array Attributes Create a 1D (one-dimensional) NumPy array and verify its dimensions, shape and size. a = np.array([1,3,-2,1]) print(a) [ 1 3 -2 1] Verify the number of dimensions: a.ndim 1 Verify the shape of the array: a.shape (4,) The shape of an array is returned as a Python tuple . The output in the cell above is a tuple of length 1. And we verify the size of the array (ie. the total number of entries in the array): a.size 4 Create a 2D (two-dimensional) NumPy array (ie. matrix): M = np.array([[1,2],[3,7],[-1,5]]) print(M) [[ 1 2] [ 3 7] [-1 5]] Verify the number of dimensions: M.ndim 2 Verify the shape of the array: M.shape (3, 2) Finally, verify the total number of entries in the array: M.size 6 Select a row or column from a 2D NumPy array and we get a 1D array: col = M[:,1] print(col) [2 7 5] Verify the number of dimensions of the slice: col.ndim 1 Verify the shape and size of the slice: col.shape (3,) col.size 3 When we select a row of column from a 2D NumPy array, the result is a 1D NumPy array. However, we may want to select a column as a 2D column vector. This requires us to use the reshape method. For example, create a 2D column vector from the 1D slice selected from the matrix M above: print(col) [2 7 5] column = np.array([2,7,5]).reshape(3,1) print(column) [[2] [7] [5]] Verify the dimensions, shape and size of the array: print('Dimensions:', column.ndim) print('Shape:', column.shape) print('Size:', column.size) Dimensions: 2 Shape: (3, 1) Size: 3 The variables col and column are different types of objects even though they have the \"same\" data. print(col) [2 7 5] print('Dimensions:',col.ndim) print('Shape:',col.shape) print('Size:',col.size) Dimensions: 1 Shape: (3,) Size: 3 Matrix Operations and Functions Arithmetic Operations Recall that arithmetic array operations + , - , / , * and ** are performed elementwise on NumPy arrays. Let's create a NumPy array and do some computations: M = np.array([[3,4],[-1,5]]) print(M) [[ 3 4] [-1 5]] M * M array([[ 9, 16], [ 1, 25]]) Matrix Multiplication We use the @ operator to do matrix multiplication with NumPy arrays: M @ M array([[ 5, 32], [-8, 21]]) Let's compute $2I + 3A - AB$ for $$ A = \\begin{bmatrix} 1 & 3 \\\\ -1 & 7 \\end{bmatrix} \\ \\ \\ \\ B = \\begin{bmatrix} 5 & 2 \\\\ 1 & 2 \\end{bmatrix} $$ and $I$ is the identity matrix of size 2: A = np.array([[1,3],[-1,7]]) print(A) [[ 1 3] [-1 7]] B = np.array([[5,2],[1,2]]) print(B) [[5 2] [1 2]] I = np.eye(2) print(I) [[1. 0.] [0. 1.]] 2*I + 3*A - A@B array([[-3., 1.], [-5., 11.]]) Matrix Powers There's no symbol for matrix powers and so we must import the function matrix_power from the subpackage numpy.linalg . from numpy.linalg import matrix_power as mpow M = np.array([[3,4],[-1,5]]) print(M) [[ 3 4] [-1 5]] mpow(M,2) array([[ 5, 32], [-8, 21]]) mpow(M,5) array([[-1525, 3236], [ -809, 93]]) Compare with the matrix multiplcation operator: M @ M @ M @ M @ M array([[-1525, 3236], [ -809, 93]]) mpow(M,3) array([[-17, 180], [-45, 73]]) M @ M @ M array([[-17, 180], [-45, 73]]) Tranpose We can take the transpose with .T attribute: print(M) [[ 3 4] [-1 5]] print(M.T) [[ 3 -1] [ 4 5]] Notice that $M M^T$ is a symmetric matrix: M @ M.T array([[25, 17], [17, 26]]) Inverse We can find the inverse using the function scipy.linalg.inv : A = np.array([[1,2],[3,4]]) print(A) [[1 2] [3 4]] la.inv(A) array([[-2. , 1. ], [ 1.5, -0.5]]) Trace We can find the trace of a matrix using the function numpy.trace : np.trace(A) 5 Norm Under construction Determinant We find the determinant using the function scipy.linalg.det : A = np.array([[1,2],[3,4]]) print(A) [[1 2] [3 4]] la.det(A) -2.0 Dot Product Under construction Examples Characteristic Polynomials and Cayley-Hamilton Theorem The characteristic polynomial of a 2 by 2 square matrix $A$ is $$ p_A(\\lambda) = \\det(A - \\lambda I) = \\lambda^2 - \\mathrm{tr}(A) \\lambda + \\mathrm{det}(A) $$ The Cayley-Hamilton Theorem states that any square matrix satisfies its characteristic polynomial. For a matrix $A$ of size 2, this means that $$ p_A(A) = A^2 - \\mathrm{tr}(A) A + \\mathrm{det}(A) I = 0 $$ Let's verify the Cayley-Hamilton Theorem for a few different matrices. print(A) [[1 2] [3 4]] trace_A = np.trace(A) det_A = la.det(A) I = np.eye(2) A @ A - trace_A * A + det_A * I array([[0., 0.], [0., 0.]]) Let's do this again for some random matrices: N = np.random.randint(0,10,[2,2]) print(N) [[1 9] [4 3]] trace_N = np.trace(N) det_N = la.det(N) I = np.eye(2) N @ N - trace_N * N + det_N * I array([[0., 0.], [0., 0.]]) Projections The formula to project a vector $v$ onto a vector $w$ is $$ \\mathrm{proj}_w(v) = \\frac{v \\cdot w}{w \\cdot w} w $$ Let's write a function called proj which computes the projection $v$ onto $w$. def proj(v,w): '''Project vector v onto w.''' v = np.array(v) w = np.array(w) return np.sum(v * w)/np.sum(w * w) * w # or (v @ w)/(w @ w) * w proj([1,2,3],[1,1,1]) array([2., 2., 2.]) Exercises Exercise 1. Write a function which takes an input parameter $A$, $i$ and $j$ and returns the dot product of the $i$th and $j$th row (indexing starts at 0). Exercise 2. Compute the matrix equation $AB + 2B^2 - I$ for matrices $$A = \\begin{bmatrix} 3 & 4 \\\\ -1 & 2 \\end{bmatrix}\\hspace{20mm} B = \\begin{bmatrix} 5 & 2 \\\\ 8 & -3 \\end{bmatrix}$$","title":"Linear Algebra with SciPy"},{"location":"linear-algebra/linear-algebra-scipy/#linear-algebra-with-scipy","text":"The main Python package for linear algebra is the SciPy subpackage scipy.linalg which builds on NumPy. Let's import both packages: import numpy as np import scipy.linalg as la","title":"Linear Algebra with SciPy"},{"location":"linear-algebra/linear-algebra-scipy/#numpy-arrays","text":"Let's begin with a quick review of NumPy arrays . We can think of a 1D NumPy array as a list of numbers. We can think of a 2D NumPy array as a matrix. And we can think of a 3D array as a cube of numbers. When we select a row or column from a 2D NumPy array, the result is a 1D NumPy array (called a slice ). This is different from MATLAB where when you select a column from a matrix it's returned as a column vector which is a 2D MATLAB matrix. It can get a bit confusing and so we need to keep track of the shape, size and dimension of our NumPy arrays.","title":"NumPy Arrays"},{"location":"linear-algebra/linear-algebra-scipy/#array-attributes","text":"Create a 1D (one-dimensional) NumPy array and verify its dimensions, shape and size. a = np.array([1,3,-2,1]) print(a) [ 1 3 -2 1] Verify the number of dimensions: a.ndim 1 Verify the shape of the array: a.shape (4,) The shape of an array is returned as a Python tuple . The output in the cell above is a tuple of length 1. And we verify the size of the array (ie. the total number of entries in the array): a.size 4 Create a 2D (two-dimensional) NumPy array (ie. matrix): M = np.array([[1,2],[3,7],[-1,5]]) print(M) [[ 1 2] [ 3 7] [-1 5]] Verify the number of dimensions: M.ndim 2 Verify the shape of the array: M.shape (3, 2) Finally, verify the total number of entries in the array: M.size 6 Select a row or column from a 2D NumPy array and we get a 1D array: col = M[:,1] print(col) [2 7 5] Verify the number of dimensions of the slice: col.ndim 1 Verify the shape and size of the slice: col.shape (3,) col.size 3 When we select a row of column from a 2D NumPy array, the result is a 1D NumPy array. However, we may want to select a column as a 2D column vector. This requires us to use the reshape method. For example, create a 2D column vector from the 1D slice selected from the matrix M above: print(col) [2 7 5] column = np.array([2,7,5]).reshape(3,1) print(column) [[2] [7] [5]] Verify the dimensions, shape and size of the array: print('Dimensions:', column.ndim) print('Shape:', column.shape) print('Size:', column.size) Dimensions: 2 Shape: (3, 1) Size: 3 The variables col and column are different types of objects even though they have the \"same\" data. print(col) [2 7 5] print('Dimensions:',col.ndim) print('Shape:',col.shape) print('Size:',col.size) Dimensions: 1 Shape: (3,) Size: 3","title":"Array Attributes"},{"location":"linear-algebra/linear-algebra-scipy/#matrix-operations-and-functions","text":"","title":"Matrix Operations and Functions"},{"location":"linear-algebra/linear-algebra-scipy/#arithmetic-operations","text":"Recall that arithmetic array operations + , - , / , * and ** are performed elementwise on NumPy arrays. Let's create a NumPy array and do some computations: M = np.array([[3,4],[-1,5]]) print(M) [[ 3 4] [-1 5]] M * M array([[ 9, 16], [ 1, 25]])","title":"Arithmetic Operations"},{"location":"linear-algebra/linear-algebra-scipy/#matrix-multiplication","text":"We use the @ operator to do matrix multiplication with NumPy arrays: M @ M array([[ 5, 32], [-8, 21]]) Let's compute $2I + 3A - AB$ for $$ A = \\begin{bmatrix} 1 & 3 \\\\ -1 & 7 \\end{bmatrix} \\ \\ \\ \\ B = \\begin{bmatrix} 5 & 2 \\\\ 1 & 2 \\end{bmatrix} $$ and $I$ is the identity matrix of size 2: A = np.array([[1,3],[-1,7]]) print(A) [[ 1 3] [-1 7]] B = np.array([[5,2],[1,2]]) print(B) [[5 2] [1 2]] I = np.eye(2) print(I) [[1. 0.] [0. 1.]] 2*I + 3*A - A@B array([[-3., 1.], [-5., 11.]])","title":"Matrix Multiplication"},{"location":"linear-algebra/linear-algebra-scipy/#matrix-powers","text":"There's no symbol for matrix powers and so we must import the function matrix_power from the subpackage numpy.linalg . from numpy.linalg import matrix_power as mpow M = np.array([[3,4],[-1,5]]) print(M) [[ 3 4] [-1 5]] mpow(M,2) array([[ 5, 32], [-8, 21]]) mpow(M,5) array([[-1525, 3236], [ -809, 93]]) Compare with the matrix multiplcation operator: M @ M @ M @ M @ M array([[-1525, 3236], [ -809, 93]]) mpow(M,3) array([[-17, 180], [-45, 73]]) M @ M @ M array([[-17, 180], [-45, 73]])","title":"Matrix Powers"},{"location":"linear-algebra/linear-algebra-scipy/#tranpose","text":"We can take the transpose with .T attribute: print(M) [[ 3 4] [-1 5]] print(M.T) [[ 3 -1] [ 4 5]] Notice that $M M^T$ is a symmetric matrix: M @ M.T array([[25, 17], [17, 26]])","title":"Tranpose"},{"location":"linear-algebra/linear-algebra-scipy/#inverse","text":"We can find the inverse using the function scipy.linalg.inv : A = np.array([[1,2],[3,4]]) print(A) [[1 2] [3 4]] la.inv(A) array([[-2. , 1. ], [ 1.5, -0.5]])","title":"Inverse"},{"location":"linear-algebra/linear-algebra-scipy/#trace","text":"We can find the trace of a matrix using the function numpy.trace : np.trace(A) 5","title":"Trace"},{"location":"linear-algebra/linear-algebra-scipy/#norm","text":"Under construction","title":"Norm"},{"location":"linear-algebra/linear-algebra-scipy/#determinant","text":"We find the determinant using the function scipy.linalg.det : A = np.array([[1,2],[3,4]]) print(A) [[1 2] [3 4]] la.det(A) -2.0","title":"Determinant"},{"location":"linear-algebra/linear-algebra-scipy/#dot-product","text":"Under construction","title":"Dot Product"},{"location":"linear-algebra/linear-algebra-scipy/#examples","text":"","title":"Examples"},{"location":"linear-algebra/linear-algebra-scipy/#characteristic-polynomials-and-cayley-hamilton-theorem","text":"The characteristic polynomial of a 2 by 2 square matrix $A$ is $$ p_A(\\lambda) = \\det(A - \\lambda I) = \\lambda^2 - \\mathrm{tr}(A) \\lambda + \\mathrm{det}(A) $$ The Cayley-Hamilton Theorem states that any square matrix satisfies its characteristic polynomial. For a matrix $A$ of size 2, this means that $$ p_A(A) = A^2 - \\mathrm{tr}(A) A + \\mathrm{det}(A) I = 0 $$ Let's verify the Cayley-Hamilton Theorem for a few different matrices. print(A) [[1 2] [3 4]] trace_A = np.trace(A) det_A = la.det(A) I = np.eye(2) A @ A - trace_A * A + det_A * I array([[0., 0.], [0., 0.]]) Let's do this again for some random matrices: N = np.random.randint(0,10,[2,2]) print(N) [[1 9] [4 3]] trace_N = np.trace(N) det_N = la.det(N) I = np.eye(2) N @ N - trace_N * N + det_N * I array([[0., 0.], [0., 0.]])","title":"Characteristic Polynomials and Cayley-Hamilton Theorem"},{"location":"linear-algebra/linear-algebra-scipy/#projections","text":"The formula to project a vector $v$ onto a vector $w$ is $$ \\mathrm{proj}_w(v) = \\frac{v \\cdot w}{w \\cdot w} w $$ Let's write a function called proj which computes the projection $v$ onto $w$. def proj(v,w): '''Project vector v onto w.''' v = np.array(v) w = np.array(w) return np.sum(v * w)/np.sum(w * w) * w # or (v @ w)/(w @ w) * w proj([1,2,3],[1,1,1]) array([2., 2., 2.])","title":"Projections"},{"location":"linear-algebra/linear-algebra-scipy/#exercises","text":"Exercise 1. Write a function which takes an input parameter $A$, $i$ and $j$ and returns the dot product of the $i$th and $j$th row (indexing starts at 0). Exercise 2. Compute the matrix equation $AB + 2B^2 - I$ for matrices $$A = \\begin{bmatrix} 3 & 4 \\\\ -1 & 2 \\end{bmatrix}\\hspace{20mm} B = \\begin{bmatrix} 5 & 2 \\\\ 8 & -3 \\end{bmatrix}$$","title":"Exercises"},{"location":"linear-algebra/solving-linear-systems/","text":"Solving Linear Systems import numpy as np import matplotlib.pyplot as plt import scipy.linalg as la %matplotlib inline Linear Systems A linear system of equations is a collection of linear equations \\begin{align} a_{0,0}x_0 + a_{0,1}x_2 + \\cdots + a_{0,n}x_n & = b_0 \\\\ a_{1,0}x_0 + a_{1,1}x_2 + \\cdots + a_{1,n}x_n & = b_1 \\\\ & \\vdots \\\\ a_{m,0}x_0 + a_{m,1}x_2 + \\cdots + a_{m,n}x_n & = b_m \\\\ \\end{align} In matrix notation, a linear system is $A \\mathbf{x}= \\mathbf{b}$ where $$ A = \\begin{bmatrix} a_{0,0} & a_{0,1} & \\cdots & a_{0,n} \\\\ a_{1,0} & a_{1,1} & \\cdots & a_{1,n} \\\\ \\vdots & & & \\vdots \\\\ a_{m,0} & a_{m,1} & \\cdots & a_{m,n} \\\\ \\end{bmatrix} \\ \\ , \\ \\ \\mathbf{x} = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\ \\ , \\ \\ \\mathbf{b} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} $$ Gaussian elimination The general procedure to solve a linear system of equation is called Gaussian elimination . The idea is to perform elementary row operations to reduce the system to its row echelon form and then solve. Elementary Row Operations Elementary row operations include: Add $k$ times row $j$ to row $i$. Multiply row $i$ by scalar $k$. Switch rows $i$ and $j$. Each of the elementary row operations is the result of matrix multiplication by an elementary matrix (on the left). To add $k$ times row $i$ to row $j$ in a matrix $A$, we multiply $A$ by the matrix $E$ where $E$ is equal to the identity matrix except the $i,j$ entry is $E_{i,j} = k$. For example, if $A$ is 3 by 3 and we want to add 3 times row 2 to row 0 (using 0 indexing) then $$ E_1 = \\begin{bmatrix} 1 & 0 & 3 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} $$ Let's verify the calculation: A = np.array([[1,1,2],[-1,3,1],[0,5,2]]) print(A) [[ 1 1 2] [-1 3 1] [ 0 5 2]] E1 = np.array([[1,0,3],[0,1,0],[0,0,1]]) print(E1) [[1 0 3] [0 1 0] [0 0 1]] E1 @ A array([[ 1, 16, 8], [-1, 3, 1], [ 0, 5, 2]]) To multiply $k$ times row $i$ in a matrix $A$, we multiply $A$ by the matrix $E$ where $E$ is equal to the identity matrix except the $,i,j$ entry is $E_{i,i} = k$. For example, if $A$ is 3 by 3 and we want to multiply row 1 by -2 then $$ E_2 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & -2 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} $$ Let's verify the calculation: E2 = np.array([[1,0,0],[0,-2,0],[0,0,1]]) print(E2) [[ 1 0 0] [ 0 -2 0] [ 0 0 1]] E2 @ A array([[ 1, 1, 2], [ 2, -6, -2], [ 0, 5, 2]]) Finally, to switch row $i$ and row $j$ in a matrix $A$, we multiply $A$ by the matrix $E$ where $E$ is equal to the identity matrix except $E_{i,i} = 0$, $E_{j,j} = 0$, $E_{i,j} = 1$ and $E_{j,i} = 1$. For example, if $A$ is 3 by 3 and we want to switch row 1 and row 2 then $$ E^3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix} $$ Let's verify the calculation: E3 = np.array([[1,0,0],[0,0,1],[0,1,0]]) print(E3) [[1 0 0] [0 0 1] [0 1 0]] E3 @ A array([[ 1, 1, 2], [ 0, 5, 2], [-1, 3, 1]]) Implementation Let's write function to implement the elementary row operations. First of all, let's write a function called add_rows which takes input parameters $A$, $k$, $i$ and $j$ and returns the NumPy array resulting from adding $k$ times row $j$ to row $i$ in the matrix $A$. If $i=j$, then let's say that the function scales row $i$ by $k+1$ since this would be the result of $k$ times row $i$ added to row $i$. def add_row(A,k,i,j): \"Add k times row j to row i in matrix A.\" n = A.shape[0] E = np.eye(n) if i == j: E[i,i] = k + 1 else: E[i,j] = k return E @ A Let's test our function: M = np.array([[1,1],[3,2]]) print(M) [[1 1] [3 2]] add_row(M,2,0,1) array([[7., 5.], [3., 2.]]) add_row(M,3,1,1) array([[ 1., 1.], [12., 8.]]) Let's write a function called scale_row which takes 3 input parameters $A$, $k$, and $i$ and returns the matrix that results from $k$ times row $i$ in the matrix $A$. def scale_row(A,k,i): \"Multiply row i by k in matrix A.\" n = A.shape[0] E = np.eye(n) E[i,i] = k return E @ A M = np.array([[3,1],[-2,7]]) print(M) [[ 3 1] [-2 7]] scale_row(M,3,1) array([[ 3., 1.], [-6., 21.]]) A = np.array([[1,1,1],[1,-1,0]]) print(A) [[ 1 1 1] [ 1 -1 0]] scale_row(A,5,1) array([[ 1., 1., 1.], [ 5., -5., 0.]]) Let's write a function called switch_rows which takes 3 input parameters $A$, $i$ and $j$ and returns the matrix that results from switching rows $i$ and $j$ in the matrix $A$. def switch_rows(A,i,j): \"Switch rows i and j in matrix A.\" n = A.shape[0] E = np.eye(n) E[i,i] = 0 E[j,j] = 0 E[i,j] = 1 E[j,i] = 1 return E @ A A = np.array([[1,1,1],[1,-1,0]]) print(A) [[ 1 1 1] [ 1 -1 0]] switch_rows(A,0,1) array([[ 1., -1., 0.], [ 1., 1., 1.]]) Examples Find the Inverse Let's apply our functions to the augmented matrix $[M \\ | \\ I]$ to find the inverse of the matrix $M$: M = np.array([[5,4,2],[-1,2,1],[1,1,1]]) print(M) [[ 5 4 2] [-1 2 1] [ 1 1 1]] A = np.hstack([M,np.eye(3)]) print(A) [[ 5. 4. 2. 1. 0. 0.] [-1. 2. 1. 0. 1. 0.] [ 1. 1. 1. 0. 0. 1.]] A1 = switch_rows(A,0,2) print(A1) [[ 1. 1. 1. 0. 0. 1.] [-1. 2. 1. 0. 1. 0.] [ 5. 4. 2. 1. 0. 0.]] A2 = add_row(A1,1,1,0) print(A2) [[1. 1. 1. 0. 0. 1.] [0. 3. 2. 0. 1. 1.] [5. 4. 2. 1. 0. 0.]] A3 = add_row(A2,-5,2,0) print(A3) [[ 1. 1. 1. 0. 0. 1.] [ 0. 3. 2. 0. 1. 1.] [ 0. -1. -3. 1. 0. -5.]] A4 = switch_rows(A3,1,2) print(A4) [[ 1. 1. 1. 0. 0. 1.] [ 0. -1. -3. 1. 0. -5.] [ 0. 3. 2. 0. 1. 1.]] A5 = scale_row(A4,-1,1) print(A5) [[ 1. 1. 1. 0. 0. 1.] [ 0. 1. 3. -1. 0. 5.] [ 0. 3. 2. 0. 1. 1.]] A6 = add_row(A5,-3,2,1) print(A6) [[ 1. 1. 1. 0. 0. 1.] [ 0. 1. 3. -1. 0. 5.] [ 0. 0. -7. 3. 1. -14.]] A7 = scale_row(A6,-1/7,2) print(A7) [[ 1. 1. 1. 0. 0. 1. ] [ 0. 1. 3. -1. 0. 5. ] [ 0. 0. 1. -0.42857143 -0.14285714 2. ]] A8 = add_row(A7,-3,1,2) print(A8) [[ 1. 1. 1. 0. 0. 1. ] [ 0. 1. 0. 0.28571429 0.42857143 -1. ] [ 0. 0. 1. -0.42857143 -0.14285714 2. ]] A9 = add_row(A8,-1,0,2) print(A9) [[ 1. 1. 0. 0.42857143 0.14285714 -1. ] [ 0. 1. 0. 0.28571429 0.42857143 -1. ] [ 0. 0. 1. -0.42857143 -0.14285714 2. ]] A10 = add_row(A9,-1,0,1) print(A10) [[ 1. 0. 0. 0.14285714 -0.28571429 0. ] [ 0. 1. 0. 0.28571429 0.42857143 -1. ] [ 0. 0. 1. -0.42857143 -0.14285714 2. ]] Let's verify that we found the inverse $M^{-1}$ correctly: Minv = A10[:,3:] print(Minv) [[ 0.14285714 -0.28571429 0. ] [ 0.28571429 0.42857143 -1. ] [-0.42857143 -0.14285714 2. ]] result = Minv @ M print(result) [[ 1.00000000e+00 4.44089210e-16 2.22044605e-16] [-6.66133815e-16 1.00000000e+00 -2.22044605e-16] [ 0.00000000e+00 0.00000000e+00 1.00000000e+00]] Success! We can see the result more clearly if we round to 15 decimal places: np.round(result,15) array([[ 1.e+00, 0.e+00, 0.e+00], [-1.e-15, 1.e+00, -0.e+00], [ 0.e+00, 0.e+00, 1.e+00]]) Solve a System Let's use our functions to perform Gaussian elimination and solve a linear system of equations $A \\mathbf{x} = \\mathbf{b}$. A = np.array([[6,15,1],[8,7,12],[2,7,8]]) print(A) [[ 6 15 1] [ 8 7 12] [ 2 7 8]] b = np.array([[2],[14],[10]]) print(b) [[ 2] [14] [10]] Form the augemented matrix $M$: M = np.hstack([A,b]) print(M) [[ 6 15 1 2] [ 8 7 12 14] [ 2 7 8 10]] Perform row operations: M1 = scale_row(M,1/6,0) print(M1) [[ 1. 2.5 0.16666667 0.33333333] [ 8. 7. 12. 14. ] [ 2. 7. 8. 10. ]] M2 = add_row(M1,-8,1,0) print(M2) [[ 1. 2.5 0.16666667 0.33333333] [ 0. -13. 10.66666667 11.33333333] [ 2. 7. 8. 10. ]] M3 = add_row(M2,-2,2,0) print(M3) [[ 1. 2.5 0.16666667 0.33333333] [ 0. -13. 10.66666667 11.33333333] [ 0. 2. 7.66666667 9.33333333]] M4 = scale_row(M3,-1/13,1) print(M4) [[ 1. 2.5 0.16666667 0.33333333] [ 0. 1. -0.82051282 -0.87179487] [ 0. 2. 7.66666667 9.33333333]] M5 = add_row(M4,-2,2,1) print(M5) [[ 1. 2.5 0.16666667 0.33333333] [ 0. 1. -0.82051282 -0.87179487] [ 0. 0. 9.30769231 11.07692308]] M6 = scale_row(M5,1/M5[2,2],2) print(M6) [[ 1. 2.5 0.16666667 0.33333333] [ 0. 1. -0.82051282 -0.87179487] [ 0. 0. 1. 1.19008264]] M7 = add_row(M6,-M6[1,2],1,2) print(M7) [[1. 2.5 0.16666667 0.33333333] [0. 1. 0. 0.1046832 ] [0. 0. 1. 1.19008264]] M8 = add_row(M7,-M7[0,2],0,2) print(M8) [[1. 2.5 0. 0.13498623] [0. 1. 0. 0.1046832 ] [0. 0. 1. 1.19008264]] M9 = add_row(M8,-M8[0,1],0,1) print(M9) [[ 1. 0. 0. -0.12672176] [ 0. 1. 0. 0.1046832 ] [ 0. 0. 1. 1.19008264]] Success! The solution of $Ax=b$ is x = M9[:,3].reshape(3,1) print(x) [[-0.12672176] [ 0.1046832 ] [ 1.19008264]] Or, we can do it the easy way... x = la.solve(A,b) print(x) [[-0.12672176] [ 0.1046832 ] [ 1.19008264]] scipy.linalg.solve We are mostly interested in linear systems $A \\mathbf{x} = \\mathbf{b}$ where there is a unique solution $\\mathbf{x}$. This is the case when $A$ is a square matrix ($m=n$) and $\\mathrm{det}(A) \\not= 0$. To solve such a system, we can use the function scipy.linalg.solve . The function returns a solution of the system of equations $A \\mathbf{x} = \\mathbf{b}$. For example: A = np.array([[1,1],[1,-1]]) print(A) [[ 1 1] [ 1 -1]] b1 = np.array([2,0]) print(b1) [2 0] And solve: x1 = la.solve(A,b1) print(x1) [1. 1.] Note that the output $\\mathbf{x}$ is returned as a 1D NumPy array when the vector $\\mathbf{b}$ (the right hand side) is entered as a 1D NumPy array. If we input $\\mathbf{b}$ as a 2D NumPy array, then the output is a 2D NumPy array. For example: A = np.array([[1,1],[1,-1]]) b2 = np.array([2,0]).reshape(2,1) x2 = la.solve(A,b2) print(x2) [[1.] [1.]] Finally, if the right hand side $\\mathbf{b}$ is a matrix, then the output is a matrix of the same size. It is the solution of $A \\mathbf{x} = \\mathbf{b}$ when $\\mathbf{b}$ is a matrix. For example: A = np.array([[1,1],[1,-1]]) b3 = np.array([[2,2],[0,1]]) x3 = la.solve(A,b3) print(x3) [[1. 1.5] [1. 0.5]] Simple Example Let's compute the solution of the system of equations \\begin{align} 2x + y &= 1 \\\\ x + y &= 1 \\end{align} Create the matrix of coefficients: A = np.array([[2,1],[1,1]]) print(A) [[2 1] [1 1]] And the vector $\\mathbf{b}$: b = np.array([1,-1]).reshape(2,1) print(b) [[ 1] [-1]] And solve: x = la.solve(A,b) print(x) [[ 2.] [-3.]] We can verify the solution by computing the inverse of $A$: Ainv = la.inv(A) print(Ainv) [[ 1. -1.] [-1. 2.]] And multiply $A^{-1} \\mathbf{b}$ to solve for $\\mathbf{x}$: x = Ainv @ b print(x) [[ 2.] [-3.]] We get the same result. Success! Inverse or Solve It's a bad idea to use the inverse $A^{-1}$ to solve $A \\mathbf{x} = \\mathbf{b}$ if $A$ is large. It's too computationally expensive. Let's create a large random matrix $A$ and vector $\\mathbf{b}$ and compute the solution $\\mathbf{x}$ in 2 ways: N = 1000 A = np.random.rand(N,N) b = np.random.rand(N,1) Check the first entries $A$: A[:3,:3] array([[0.35754719, 0.63135432, 0.6572258 ], [0.18450506, 0.14639832, 0.23528745], [0.27576474, 0.46264005, 0.26589724]]) And for $\\mathbf{b}$: b[:4,:] array([[0.82726751], [0.96946096], [0.31351176], [0.63757837]]) Now we compare the speed of scipy.linalg.solve with scipy.linalg.inv : %%timeit x = la.solve(A,b) 2.77 s \u00b1 509 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %%timeit x = la.inv(A) @ b 4.46 s \u00b1 2.04 s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) Solving with scipy.linalg.solve is about twice as fast! Exercises Under construction","title":"Solving Linear Systems"},{"location":"linear-algebra/solving-linear-systems/#solving-linear-systems","text":"import numpy as np import matplotlib.pyplot as plt import scipy.linalg as la %matplotlib inline","title":"Solving Linear Systems"},{"location":"linear-algebra/solving-linear-systems/#linear-systems","text":"A linear system of equations is a collection of linear equations \\begin{align} a_{0,0}x_0 + a_{0,1}x_2 + \\cdots + a_{0,n}x_n & = b_0 \\\\ a_{1,0}x_0 + a_{1,1}x_2 + \\cdots + a_{1,n}x_n & = b_1 \\\\ & \\vdots \\\\ a_{m,0}x_0 + a_{m,1}x_2 + \\cdots + a_{m,n}x_n & = b_m \\\\ \\end{align} In matrix notation, a linear system is $A \\mathbf{x}= \\mathbf{b}$ where $$ A = \\begin{bmatrix} a_{0,0} & a_{0,1} & \\cdots & a_{0,n} \\\\ a_{1,0} & a_{1,1} & \\cdots & a_{1,n} \\\\ \\vdots & & & \\vdots \\\\ a_{m,0} & a_{m,1} & \\cdots & a_{m,n} \\\\ \\end{bmatrix} \\ \\ , \\ \\ \\mathbf{x} = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\ \\ , \\ \\ \\mathbf{b} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} $$","title":"Linear Systems"},{"location":"linear-algebra/solving-linear-systems/#gaussian-elimination","text":"The general procedure to solve a linear system of equation is called Gaussian elimination . The idea is to perform elementary row operations to reduce the system to its row echelon form and then solve.","title":"Gaussian elimination"},{"location":"linear-algebra/solving-linear-systems/#elementary-row-operations","text":"Elementary row operations include: Add $k$ times row $j$ to row $i$. Multiply row $i$ by scalar $k$. Switch rows $i$ and $j$. Each of the elementary row operations is the result of matrix multiplication by an elementary matrix (on the left). To add $k$ times row $i$ to row $j$ in a matrix $A$, we multiply $A$ by the matrix $E$ where $E$ is equal to the identity matrix except the $i,j$ entry is $E_{i,j} = k$. For example, if $A$ is 3 by 3 and we want to add 3 times row 2 to row 0 (using 0 indexing) then $$ E_1 = \\begin{bmatrix} 1 & 0 & 3 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} $$ Let's verify the calculation: A = np.array([[1,1,2],[-1,3,1],[0,5,2]]) print(A) [[ 1 1 2] [-1 3 1] [ 0 5 2]] E1 = np.array([[1,0,3],[0,1,0],[0,0,1]]) print(E1) [[1 0 3] [0 1 0] [0 0 1]] E1 @ A array([[ 1, 16, 8], [-1, 3, 1], [ 0, 5, 2]]) To multiply $k$ times row $i$ in a matrix $A$, we multiply $A$ by the matrix $E$ where $E$ is equal to the identity matrix except the $,i,j$ entry is $E_{i,i} = k$. For example, if $A$ is 3 by 3 and we want to multiply row 1 by -2 then $$ E_2 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & -2 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} $$ Let's verify the calculation: E2 = np.array([[1,0,0],[0,-2,0],[0,0,1]]) print(E2) [[ 1 0 0] [ 0 -2 0] [ 0 0 1]] E2 @ A array([[ 1, 1, 2], [ 2, -6, -2], [ 0, 5, 2]]) Finally, to switch row $i$ and row $j$ in a matrix $A$, we multiply $A$ by the matrix $E$ where $E$ is equal to the identity matrix except $E_{i,i} = 0$, $E_{j,j} = 0$, $E_{i,j} = 1$ and $E_{j,i} = 1$. For example, if $A$ is 3 by 3 and we want to switch row 1 and row 2 then $$ E^3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix} $$ Let's verify the calculation: E3 = np.array([[1,0,0],[0,0,1],[0,1,0]]) print(E3) [[1 0 0] [0 0 1] [0 1 0]] E3 @ A array([[ 1, 1, 2], [ 0, 5, 2], [-1, 3, 1]])","title":"Elementary Row Operations"},{"location":"linear-algebra/solving-linear-systems/#implementation","text":"Let's write function to implement the elementary row operations. First of all, let's write a function called add_rows which takes input parameters $A$, $k$, $i$ and $j$ and returns the NumPy array resulting from adding $k$ times row $j$ to row $i$ in the matrix $A$. If $i=j$, then let's say that the function scales row $i$ by $k+1$ since this would be the result of $k$ times row $i$ added to row $i$. def add_row(A,k,i,j): \"Add k times row j to row i in matrix A.\" n = A.shape[0] E = np.eye(n) if i == j: E[i,i] = k + 1 else: E[i,j] = k return E @ A Let's test our function: M = np.array([[1,1],[3,2]]) print(M) [[1 1] [3 2]] add_row(M,2,0,1) array([[7., 5.], [3., 2.]]) add_row(M,3,1,1) array([[ 1., 1.], [12., 8.]]) Let's write a function called scale_row which takes 3 input parameters $A$, $k$, and $i$ and returns the matrix that results from $k$ times row $i$ in the matrix $A$. def scale_row(A,k,i): \"Multiply row i by k in matrix A.\" n = A.shape[0] E = np.eye(n) E[i,i] = k return E @ A M = np.array([[3,1],[-2,7]]) print(M) [[ 3 1] [-2 7]] scale_row(M,3,1) array([[ 3., 1.], [-6., 21.]]) A = np.array([[1,1,1],[1,-1,0]]) print(A) [[ 1 1 1] [ 1 -1 0]] scale_row(A,5,1) array([[ 1., 1., 1.], [ 5., -5., 0.]]) Let's write a function called switch_rows which takes 3 input parameters $A$, $i$ and $j$ and returns the matrix that results from switching rows $i$ and $j$ in the matrix $A$. def switch_rows(A,i,j): \"Switch rows i and j in matrix A.\" n = A.shape[0] E = np.eye(n) E[i,i] = 0 E[j,j] = 0 E[i,j] = 1 E[j,i] = 1 return E @ A A = np.array([[1,1,1],[1,-1,0]]) print(A) [[ 1 1 1] [ 1 -1 0]] switch_rows(A,0,1) array([[ 1., -1., 0.], [ 1., 1., 1.]])","title":"Implementation"},{"location":"linear-algebra/solving-linear-systems/#examples","text":"","title":"Examples"},{"location":"linear-algebra/solving-linear-systems/#find-the-inverse","text":"Let's apply our functions to the augmented matrix $[M \\ | \\ I]$ to find the inverse of the matrix $M$: M = np.array([[5,4,2],[-1,2,1],[1,1,1]]) print(M) [[ 5 4 2] [-1 2 1] [ 1 1 1]] A = np.hstack([M,np.eye(3)]) print(A) [[ 5. 4. 2. 1. 0. 0.] [-1. 2. 1. 0. 1. 0.] [ 1. 1. 1. 0. 0. 1.]] A1 = switch_rows(A,0,2) print(A1) [[ 1. 1. 1. 0. 0. 1.] [-1. 2. 1. 0. 1. 0.] [ 5. 4. 2. 1. 0. 0.]] A2 = add_row(A1,1,1,0) print(A2) [[1. 1. 1. 0. 0. 1.] [0. 3. 2. 0. 1. 1.] [5. 4. 2. 1. 0. 0.]] A3 = add_row(A2,-5,2,0) print(A3) [[ 1. 1. 1. 0. 0. 1.] [ 0. 3. 2. 0. 1. 1.] [ 0. -1. -3. 1. 0. -5.]] A4 = switch_rows(A3,1,2) print(A4) [[ 1. 1. 1. 0. 0. 1.] [ 0. -1. -3. 1. 0. -5.] [ 0. 3. 2. 0. 1. 1.]] A5 = scale_row(A4,-1,1) print(A5) [[ 1. 1. 1. 0. 0. 1.] [ 0. 1. 3. -1. 0. 5.] [ 0. 3. 2. 0. 1. 1.]] A6 = add_row(A5,-3,2,1) print(A6) [[ 1. 1. 1. 0. 0. 1.] [ 0. 1. 3. -1. 0. 5.] [ 0. 0. -7. 3. 1. -14.]] A7 = scale_row(A6,-1/7,2) print(A7) [[ 1. 1. 1. 0. 0. 1. ] [ 0. 1. 3. -1. 0. 5. ] [ 0. 0. 1. -0.42857143 -0.14285714 2. ]] A8 = add_row(A7,-3,1,2) print(A8) [[ 1. 1. 1. 0. 0. 1. ] [ 0. 1. 0. 0.28571429 0.42857143 -1. ] [ 0. 0. 1. -0.42857143 -0.14285714 2. ]] A9 = add_row(A8,-1,0,2) print(A9) [[ 1. 1. 0. 0.42857143 0.14285714 -1. ] [ 0. 1. 0. 0.28571429 0.42857143 -1. ] [ 0. 0. 1. -0.42857143 -0.14285714 2. ]] A10 = add_row(A9,-1,0,1) print(A10) [[ 1. 0. 0. 0.14285714 -0.28571429 0. ] [ 0. 1. 0. 0.28571429 0.42857143 -1. ] [ 0. 0. 1. -0.42857143 -0.14285714 2. ]] Let's verify that we found the inverse $M^{-1}$ correctly: Minv = A10[:,3:] print(Minv) [[ 0.14285714 -0.28571429 0. ] [ 0.28571429 0.42857143 -1. ] [-0.42857143 -0.14285714 2. ]] result = Minv @ M print(result) [[ 1.00000000e+00 4.44089210e-16 2.22044605e-16] [-6.66133815e-16 1.00000000e+00 -2.22044605e-16] [ 0.00000000e+00 0.00000000e+00 1.00000000e+00]] Success! We can see the result more clearly if we round to 15 decimal places: np.round(result,15) array([[ 1.e+00, 0.e+00, 0.e+00], [-1.e-15, 1.e+00, -0.e+00], [ 0.e+00, 0.e+00, 1.e+00]])","title":"Find the Inverse"},{"location":"linear-algebra/solving-linear-systems/#solve-a-system","text":"Let's use our functions to perform Gaussian elimination and solve a linear system of equations $A \\mathbf{x} = \\mathbf{b}$. A = np.array([[6,15,1],[8,7,12],[2,7,8]]) print(A) [[ 6 15 1] [ 8 7 12] [ 2 7 8]] b = np.array([[2],[14],[10]]) print(b) [[ 2] [14] [10]] Form the augemented matrix $M$: M = np.hstack([A,b]) print(M) [[ 6 15 1 2] [ 8 7 12 14] [ 2 7 8 10]] Perform row operations: M1 = scale_row(M,1/6,0) print(M1) [[ 1. 2.5 0.16666667 0.33333333] [ 8. 7. 12. 14. ] [ 2. 7. 8. 10. ]] M2 = add_row(M1,-8,1,0) print(M2) [[ 1. 2.5 0.16666667 0.33333333] [ 0. -13. 10.66666667 11.33333333] [ 2. 7. 8. 10. ]] M3 = add_row(M2,-2,2,0) print(M3) [[ 1. 2.5 0.16666667 0.33333333] [ 0. -13. 10.66666667 11.33333333] [ 0. 2. 7.66666667 9.33333333]] M4 = scale_row(M3,-1/13,1) print(M4) [[ 1. 2.5 0.16666667 0.33333333] [ 0. 1. -0.82051282 -0.87179487] [ 0. 2. 7.66666667 9.33333333]] M5 = add_row(M4,-2,2,1) print(M5) [[ 1. 2.5 0.16666667 0.33333333] [ 0. 1. -0.82051282 -0.87179487] [ 0. 0. 9.30769231 11.07692308]] M6 = scale_row(M5,1/M5[2,2],2) print(M6) [[ 1. 2.5 0.16666667 0.33333333] [ 0. 1. -0.82051282 -0.87179487] [ 0. 0. 1. 1.19008264]] M7 = add_row(M6,-M6[1,2],1,2) print(M7) [[1. 2.5 0.16666667 0.33333333] [0. 1. 0. 0.1046832 ] [0. 0. 1. 1.19008264]] M8 = add_row(M7,-M7[0,2],0,2) print(M8) [[1. 2.5 0. 0.13498623] [0. 1. 0. 0.1046832 ] [0. 0. 1. 1.19008264]] M9 = add_row(M8,-M8[0,1],0,1) print(M9) [[ 1. 0. 0. -0.12672176] [ 0. 1. 0. 0.1046832 ] [ 0. 0. 1. 1.19008264]] Success! The solution of $Ax=b$ is x = M9[:,3].reshape(3,1) print(x) [[-0.12672176] [ 0.1046832 ] [ 1.19008264]] Or, we can do it the easy way... x = la.solve(A,b) print(x) [[-0.12672176] [ 0.1046832 ] [ 1.19008264]]","title":"Solve a System"},{"location":"linear-algebra/solving-linear-systems/#scipylinalgsolve","text":"We are mostly interested in linear systems $A \\mathbf{x} = \\mathbf{b}$ where there is a unique solution $\\mathbf{x}$. This is the case when $A$ is a square matrix ($m=n$) and $\\mathrm{det}(A) \\not= 0$. To solve such a system, we can use the function scipy.linalg.solve . The function returns a solution of the system of equations $A \\mathbf{x} = \\mathbf{b}$. For example: A = np.array([[1,1],[1,-1]]) print(A) [[ 1 1] [ 1 -1]] b1 = np.array([2,0]) print(b1) [2 0] And solve: x1 = la.solve(A,b1) print(x1) [1. 1.] Note that the output $\\mathbf{x}$ is returned as a 1D NumPy array when the vector $\\mathbf{b}$ (the right hand side) is entered as a 1D NumPy array. If we input $\\mathbf{b}$ as a 2D NumPy array, then the output is a 2D NumPy array. For example: A = np.array([[1,1],[1,-1]]) b2 = np.array([2,0]).reshape(2,1) x2 = la.solve(A,b2) print(x2) [[1.] [1.]] Finally, if the right hand side $\\mathbf{b}$ is a matrix, then the output is a matrix of the same size. It is the solution of $A \\mathbf{x} = \\mathbf{b}$ when $\\mathbf{b}$ is a matrix. For example: A = np.array([[1,1],[1,-1]]) b3 = np.array([[2,2],[0,1]]) x3 = la.solve(A,b3) print(x3) [[1. 1.5] [1. 0.5]]","title":"scipy.linalg.solve"},{"location":"linear-algebra/solving-linear-systems/#simple-example","text":"Let's compute the solution of the system of equations \\begin{align} 2x + y &= 1 \\\\ x + y &= 1 \\end{align} Create the matrix of coefficients: A = np.array([[2,1],[1,1]]) print(A) [[2 1] [1 1]] And the vector $\\mathbf{b}$: b = np.array([1,-1]).reshape(2,1) print(b) [[ 1] [-1]] And solve: x = la.solve(A,b) print(x) [[ 2.] [-3.]] We can verify the solution by computing the inverse of $A$: Ainv = la.inv(A) print(Ainv) [[ 1. -1.] [-1. 2.]] And multiply $A^{-1} \\mathbf{b}$ to solve for $\\mathbf{x}$: x = Ainv @ b print(x) [[ 2.] [-3.]] We get the same result. Success!","title":"Simple Example"},{"location":"linear-algebra/solving-linear-systems/#inverse-or-solve","text":"It's a bad idea to use the inverse $A^{-1}$ to solve $A \\mathbf{x} = \\mathbf{b}$ if $A$ is large. It's too computationally expensive. Let's create a large random matrix $A$ and vector $\\mathbf{b}$ and compute the solution $\\mathbf{x}$ in 2 ways: N = 1000 A = np.random.rand(N,N) b = np.random.rand(N,1) Check the first entries $A$: A[:3,:3] array([[0.35754719, 0.63135432, 0.6572258 ], [0.18450506, 0.14639832, 0.23528745], [0.27576474, 0.46264005, 0.26589724]]) And for $\\mathbf{b}$: b[:4,:] array([[0.82726751], [0.96946096], [0.31351176], [0.63757837]]) Now we compare the speed of scipy.linalg.solve with scipy.linalg.inv : %%timeit x = la.solve(A,b) 2.77 s \u00b1 509 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %%timeit x = la.inv(A) @ b 4.46 s \u00b1 2.04 s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) Solving with scipy.linalg.solve is about twice as fast!","title":"Inverse or Solve"},{"location":"linear-algebra/solving-linear-systems/#exercises","text":"Under construction","title":"Exercises"},{"location":"problems/problems/","text":"Problems Optimization Newton's Method Write a function called newton which takes input parameters $f$, $x_0$, $h$ (with default value 0.001), tolerance (with default value 0.001) and max_iter (with default value 100). The function implements Newton's method to approximate a solution of $f(x) = 0$. In other words, compute the values of the recursive sequence starting at $x_0$ and defined by $$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} $$ Use the central difference formula with step size $h$ to approximate the derivative $f'(x_n)$. The desired result is that the method converges to an approximate root of $f(x)$ however there are several possibilities: The sequence reaches the desired tolerance $|f(x_n)| \\leq \\mathtt{tolerance}$ and newton returns the value $x_n$. The number of iterations exceeds the maximum number of iterations max_iter , the function prints the statement \"Maximum iterations exceeded\" and returns None . A zero derivative is computed $f'(x_n) = 0$, the function prints the statement \"Zero derivative\" and returns None . Numerical Differentiation Central Difference Formula Write a function called derivatives which takes input parameters $f$, $a$, $n$ and $h$ (with default value h = 0.001 ) and returns approximations of the derivatives $f'(a)$, $f''(a)$, $\\dots$, $f^{(n)}(a)$ (as a NumPy array) using the formula $$ f^{(n)}(a) \\approx \\frac{1}{2^n h^n} \\sum_{k=0}^n (-1)^k {n \\choose k} f \\left( a + ( n - 2k ) h \\right) $$ Use either scipy.misc.factorial or scipy.misc.comb to compute $n$ choose $k$: $$ {n \\choose k} = \\frac{n!}{k!(n-k)!} $$ Taylor Polynomials Write a function called taylor which takes input parameters $f$, $a$, $n$ and $L$ and plots both $f(x)$ and the Taylor polynomial $T_n(x)$ of $f(x)$ at $x=a$ of degree $n$ $$ T_n(x) = \\sum_{k=0}^n \\frac{f^{(k)}(a)}{k!}(x - a)^k $$ on the interval $[a-L,a+L]$ (in the same figure). Numerical Integration Trapezoid Rule Find $f''(x)$ for $f(x) = \\ln( \\ln x)$. Prove $|f''(x)| \\leq \\displaystyle \\frac{2}{e^2}$ for $x \\geq e$. Write a function called log_log which takes input parameters u and abs_tolerance such that $u \\geq e$ and abs_tolerance is a positive number (with default value 0.0001). The function uses the trapezoid rule to compute and return an approximation of the integral $$ \\int_e^u \\ln( \\ln x) dx $$ The number $N$ of subintervals used in the trapezoid rule must be large enough to guarantee that the approximation is within abs_tolerance of the true value. You may use the function scipy.integrate.trapz . Simpson's Rule Find $f''''(x)$ for $f(x) = e^{-x^2}$. Plot $f''''(x)$ for $x \\in [0,5]$. Determine a bound $M$ such that $|f''''(x)| \\leq M$ for $x \\geq 0$. Write a function called erf which takes input parameters u and abs_tolerance (with default value 0.0001) such that both are positive numbers. The function uses Simpson's rule to compute and return an approximation of the integral $$ \\int_0^u e^{-x^2} dx $$ The number $N$ of subintervals used in Simpson's rule must be large enough to guarantee that the approximation is within abs_tolerance of the true value. You may use the function scipy.integrate.simps . Differential Equations Lorenz Equations The Lorenz equations are the system of nonlinear differential equations \\begin{align} \\frac{dx}{dt} &= \\sigma(y - x) \\\\ \\frac{dy}{dt} &= x(\\rho - z) - y \\\\ \\frac{dz}{dt} &= xy - \\beta z \\end{align} where $\\sigma$, $\\rho$ and $\\beta$ are positive numbers. Write a function called lorenz which takes input parameters sigma , rho , beta , u0 , t0 , tf , N and plot_vars (with default value [0,1] ). The function computes and plots a numerical approximation of the corresponding solution of the Lorenz equations using the function scipy.integrate.odeint . The input parameters are: sigma , rho and beta define the parameters $\\sigma$, $\\rho$ and $\\beta$ u0 is a list of numbers of length 3 defining the initial conditions $[x(t_0),y(t_0),z(t_0)]$ t0 is the start of the interval of integration $[t_0,t_f]$ tf is the end of the interval of integration $[t_0,t_f]$ N is an integer specifying the number of evenly spaced points from $t_0$ to $t_f$ (inclusively) over which to compute the solution of the system plot_vars is a list of length 2 specifying which 2 components to plot where $x=0$, $y=1$ and $z=2$. For example, if plot_vars is $[0,1]$ then plot the solution $x$ versus $y$. If plot_vars is $[1,2]$ then plot the solution $y$ versus $z$. Note $x$ versus $y$ means $x$ is the horizontal axis and $y$ is the vertical. Default value is [0,1] which plots $x$ versus $y$. The function lorenz returns a 2D NumPy array with 4 columns: column at index 0 is the array of $N$ evenly spaced $t$ values from $t_0$ to $t_f$ (inclusively) column at index 1 is the array of $x$ values of the solution column at index 2 is the array of $y$ values of the solution column at index 3 is the array of $z$ values of the solution Damped Oscillator Write a function called damping which takes input parameters m , b , k , F , u0 , t0 , tf and N . The function uses scipy.integrate.odeint to compute a numerical approximation of the corresponding solution of the nonlinear damping equation: $$ m y'' + b |y'| y' + ky = F(t) $$ The input parameters are: m , b and k are positive numbers in the nonlinear damping equation F is a function of one variable $F(t)$ in the nonlinear damping equation u0 is a list of numbers of length 2 defining the initial conditions $[y(t_0),y'(t_0)]$ t0 is the start of the interval of integration $[t_0,t_f]$ tf is the end of the interval of integration $[t_0,t_f]$ N is an integer specifying the number of evenly spaced points from $t_0$ to $t_f$ (inclusively) over which to compute the solution The function damping plots the approximation of the solution $y(t)$ and returns a 2D Numpy array with 2 columns: column at index 0 is the array of $N$ evenly spaced $t$ values from $t_0$ to $t_f$ (inclusively) column at index 1 is the array of $y$ values of the solution","title":"Problems"},{"location":"problems/problems/#problems","text":"","title":"Problems"},{"location":"problems/problems/#optimization","text":"","title":"Optimization"},{"location":"problems/problems/#newtons-method","text":"Write a function called newton which takes input parameters $f$, $x_0$, $h$ (with default value 0.001), tolerance (with default value 0.001) and max_iter (with default value 100). The function implements Newton's method to approximate a solution of $f(x) = 0$. In other words, compute the values of the recursive sequence starting at $x_0$ and defined by $$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} $$ Use the central difference formula with step size $h$ to approximate the derivative $f'(x_n)$. The desired result is that the method converges to an approximate root of $f(x)$ however there are several possibilities: The sequence reaches the desired tolerance $|f(x_n)| \\leq \\mathtt{tolerance}$ and newton returns the value $x_n$. The number of iterations exceeds the maximum number of iterations max_iter , the function prints the statement \"Maximum iterations exceeded\" and returns None . A zero derivative is computed $f'(x_n) = 0$, the function prints the statement \"Zero derivative\" and returns None .","title":"Newton's Method"},{"location":"problems/problems/#numerical-differentiation","text":"","title":"Numerical Differentiation"},{"location":"problems/problems/#central-difference-formula","text":"Write a function called derivatives which takes input parameters $f$, $a$, $n$ and $h$ (with default value h = 0.001 ) and returns approximations of the derivatives $f'(a)$, $f''(a)$, $\\dots$, $f^{(n)}(a)$ (as a NumPy array) using the formula $$ f^{(n)}(a) \\approx \\frac{1}{2^n h^n} \\sum_{k=0}^n (-1)^k {n \\choose k} f \\left( a + ( n - 2k ) h \\right) $$ Use either scipy.misc.factorial or scipy.misc.comb to compute $n$ choose $k$: $$ {n \\choose k} = \\frac{n!}{k!(n-k)!} $$","title":"Central Difference Formula"},{"location":"problems/problems/#taylor-polynomials","text":"Write a function called taylor which takes input parameters $f$, $a$, $n$ and $L$ and plots both $f(x)$ and the Taylor polynomial $T_n(x)$ of $f(x)$ at $x=a$ of degree $n$ $$ T_n(x) = \\sum_{k=0}^n \\frac{f^{(k)}(a)}{k!}(x - a)^k $$ on the interval $[a-L,a+L]$ (in the same figure).","title":"Taylor Polynomials"},{"location":"problems/problems/#numerical-integration","text":"","title":"Numerical Integration"},{"location":"problems/problems/#trapezoid-rule","text":"Find $f''(x)$ for $f(x) = \\ln( \\ln x)$. Prove $|f''(x)| \\leq \\displaystyle \\frac{2}{e^2}$ for $x \\geq e$. Write a function called log_log which takes input parameters u and abs_tolerance such that $u \\geq e$ and abs_tolerance is a positive number (with default value 0.0001). The function uses the trapezoid rule to compute and return an approximation of the integral $$ \\int_e^u \\ln( \\ln x) dx $$ The number $N$ of subintervals used in the trapezoid rule must be large enough to guarantee that the approximation is within abs_tolerance of the true value. You may use the function scipy.integrate.trapz .","title":"Trapezoid Rule"},{"location":"problems/problems/#simpsons-rule","text":"Find $f''''(x)$ for $f(x) = e^{-x^2}$. Plot $f''''(x)$ for $x \\in [0,5]$. Determine a bound $M$ such that $|f''''(x)| \\leq M$ for $x \\geq 0$. Write a function called erf which takes input parameters u and abs_tolerance (with default value 0.0001) such that both are positive numbers. The function uses Simpson's rule to compute and return an approximation of the integral $$ \\int_0^u e^{-x^2} dx $$ The number $N$ of subintervals used in Simpson's rule must be large enough to guarantee that the approximation is within abs_tolerance of the true value. You may use the function scipy.integrate.simps .","title":"Simpson's Rule"},{"location":"problems/problems/#differential-equations","text":"","title":"Differential Equations"},{"location":"problems/problems/#lorenz-equations","text":"The Lorenz equations are the system of nonlinear differential equations \\begin{align} \\frac{dx}{dt} &= \\sigma(y - x) \\\\ \\frac{dy}{dt} &= x(\\rho - z) - y \\\\ \\frac{dz}{dt} &= xy - \\beta z \\end{align} where $\\sigma$, $\\rho$ and $\\beta$ are positive numbers. Write a function called lorenz which takes input parameters sigma , rho , beta , u0 , t0 , tf , N and plot_vars (with default value [0,1] ). The function computes and plots a numerical approximation of the corresponding solution of the Lorenz equations using the function scipy.integrate.odeint . The input parameters are: sigma , rho and beta define the parameters $\\sigma$, $\\rho$ and $\\beta$ u0 is a list of numbers of length 3 defining the initial conditions $[x(t_0),y(t_0),z(t_0)]$ t0 is the start of the interval of integration $[t_0,t_f]$ tf is the end of the interval of integration $[t_0,t_f]$ N is an integer specifying the number of evenly spaced points from $t_0$ to $t_f$ (inclusively) over which to compute the solution of the system plot_vars is a list of length 2 specifying which 2 components to plot where $x=0$, $y=1$ and $z=2$. For example, if plot_vars is $[0,1]$ then plot the solution $x$ versus $y$. If plot_vars is $[1,2]$ then plot the solution $y$ versus $z$. Note $x$ versus $y$ means $x$ is the horizontal axis and $y$ is the vertical. Default value is [0,1] which plots $x$ versus $y$. The function lorenz returns a 2D NumPy array with 4 columns: column at index 0 is the array of $N$ evenly spaced $t$ values from $t_0$ to $t_f$ (inclusively) column at index 1 is the array of $x$ values of the solution column at index 2 is the array of $y$ values of the solution column at index 3 is the array of $z$ values of the solution","title":"Lorenz Equations"},{"location":"problems/problems/#damped-oscillator","text":"Write a function called damping which takes input parameters m , b , k , F , u0 , t0 , tf and N . The function uses scipy.integrate.odeint to compute a numerical approximation of the corresponding solution of the nonlinear damping equation: $$ m y'' + b |y'| y' + ky = F(t) $$ The input parameters are: m , b and k are positive numbers in the nonlinear damping equation F is a function of one variable $F(t)$ in the nonlinear damping equation u0 is a list of numbers of length 2 defining the initial conditions $[y(t_0),y'(t_0)]$ t0 is the start of the interval of integration $[t_0,t_f]$ tf is the end of the interval of integration $[t_0,t_f]$ N is an integer specifying the number of evenly spaced points from $t_0$ to $t_f$ (inclusively) over which to compute the solution The function damping plots the approximation of the solution $y(t)$ and returns a 2D Numpy array with 2 columns: column at index 0 is the array of $N$ evenly spaced $t$ values from $t_0$ to $t_f$ (inclusively) column at index 1 is the array of $y$ values of the solution","title":"Damped Oscillator"},{"location":"python/functions/","text":"Functions A function takes input parameters, executes a series of computations with those inputs and then returns a final output value. Functions give us an efficient way to save and reuse a block of code over and over again with different input values. There are built-in functions in the standard Python library and we can define our own functions. Built-in Functions The standard Python library has a collection of built-in functions ready for us to use. We have already seen a few of these functions in previous sections such as type() , print() and sum() . The following is a list of built-in functions that we'll use most often: Function Description print(object) print object to output type(object) return the type of object abs(x) return the absolute value of x (or modulus if x is complex) int(x) return the integer constructed from float x by truncating decimal len(sequence) return the length of the sequence sum(sequence) return the sum of the entries of sequence max(sequence) return the maximum value in sequence min(sequence) return the minimum value in sequence range(a,b,step) return the range object of integers from a to b (exclusive) by step list(sequence) return a list constructed from sequence sorted(sequence) return the sorted list from the items in sequence reversed(sequence) return the reversed iterator object from the items in sequence enumerate(sequence) return the enumerate object constructed from sequence zip(a,b) return an iterator that aggregates items from sequences a and b Use the function print() to display values: pi = 3.14159 print(pi) 3.14159 Use the function type() to see the datatype of a value: type(pi) float Use the function abs() to compute the absolute value of a real number: x = -2019 abs(x) 2019 Or compute the magnitude of a complex number: z = 3 - 4j abs(z) 5.0 Use the function int() to truncate a float into an int: pi = 3.14159 int(pi) 3 The function truncates floats always towards 0: c = -1.2345 int(c) -1 Use the function len() to compute the length of a sequence: primes = [2,3,5,7,11,13,17,19,23,29,31,37,41] len(primes) 13 Use the function sum() to compute the sum of a sequence: one_to_hundred = range(1,101) sum(one_to_hundred) 5050 Use the functions max() and min() to compute the maximum and minimum values in a sequence. random = [8,27,3,7,6,14,28,19] print(max(random)) print(min(random)) 28 3 Use the function list() to convert a sequence (such as a range or a tuple) into a list: list(range(0,10,2)) [0, 2, 4, 6, 8] Use the function sorted() to sort a sequence: sorted_random = sorted(random) print(random) print(sorted_random) [8, 27, 3, 7, 6, 14, 28, 19] [3, 6, 7, 8, 14, 19, 27, 28] Use the function reversed() to reverse the order of a sequence: reversed_random = list(reversed(random)) print(random) print(reversed_random) [8, 27, 3, 7, 6, 14, 28, 19] [19, 28, 14, 6, 7, 3, 27, 8] Use the function enumerate() to enumerate a sequence: squares = [n**2 for n in range(0,6)] print(squares) enum_squares = list(enumerate(squares)) print(enum_squares) [0, 1, 4, 9, 16, 25] [(0, 0), (1, 1), (2, 4), (3, 9), (4, 16), (5, 25)] Use the function zip() to combine sequences into a list of pairs: random_1 = [-2,4,0,5] random_2 = [7,-1,9,3] random_zip = list(zip(random_1,random_2)) print(random_zip) [(-2, 7), (4, -1), (0, 9), (5, 3)] Notice in the last three examples reversed() , enumerate() and zip() we use the function list() to create a list from the output of each function. This is because these functions return iterator objects (similar to range objects) which only yield values when explicitly told to do so. Defining Functions Let's begin with a simple example. Define a function which returns the average of a sequence of numbers: def average(x): \"Compute the average of the values in the sequence x.\" sum_x = sum(x) length_x = len(x) return sum_x / length_x The main points to observe are: Start the function definition with the def keyword. Follow def with the name of the function. Follow the function name with the list of input parameters separated by commas and within parentheses. End the def statement with a colon : . Indent the body of the function by 4 spaces. Use the return keyword to specify the output of the function (but it is not always necessary). The second line is a documentation string (enclosed in quotation marks \" ... \") which describes the function. In Python, code blocks are defined using indentation . This means that lines of code indented the same amount are considered one block. In the example above, the four indented lines below the def statement form the body of the function. Notice that there is no output when we execute the cell containing the function definition. This is because we've only defined the function and it's waiting for us to use it! We need to call the function with values for the input parameters and then the function will compute and return the output value. Let's test our function: average([1,2,3,4]) 2.5 The function returns the expected value. Success! Documentation Strings The first line after the def statement in a function definition should be a documentation string (or docstring). A docstring is text (enclosed in double quotes \" ... \" or triple quotes ''' ... ''' ) which describes your function. Use triple quotes for a multiline docstring. See the Python documentation for all the conventions related to documentation strings. A helpful feature of the Jupyter notebook is the question mark operator ? . This will display the docstring of a function. Keep this in mind when writing your docstrings: other people will read your docstring to learn how to use your function. For example, use the question mark ? to view the documentation for the built-in function sum() : sum? \u001b[0;31mSignature:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m \u001b[0;31mDocstring:\u001b[0m Return the sum of a 'start' value (default: 0) plus an iterable of numbers When the iterable is empty, return the start value. This function is intended specifically for use with numeric values and may reject non-numeric types. \u001b[0;31mType:\u001b[0m builtin_function_or_method I recommend (but it's up to you) a style similar to NumPy's style guide for docstrings: def function_name(param1,param2,param3): '''First line is a one-line general summary. A longer paragraph describing the function and relevant equations or algorithms used in the function. Parameters ---------- param1 : datatype Describe the parameter. param2 : datatype Describe the parameter. param3 : datatype Describe the parameters and continue with more details if necessary on a new set of indented lines. Returns ------- datatype A description of the output of the function and also describe special behaviour. Examples -------- >>> function_name(1,2,3) 1.2345 ''' See these examples and these examples . Keyword Arguments When we define functions, we list the input parameters. These are called positional parameters (or positional arguments) because the position in the def statement determines which parameter is which. def poly(x,y): \"Compute x + y**2.\" return x + y**2 poly(1,2) 5 poly(2,1) 3 A keyword argument allows us to insert default values for some parameters and we call them by name and the order doesn't matter. For example, let's write a function called psum which takes numbers x and y and keyword argument p (with default value p=1 ) and returns the sum of powers $$ x^p + y^p $$ def psum(x,y,p=2): return x**p + y**p psum(1,2) 5 psum(1,2,p=1) 3 psum(2,3,p=3) 35 In this function, x and y are positional arguments and p is a keyword argument. An exampleof a function with many keyword arguments is the function pandas.read_csv in the pandas package: import pandas as pd pd.read_csv? So many keyword arguments! The keyword arguments I use most often are encoding , skiprows and usecols . Comments Comments in a Python program are plain text descriptions of Python code which explain code to the reader. Python will ignore lines which begin with the hash symbol # and so we use the hash symbol to write comments to explain the steps in a program. See the examples below. Lambda Functions Lambda functions are small anonymous functions created with the lambda keyword: function_name = lambda parameter: return_value The expression on the right side of the assignment operator = is an anonymous lambda function and we assign it to the variable name function_name . Lambda functions are useful in at least two ways: Define a short function in a single line code Define a function within some other Python expression For example, the function average defined above can be written in a single concise line: average = lambda x: sum(x)/len(x) The lambda keyword indicates that what follows is a function definition. The variable name x before the colon is the input parameter and the expression after the colon is the return value. Let's verify our lambda function returns the correct values: average([1,2,3,4]) 2.5 average([1,2,1,2]) 1.5 Create lambda functions with several input parameters by listing variable names separated by commas. For example, let's create a function called hypotenuse which takes input parameters x and y and returns the length of the hypotenuse of the right angle triangle with sides x and y . hypotenuse = lambda x,y: (x**2 + y**2)**0.5 Verify our functin returns the correct values: hypotenuse(3,4) 5.0 hypotenuse(5,12) 13.0 Examples Riemann Zeta Function The Riemann zeta function is the infinite sum $$ \\zeta(s) = \\sum_{n = 1}^{\\infty} \\frac{1}{n^s} $$ Write a function called zeta which takes 2 input parameters s and N and returns the partial sum: $$ \\sum_{n=1}^N \\frac{1}{n^s} $$ def zeta(s,N): \"Compute the Nth partial sum of the zeta function at s.\" terms = [1/n**s for n in range(1,N+1)] partial_sum = sum(terms) return partial_sum Let's test our function on input values for which we know the result: zeta(1,1) 1.0 zeta(2,2) 1.25 Now let's use our function to approximate special values of the Riemann zeta function : $$ \\zeta(2) = \\frac{\\pi^2}{6} \\hspace{10mm} \\text{and} \\hspace{10mm} \\zeta(4) = \\frac{\\pi^4}{90} $$ Compute the partial sum for $s=2$ and $N=100000$: zeta(2,100000) 1.6449240668982423 Compare to an approximation of the special value $\\pi^2/6$: 3.14159265**2/6 1.6449340630890041 Compute the partial sum for $s=4$ and $N=100000$: zeta(4,100000) 1.082323233710861 Compare to an approximation of the special value $\\pi^4/90$: 3.14159265**4/90 1.0823232287641997 Harmonic Mean Write a function called harmonic_mean which takes an input parameter s , a list of numbers $x_1, \\dots, x_n$ of length $n$, and returns the harmonic mean of the sequence: $$ \\frac{n}{\\frac{1}{x_1} + \\frac{1}{x_2} + \\cdots + \\frac{1}{x_n}} $$ def harmonic_mean(s): \"Compute the harmonic mean of the numbers in the sequence s.\" n = len(s) terms = [1/s[i] for i in range(0,n)] result = n/sum(terms) return result Let's test our function: harmonic_mean([1,1,1,1]) 1.0 harmonic_mean([1,2,3]) 1.6363636363636365 Riemann Sums Write a function called mn_integral which takes input parameters m , n , a , b and N and returns the (right) Riemann sum : $$ \\int_a^b f(x) \\, dx \\approx \\sum_{k=1}^N f(x_k) \\Delta x \\ \\ , \\ \\ f(x) = \\frac{x^m + 1}{x^n + 1} $$ and $\\Delta x = (b-a)/N$ and $x_k = a + k \\Delta x$. def mn_integral(m,n,a,b,N): '''Compute the right Riemann sum for the function f(x) = (x^m + 1)/(x^n + 1) on interval [a,b] with a partition of N subintervals of equal size. ''' delta_x = (b - a)/N x = [a + k*delta_x for k in range(0,N+1)] terms = [(x[k]**m + 1)/(x[k]**n + 1)*delta_x for k in range(1,N+1)] riemann_sum = sum(terms) return riemann_sum Let's test our function on input for which we know the result. Let $m=0$, $n=1$, $a=0$, $b=1$ and $N=2$. Then $x_0 = 0$, $x_1 = 1/2$, $x_2 = 1$ and $\\Delta x = 1/2$, and we compute: $$ \\begin{aligned} \\sum_{k=1}^N f(x_k) \\Delta x &= \\sum_{k=1}^2 \\frac{x_k^0 + 1}{x_k^1 + 1} \\Delta x \\\\ &= \\frac{2}{(1/2) + 1} \\cdot \\frac{1}{2} + \\frac{2}{1 + 1} \\cdot \\frac{1}{2} \\\\ &= \\frac{7}{6} \\end{aligned} $$ mn_integral(0,1,0,1,2) 1.1666666666666665 7/6 1.1666666666666667 Let's test our function on another example. Let $m=1$, $n=2$, $a=0$, and $b=1$. We can solve this integral exactly: $$ \\begin{aligned} \\int_0^1 \\frac{x + 1}{x^2 + 1} dx &= \\int_0^1 \\frac{x}{x^2 + 1} dx + \\int_0^1 \\frac{1}{x^2 + 1} dx \\\\ &= \\left. \\left( \\frac{1}{2} \\ln(x^2 + 1) + \\arctan x \\right) \\right|_0^1 \\\\ &= \\frac{1}{2} \\ln(2) + \\frac{\\pi}{4} \\end{aligned} $$ Approximate this integral with a Riemann sum for $N=100000$: mn_integral(1,2,0,1,100000) 1.1319717536649336 Since $\\pi \\approx 3.14159265$ and $\\ln(2) \\approx 0.69314718$, we compare to the approximation: 0.5*0.69314718 + 3.14159265/4 1.1319717525000002 Our function computes the expected values! Exercises Exercise 1. Write a function called p_norm which takes input parameters sequence and p where sequence is a list of numbers $x_1, \\dots, x_n$ and p is a nonzero number. The function returns the $p$-norm : $$ \\left( \\frac{1}{n} \\sum_{i=1}^n | x_i |^p \\right)^{1/p} $$ Plug in large positive values of $p$ and various lists of numbers to verify $$ \\lim_{p \\to \\infty} \\left( \\frac{1}{n} \\sum_{i=1}^n | x_i |^p \\right)^{1/p} = \\max { |x_1| , \\dots, |x_n| } $$ Plug in large negative values of $p$ and various lists of numbers to verify $$ \\lim_{p \\to -\\infty} \\left( \\frac{1}{n} \\sum_{i=1}^n |x_i|^p \\right)^{1/p} = \\min { |x_1| , \\dots, |x_n| } $$ Exercise 2. Write a function called arctan_taylor which takes input parameters x and N and return the Taylor polynomial of degree $N$ of the function $\\arctan x$ evaluated at x : $$ \\sum_{k=0}^N (-1)^k \\frac{x^{2k + 1}}{2k + 1} $$ Exercise 3. Write a function called zips which takes input parameters a and b , where a and b are lists of the equal length, and returns the list of tuples which aggregates the sequence. (In other words, write your own version of the built-in function zip() ... without using zip() of course.) For example zips([-1,3,4,0],[5,7,1,-9]) returns the list [(-1, 5), (3, 7), (4, 1), (0, -9)] . Exercise 4. Write a function called sqrt_integral which takes input parameters u , p and N and returns the Riemann sum (using the midpoints $x_k^*$ of a partition of size $N$): $$ \\int_0^u \\frac{1}{\\sqrt{1 + x^p}} dx \\approx \\sum_{k=1}^N \\frac{1}{\\sqrt{1 + (x_k^*)^p}} \\Delta x $$ where $\\Delta x = u/N$ and $x_k^* = (x_k + x_{k-1})/2$ for endpoints $x_k = k \\Delta x$.","title":"Functions"},{"location":"python/functions/#functions","text":"A function takes input parameters, executes a series of computations with those inputs and then returns a final output value. Functions give us an efficient way to save and reuse a block of code over and over again with different input values. There are built-in functions in the standard Python library and we can define our own functions.","title":"Functions"},{"location":"python/functions/#built-in-functions","text":"The standard Python library has a collection of built-in functions ready for us to use. We have already seen a few of these functions in previous sections such as type() , print() and sum() . The following is a list of built-in functions that we'll use most often: Function Description print(object) print object to output type(object) return the type of object abs(x) return the absolute value of x (or modulus if x is complex) int(x) return the integer constructed from float x by truncating decimal len(sequence) return the length of the sequence sum(sequence) return the sum of the entries of sequence max(sequence) return the maximum value in sequence min(sequence) return the minimum value in sequence range(a,b,step) return the range object of integers from a to b (exclusive) by step list(sequence) return a list constructed from sequence sorted(sequence) return the sorted list from the items in sequence reversed(sequence) return the reversed iterator object from the items in sequence enumerate(sequence) return the enumerate object constructed from sequence zip(a,b) return an iterator that aggregates items from sequences a and b Use the function print() to display values: pi = 3.14159 print(pi) 3.14159 Use the function type() to see the datatype of a value: type(pi) float Use the function abs() to compute the absolute value of a real number: x = -2019 abs(x) 2019 Or compute the magnitude of a complex number: z = 3 - 4j abs(z) 5.0 Use the function int() to truncate a float into an int: pi = 3.14159 int(pi) 3 The function truncates floats always towards 0: c = -1.2345 int(c) -1 Use the function len() to compute the length of a sequence: primes = [2,3,5,7,11,13,17,19,23,29,31,37,41] len(primes) 13 Use the function sum() to compute the sum of a sequence: one_to_hundred = range(1,101) sum(one_to_hundred) 5050 Use the functions max() and min() to compute the maximum and minimum values in a sequence. random = [8,27,3,7,6,14,28,19] print(max(random)) print(min(random)) 28 3 Use the function list() to convert a sequence (such as a range or a tuple) into a list: list(range(0,10,2)) [0, 2, 4, 6, 8] Use the function sorted() to sort a sequence: sorted_random = sorted(random) print(random) print(sorted_random) [8, 27, 3, 7, 6, 14, 28, 19] [3, 6, 7, 8, 14, 19, 27, 28] Use the function reversed() to reverse the order of a sequence: reversed_random = list(reversed(random)) print(random) print(reversed_random) [8, 27, 3, 7, 6, 14, 28, 19] [19, 28, 14, 6, 7, 3, 27, 8] Use the function enumerate() to enumerate a sequence: squares = [n**2 for n in range(0,6)] print(squares) enum_squares = list(enumerate(squares)) print(enum_squares) [0, 1, 4, 9, 16, 25] [(0, 0), (1, 1), (2, 4), (3, 9), (4, 16), (5, 25)] Use the function zip() to combine sequences into a list of pairs: random_1 = [-2,4,0,5] random_2 = [7,-1,9,3] random_zip = list(zip(random_1,random_2)) print(random_zip) [(-2, 7), (4, -1), (0, 9), (5, 3)] Notice in the last three examples reversed() , enumerate() and zip() we use the function list() to create a list from the output of each function. This is because these functions return iterator objects (similar to range objects) which only yield values when explicitly told to do so.","title":"Built-in Functions"},{"location":"python/functions/#defining-functions","text":"Let's begin with a simple example. Define a function which returns the average of a sequence of numbers: def average(x): \"Compute the average of the values in the sequence x.\" sum_x = sum(x) length_x = len(x) return sum_x / length_x The main points to observe are: Start the function definition with the def keyword. Follow def with the name of the function. Follow the function name with the list of input parameters separated by commas and within parentheses. End the def statement with a colon : . Indent the body of the function by 4 spaces. Use the return keyword to specify the output of the function (but it is not always necessary). The second line is a documentation string (enclosed in quotation marks \" ... \") which describes the function. In Python, code blocks are defined using indentation . This means that lines of code indented the same amount are considered one block. In the example above, the four indented lines below the def statement form the body of the function. Notice that there is no output when we execute the cell containing the function definition. This is because we've only defined the function and it's waiting for us to use it! We need to call the function with values for the input parameters and then the function will compute and return the output value. Let's test our function: average([1,2,3,4]) 2.5 The function returns the expected value. Success!","title":"Defining Functions"},{"location":"python/functions/#documentation-strings","text":"The first line after the def statement in a function definition should be a documentation string (or docstring). A docstring is text (enclosed in double quotes \" ... \" or triple quotes ''' ... ''' ) which describes your function. Use triple quotes for a multiline docstring. See the Python documentation for all the conventions related to documentation strings. A helpful feature of the Jupyter notebook is the question mark operator ? . This will display the docstring of a function. Keep this in mind when writing your docstrings: other people will read your docstring to learn how to use your function. For example, use the question mark ? to view the documentation for the built-in function sum() : sum? \u001b[0;31mSignature:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m \u001b[0;31mDocstring:\u001b[0m Return the sum of a 'start' value (default: 0) plus an iterable of numbers When the iterable is empty, return the start value. This function is intended specifically for use with numeric values and may reject non-numeric types. \u001b[0;31mType:\u001b[0m builtin_function_or_method I recommend (but it's up to you) a style similar to NumPy's style guide for docstrings: def function_name(param1,param2,param3): '''First line is a one-line general summary. A longer paragraph describing the function and relevant equations or algorithms used in the function. Parameters ---------- param1 : datatype Describe the parameter. param2 : datatype Describe the parameter. param3 : datatype Describe the parameters and continue with more details if necessary on a new set of indented lines. Returns ------- datatype A description of the output of the function and also describe special behaviour. Examples -------- >>> function_name(1,2,3) 1.2345 ''' See these examples and these examples .","title":"Documentation Strings"},{"location":"python/functions/#keyword-arguments","text":"When we define functions, we list the input parameters. These are called positional parameters (or positional arguments) because the position in the def statement determines which parameter is which. def poly(x,y): \"Compute x + y**2.\" return x + y**2 poly(1,2) 5 poly(2,1) 3 A keyword argument allows us to insert default values for some parameters and we call them by name and the order doesn't matter. For example, let's write a function called psum which takes numbers x and y and keyword argument p (with default value p=1 ) and returns the sum of powers $$ x^p + y^p $$ def psum(x,y,p=2): return x**p + y**p psum(1,2) 5 psum(1,2,p=1) 3 psum(2,3,p=3) 35 In this function, x and y are positional arguments and p is a keyword argument. An exampleof a function with many keyword arguments is the function pandas.read_csv in the pandas package: import pandas as pd pd.read_csv? So many keyword arguments! The keyword arguments I use most often are encoding , skiprows and usecols .","title":"Keyword Arguments"},{"location":"python/functions/#comments","text":"Comments in a Python program are plain text descriptions of Python code which explain code to the reader. Python will ignore lines which begin with the hash symbol # and so we use the hash symbol to write comments to explain the steps in a program. See the examples below.","title":"Comments"},{"location":"python/functions/#lambda-functions","text":"Lambda functions are small anonymous functions created with the lambda keyword: function_name = lambda parameter: return_value The expression on the right side of the assignment operator = is an anonymous lambda function and we assign it to the variable name function_name . Lambda functions are useful in at least two ways: Define a short function in a single line code Define a function within some other Python expression For example, the function average defined above can be written in a single concise line: average = lambda x: sum(x)/len(x) The lambda keyword indicates that what follows is a function definition. The variable name x before the colon is the input parameter and the expression after the colon is the return value. Let's verify our lambda function returns the correct values: average([1,2,3,4]) 2.5 average([1,2,1,2]) 1.5 Create lambda functions with several input parameters by listing variable names separated by commas. For example, let's create a function called hypotenuse which takes input parameters x and y and returns the length of the hypotenuse of the right angle triangle with sides x and y . hypotenuse = lambda x,y: (x**2 + y**2)**0.5 Verify our functin returns the correct values: hypotenuse(3,4) 5.0 hypotenuse(5,12) 13.0","title":"Lambda Functions"},{"location":"python/functions/#examples","text":"","title":"Examples"},{"location":"python/functions/#riemann-zeta-function","text":"The Riemann zeta function is the infinite sum $$ \\zeta(s) = \\sum_{n = 1}^{\\infty} \\frac{1}{n^s} $$ Write a function called zeta which takes 2 input parameters s and N and returns the partial sum: $$ \\sum_{n=1}^N \\frac{1}{n^s} $$ def zeta(s,N): \"Compute the Nth partial sum of the zeta function at s.\" terms = [1/n**s for n in range(1,N+1)] partial_sum = sum(terms) return partial_sum Let's test our function on input values for which we know the result: zeta(1,1) 1.0 zeta(2,2) 1.25 Now let's use our function to approximate special values of the Riemann zeta function : $$ \\zeta(2) = \\frac{\\pi^2}{6} \\hspace{10mm} \\text{and} \\hspace{10mm} \\zeta(4) = \\frac{\\pi^4}{90} $$ Compute the partial sum for $s=2$ and $N=100000$: zeta(2,100000) 1.6449240668982423 Compare to an approximation of the special value $\\pi^2/6$: 3.14159265**2/6 1.6449340630890041 Compute the partial sum for $s=4$ and $N=100000$: zeta(4,100000) 1.082323233710861 Compare to an approximation of the special value $\\pi^4/90$: 3.14159265**4/90 1.0823232287641997","title":"Riemann Zeta Function"},{"location":"python/functions/#harmonic-mean","text":"Write a function called harmonic_mean which takes an input parameter s , a list of numbers $x_1, \\dots, x_n$ of length $n$, and returns the harmonic mean of the sequence: $$ \\frac{n}{\\frac{1}{x_1} + \\frac{1}{x_2} + \\cdots + \\frac{1}{x_n}} $$ def harmonic_mean(s): \"Compute the harmonic mean of the numbers in the sequence s.\" n = len(s) terms = [1/s[i] for i in range(0,n)] result = n/sum(terms) return result Let's test our function: harmonic_mean([1,1,1,1]) 1.0 harmonic_mean([1,2,3]) 1.6363636363636365","title":"Harmonic Mean"},{"location":"python/functions/#riemann-sums","text":"Write a function called mn_integral which takes input parameters m , n , a , b and N and returns the (right) Riemann sum : $$ \\int_a^b f(x) \\, dx \\approx \\sum_{k=1}^N f(x_k) \\Delta x \\ \\ , \\ \\ f(x) = \\frac{x^m + 1}{x^n + 1} $$ and $\\Delta x = (b-a)/N$ and $x_k = a + k \\Delta x$. def mn_integral(m,n,a,b,N): '''Compute the right Riemann sum for the function f(x) = (x^m + 1)/(x^n + 1) on interval [a,b] with a partition of N subintervals of equal size. ''' delta_x = (b - a)/N x = [a + k*delta_x for k in range(0,N+1)] terms = [(x[k]**m + 1)/(x[k]**n + 1)*delta_x for k in range(1,N+1)] riemann_sum = sum(terms) return riemann_sum Let's test our function on input for which we know the result. Let $m=0$, $n=1$, $a=0$, $b=1$ and $N=2$. Then $x_0 = 0$, $x_1 = 1/2$, $x_2 = 1$ and $\\Delta x = 1/2$, and we compute: $$ \\begin{aligned} \\sum_{k=1}^N f(x_k) \\Delta x &= \\sum_{k=1}^2 \\frac{x_k^0 + 1}{x_k^1 + 1} \\Delta x \\\\ &= \\frac{2}{(1/2) + 1} \\cdot \\frac{1}{2} + \\frac{2}{1 + 1} \\cdot \\frac{1}{2} \\\\ &= \\frac{7}{6} \\end{aligned} $$ mn_integral(0,1,0,1,2) 1.1666666666666665 7/6 1.1666666666666667 Let's test our function on another example. Let $m=1$, $n=2$, $a=0$, and $b=1$. We can solve this integral exactly: $$ \\begin{aligned} \\int_0^1 \\frac{x + 1}{x^2 + 1} dx &= \\int_0^1 \\frac{x}{x^2 + 1} dx + \\int_0^1 \\frac{1}{x^2 + 1} dx \\\\ &= \\left. \\left( \\frac{1}{2} \\ln(x^2 + 1) + \\arctan x \\right) \\right|_0^1 \\\\ &= \\frac{1}{2} \\ln(2) + \\frac{\\pi}{4} \\end{aligned} $$ Approximate this integral with a Riemann sum for $N=100000$: mn_integral(1,2,0,1,100000) 1.1319717536649336 Since $\\pi \\approx 3.14159265$ and $\\ln(2) \\approx 0.69314718$, we compare to the approximation: 0.5*0.69314718 + 3.14159265/4 1.1319717525000002 Our function computes the expected values!","title":"Riemann Sums"},{"location":"python/functions/#exercises","text":"Exercise 1. Write a function called p_norm which takes input parameters sequence and p where sequence is a list of numbers $x_1, \\dots, x_n$ and p is a nonzero number. The function returns the $p$-norm : $$ \\left( \\frac{1}{n} \\sum_{i=1}^n | x_i |^p \\right)^{1/p} $$ Plug in large positive values of $p$ and various lists of numbers to verify $$ \\lim_{p \\to \\infty} \\left( \\frac{1}{n} \\sum_{i=1}^n | x_i |^p \\right)^{1/p} = \\max { |x_1| , \\dots, |x_n| } $$ Plug in large negative values of $p$ and various lists of numbers to verify $$ \\lim_{p \\to -\\infty} \\left( \\frac{1}{n} \\sum_{i=1}^n |x_i|^p \\right)^{1/p} = \\min { |x_1| , \\dots, |x_n| } $$ Exercise 2. Write a function called arctan_taylor which takes input parameters x and N and return the Taylor polynomial of degree $N$ of the function $\\arctan x$ evaluated at x : $$ \\sum_{k=0}^N (-1)^k \\frac{x^{2k + 1}}{2k + 1} $$ Exercise 3. Write a function called zips which takes input parameters a and b , where a and b are lists of the equal length, and returns the list of tuples which aggregates the sequence. (In other words, write your own version of the built-in function zip() ... without using zip() of course.) For example zips([-1,3,4,0],[5,7,1,-9]) returns the list [(-1, 5), (3, 7), (4, 1), (0, -9)] . Exercise 4. Write a function called sqrt_integral which takes input parameters u , p and N and returns the Riemann sum (using the midpoints $x_k^*$ of a partition of size $N$): $$ \\int_0^u \\frac{1}{\\sqrt{1 + x^p}} dx \\approx \\sum_{k=1}^N \\frac{1}{\\sqrt{1 + (x_k^*)^p}} \\Delta x $$ where $\\Delta x = u/N$ and $x_k^* = (x_k + x_{k-1})/2$ for endpoints $x_k = k \\Delta x$.","title":"Exercises"},{"location":"python/logic/","text":"Logic Boolean Values The boolean type has only two values: True and False . Let's assign a boolean value to a variable and verify the type using the built-in function type() : python_is_fun = True print(python_is_fun) True type(python_is_fun) bool Let's assign the value False to a variable and again verify the type: math_is_scary = False print(math_is_scary) False type(math_is_scary) bool Comparison Operators Comparison operators produce Boolean values as output. For example, if we have variables x and y with numeric values, we can evaluate the expression x < y and the result is a boolean value either True or False . Comparison Operator Description < strictly less than <= less than or equal > strictly greater than >= greater than or equal == equal != not equal For example: 1 == 2 False 1 < 2 True 2 == 2 True 3 != 3.14159 True 20.00000001 >= 20 True Boolean Operators We combine logical expressions using boolean operators and , or and not . Boolean Operator Description A and B returns True if both A and B are True A or B returns True if either A or B is True not A returns True if A is False For example: (1 < 2) and (3 != 5) True (1 < 2) and (3 < 1) False (1 < 2) or (3 < 1) True not (1000 <= 999) True if statements An if statement consists of one or more blocks of code such that only one block is executed depending on logical expressions. For example, determine if roots of polynomial equation $ax^2 + bx + c = 0$ are are real, repeated or complex using the quadratic formula $$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ a = 10 b = -234 c = 1984 discriminant = b**2 - 4*a*c if discriminant > 0: print(\"Discriminant =\", discriminant) print(\"Roots are real and distinct.\") elif discriminant < 0: print(\"Discriminant =\", discriminant) print(\"Roots are complex.\") else: print(\"Discriminant =\", discriminant) print(\"Roots are real and repeated.\") Discriminant = -24604 Roots are complex. The main points to observe are: Start with the if keyword. Write a logical expression (returning True or False ). End line with a colon : . Indent block 4 spaces after if statement. Include elif and else statements if needed. Only one of the blocks if , elif and else is executed. The block following an else statement will execute only if all other logical expressions before it are False . Examples Invertible Matrix Represent a 2 by 2 square matrix as a list of lists. For example, represent the matrix $$ \\begin{bmatrix} 2 & -1 \\\\ 5 & 7 \\end{bmatrix} $$ as the list of lists [[2,-1],[5,7]] . Write a function called invertible which takes an input parameter M , a list of lists representing a 2 by 2 matrix, and returns True if the matrix M is invertible and False if not. def invertible(M): \"Determine if the matrix M = [[a,b],[c,d]] is invertible.\" determinant = M[0][0] * M[1][1] - M[0][1] * M[1][0] if determinant != 0: return True else: return False Let's test our function: invertible([[1,2],[3,4]]) True invertible([[1,1],[3,3]]) False Concavity of a Polynomial Write a function called concave_up which takes input parameters p and a where p is a list representing a polynomial $p(x)$ and a is a number, and returns True if the function $p(x)$ is concave up at $x=a$ (ie. its second derivative is positive at $x=a$, $p''(a) > 0$). We'll use the second derivative test for polynomials. In particular, if we have a polynomial of degree $n$ $$ p(x) = c_0 + c_1 x + c_2 x^2 + \\cdots + c_n x^n $$ then the second derivative of $p(x)$ at $x=a$ is the sum $$ p''(a) = 2(1) c_2 + 3(2)c_3 a + 4(3)c_4 a^2 + \\cdots + n(n-1)c_n a^{n-2} $$ def concave_up(p,a): \"Determine if the polynomial p(x) is concave up at x=a.\" degree = len(p) - 1 if degree < 2: return False else: # Compute the second derivative p''(a) DDp_a = sum([k*(k-1)*p[k]*a**(k-2) for k in range(2,degree + 1)]) if DDp_a > 0: return True else: return False Let's test our function on $p(x) = 1 + x - x^3$ at $x=2$. Since $p''(x) = -6x$ and $p''(2) = -12 < 0$, the polynomial is concave down at $x=2$. p = [1,1,0,-1] a = 2 concavity = concave_up(p,a) print(concavity) False Exercises Exercise 1. The discriminant of a cubic polynomial $p(x) = ax^3 + bx^2 + cx + d$ is $$ \\Delta = b^2c^2 - 4ac^3 - 4b^3d - 27a^2d^2 + 18abcd $$ The discriminant gives us information about the roots of the polynomial $p(x)$: if $\\Delta > 0$, then $p(x)$ has 3 distinct real roots if $\\Delta < 0$, then $p(x)$ has 2 distinct complex roots and 1 real root if $\\Delta = 0$, then $p(x)$ has at least 2 (real or complex) roots which are the same Represent a cubic polynomial $p(x) = ax^3 + bx^2 + cx + d$ as a list [d,c,b,a] of numbers. (Note the order of the coefficients is increasing degree.) For example, the polynomial $p(x) = x^3 - x + 1$ is [1,-1,0,1] . Write a function called cubic_roots which takes an input parameter p , a list of length 4 representing a cubic polynomial, and returns True if $p(x)$ has 3 real distinct roots and False otherwise. Exercise 2. Represent a 2 by 2 square matrix as a list of lists. For example, represent the matrix $$ \\begin{bmatrix} 2 & -1 \\\\ 5 & 7 \\end{bmatrix} $$ as the list of lists [[2,-1],[5,7]] . Write a function called inverse_a which takes an input parameter a and returns a list of lists representing the inverse of the matrix $$ \\begin{bmatrix} 1 & a \\\\ a & -1 \\end{bmatrix} $$ Exercise 3. Write a function called real_eigenvalues which takes an input parameter M , a list of lists representing a 2 by 2 matrix (as in the previous exercise), and returns True if the eigenvalues of the matrix M are real numebrs and False if not.","title":"Logic"},{"location":"python/logic/#logic","text":"","title":"Logic"},{"location":"python/logic/#boolean-values","text":"The boolean type has only two values: True and False . Let's assign a boolean value to a variable and verify the type using the built-in function type() : python_is_fun = True print(python_is_fun) True type(python_is_fun) bool Let's assign the value False to a variable and again verify the type: math_is_scary = False print(math_is_scary) False type(math_is_scary) bool","title":"Boolean Values"},{"location":"python/logic/#comparison-operators","text":"Comparison operators produce Boolean values as output. For example, if we have variables x and y with numeric values, we can evaluate the expression x < y and the result is a boolean value either True or False . Comparison Operator Description < strictly less than <= less than or equal > strictly greater than >= greater than or equal == equal != not equal For example: 1 == 2 False 1 < 2 True 2 == 2 True 3 != 3.14159 True 20.00000001 >= 20 True","title":"Comparison Operators"},{"location":"python/logic/#boolean-operators","text":"We combine logical expressions using boolean operators and , or and not . Boolean Operator Description A and B returns True if both A and B are True A or B returns True if either A or B is True not A returns True if A is False For example: (1 < 2) and (3 != 5) True (1 < 2) and (3 < 1) False (1 < 2) or (3 < 1) True not (1000 <= 999) True","title":"Boolean Operators"},{"location":"python/logic/#if-statements","text":"An if statement consists of one or more blocks of code such that only one block is executed depending on logical expressions. For example, determine if roots of polynomial equation $ax^2 + bx + c = 0$ are are real, repeated or complex using the quadratic formula $$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ a = 10 b = -234 c = 1984 discriminant = b**2 - 4*a*c if discriminant > 0: print(\"Discriminant =\", discriminant) print(\"Roots are real and distinct.\") elif discriminant < 0: print(\"Discriminant =\", discriminant) print(\"Roots are complex.\") else: print(\"Discriminant =\", discriminant) print(\"Roots are real and repeated.\") Discriminant = -24604 Roots are complex. The main points to observe are: Start with the if keyword. Write a logical expression (returning True or False ). End line with a colon : . Indent block 4 spaces after if statement. Include elif and else statements if needed. Only one of the blocks if , elif and else is executed. The block following an else statement will execute only if all other logical expressions before it are False .","title":"if statements"},{"location":"python/logic/#examples","text":"","title":"Examples"},{"location":"python/logic/#invertible-matrix","text":"Represent a 2 by 2 square matrix as a list of lists. For example, represent the matrix $$ \\begin{bmatrix} 2 & -1 \\\\ 5 & 7 \\end{bmatrix} $$ as the list of lists [[2,-1],[5,7]] . Write a function called invertible which takes an input parameter M , a list of lists representing a 2 by 2 matrix, and returns True if the matrix M is invertible and False if not. def invertible(M): \"Determine if the matrix M = [[a,b],[c,d]] is invertible.\" determinant = M[0][0] * M[1][1] - M[0][1] * M[1][0] if determinant != 0: return True else: return False Let's test our function: invertible([[1,2],[3,4]]) True invertible([[1,1],[3,3]]) False","title":"Invertible Matrix"},{"location":"python/logic/#concavity-of-a-polynomial","text":"Write a function called concave_up which takes input parameters p and a where p is a list representing a polynomial $p(x)$ and a is a number, and returns True if the function $p(x)$ is concave up at $x=a$ (ie. its second derivative is positive at $x=a$, $p''(a) > 0$). We'll use the second derivative test for polynomials. In particular, if we have a polynomial of degree $n$ $$ p(x) = c_0 + c_1 x + c_2 x^2 + \\cdots + c_n x^n $$ then the second derivative of $p(x)$ at $x=a$ is the sum $$ p''(a) = 2(1) c_2 + 3(2)c_3 a + 4(3)c_4 a^2 + \\cdots + n(n-1)c_n a^{n-2} $$ def concave_up(p,a): \"Determine if the polynomial p(x) is concave up at x=a.\" degree = len(p) - 1 if degree < 2: return False else: # Compute the second derivative p''(a) DDp_a = sum([k*(k-1)*p[k]*a**(k-2) for k in range(2,degree + 1)]) if DDp_a > 0: return True else: return False Let's test our function on $p(x) = 1 + x - x^3$ at $x=2$. Since $p''(x) = -6x$ and $p''(2) = -12 < 0$, the polynomial is concave down at $x=2$. p = [1,1,0,-1] a = 2 concavity = concave_up(p,a) print(concavity) False","title":"Concavity of a Polynomial"},{"location":"python/logic/#exercises","text":"Exercise 1. The discriminant of a cubic polynomial $p(x) = ax^3 + bx^2 + cx + d$ is $$ \\Delta = b^2c^2 - 4ac^3 - 4b^3d - 27a^2d^2 + 18abcd $$ The discriminant gives us information about the roots of the polynomial $p(x)$: if $\\Delta > 0$, then $p(x)$ has 3 distinct real roots if $\\Delta < 0$, then $p(x)$ has 2 distinct complex roots and 1 real root if $\\Delta = 0$, then $p(x)$ has at least 2 (real or complex) roots which are the same Represent a cubic polynomial $p(x) = ax^3 + bx^2 + cx + d$ as a list [d,c,b,a] of numbers. (Note the order of the coefficients is increasing degree.) For example, the polynomial $p(x) = x^3 - x + 1$ is [1,-1,0,1] . Write a function called cubic_roots which takes an input parameter p , a list of length 4 representing a cubic polynomial, and returns True if $p(x)$ has 3 real distinct roots and False otherwise. Exercise 2. Represent a 2 by 2 square matrix as a list of lists. For example, represent the matrix $$ \\begin{bmatrix} 2 & -1 \\\\ 5 & 7 \\end{bmatrix} $$ as the list of lists [[2,-1],[5,7]] . Write a function called inverse_a which takes an input parameter a and returns a list of lists representing the inverse of the matrix $$ \\begin{bmatrix} 1 & a \\\\ a & -1 \\end{bmatrix} $$ Exercise 3. Write a function called real_eigenvalues which takes an input parameter M , a list of lists representing a 2 by 2 matrix (as in the previous exercise), and returns True if the eigenvalues of the matrix M are real numebrs and False if not.","title":"Exercises"},{"location":"python/loops/","text":"Loops for Loops A for loop allows us to execute a block of code multiple times with some parameters updated each time through the loop. A for loop begins with the for statement: iterable = [1,2,3] for item in iterable: # code block indented 4 spaces print(item) 1 2 3 The main points to observe are: for and in keywords iterable is a sequence object such as a list, tuple or range item is a variable which takes each value in iterable end for statement with a colon : code block indented 4 spaces which executes once for each value in iterable For example, let's print $n^2$ for $n$ from 0 to 5: for n in [0,1,2,3,4,5]: square = n**2 print(n,'squared is',square) print('The for loop is complete!') 0 squared is 0 1 squared is 1 2 squared is 4 3 squared is 9 4 squared is 16 5 squared is 25 The for loop is complete! Copy and paste this code and any of the examples below into the Python visualizer to see each step in a for loop! while Loops What if we want to execute a block of code multiple times but we don't know exactly how many times? We can't write a for loop because this requires us to set the length of the loop in advance. This is a situation when a while loop is useful. The following example illustrates a while loop : n = 5 while n > 0: print(n) n = n - 1 5 4 3 2 1 The main points to observe are: while keyword a logical expression followed by a colon : loop executes its code block if the logical expression evaluates to True update the variable in the logical expression each time through the loop BEWARE! If the logical expression always evaluates to True , then you get an infinite loop ! We prefer for loops over while loops because of the last point. A for loop will never result in an infinite loop. If a loop can be constructed with for or while , we'll always choose for . Constructing Sequences There are several ways to construct a sequence of values and to save them as a Python list. We have already seen Python's list comprehension syntax. There is also the append list method described below. Sequences by a Formula If a sequence is given by a formula then we can use a list comprehension to construct it. For example, the sequence of squares from 1 to 100 can be constructed using a list comprehension: squares = [d**2 for d in range(1,11)] print(squares) [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] However, we can achieve the same result with a for loop and the append method for lists: # Intialize an empty list squares = [] for d in range(1,11): # Append the next square to the list squares.append(d**2) print(squares) [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] In fact, the two examples above are equivalent. The purpose of list comprehensions is to simplify and compress the syntax into a one-line construction. Recursive Sequences We can only use a list comprehension to construct a sequence when the sequence values are defined by a formula. But what if we want to construct a sequence where the next value depends on previous values? This is called a recursive sequence . For example, consider the Fibonacci sequence : $$ x_1 = 1, x_2 = 1, x_3 = 2, x_4 = 3, x_5 = 5, ... $$ where $$ x_{n} = x_{n-1} + x_{n-2} $$ We can't use a list comprehension to build the list of Fibonacci numbers, and so we must use a for loop with the append method instead. For example, the first 15 Fibonacci numbers are: fibonacci_numbers = [1,1] for n in range(2,15): fibonacci_n = fibonacci_numbers[n-1] + fibonacci_numbers[n-2] fibonacci_numbers.append(fibonacci_n) print(fibonacci_numbers) [1, 1, 2] [1, 1, 2, 3] [1, 1, 2, 3, 5] [1, 1, 2, 3, 5, 8] [1, 1, 2, 3, 5, 8, 13] [1, 1, 2, 3, 5, 8, 13, 21] [1, 1, 2, 3, 5, 8, 13, 21, 34] [1, 1, 2, 3, 5, 8, 13, 21, 34, 55] [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89] [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144] [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233] [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377] [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610] Computing Sums Suppose we want to compute the sum of a sequence of numbers $x_0$, $x_1$, $x_2$, $x_3$, $\\dots$, $x_n$. There are at least two approaches: Compute the entire sequence, store it as a list $[x_0,x_1,x_2,\\dots,x_n]$ and then use the built-in function sum . Initialize a variable with value 0 (and name it result for example), create and add each element in the sequence to result one at a time. The advantage of the second approach is that we don't need to store all the values at once. For example, here are two ways to write a function which computes the sum of squares. For the first approach, use a list comprehension: def sum_of_squares_1(N): \"Compute the sum of squares 1**2 + 2**2 + ... + N**2.\" return sum([n**2 for n in range(1,N + 1)]) sum_of_squares_1(4) 30 For the second approach, use a for loop with the initialize-and-update construction: def sum_of_squares_2(N): \"Compute the sum of squares 1**2 + 2**2 + ... + N**2.\" # Initialize the output value to 0 result = 0 for n in range(1,N + 1): # Update the result by adding the next term result = result + n**2 return result sum_of_squares_2(4) 30 Again, both methods yield the same result however the second uses less memory! Computing Products There is no built-in function to compute products of sequences therefore we'll use an initialize-and-update construction similar to the example above for computing sums. Write a function called factorial which takes a positive integer $N$ and return the factorial $N!$. def factorial(N): \"Compute N! = N(N-1) ... (2)(1) for N >= 1.\" # Initialize the output variable to 1 product = 1 for n in range(2,N + 1): # Update the output variable product = product * n return product Let's test our function for input values for which we know the result: factorial(2) 2 factorial(5) 120 We can use our function to approximate $e$ using the Taylor series for $e^x$: $$ e^x = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!} $$ For example, let's compute the 100th partial sum of the series with $x=1$: sum([1/factorial(k) for k in range(0,101)]) 2.7182818284590455 Searching for Solutions We can use for loops to search for integer solutions of equations. For example, suppose we would like to find all representations of a positive integer $N$ as a sum of two squares . In other words, we want to find all integer solutions $(x,y)$ of the equation: $$ x^2 + y^2 = N $$ Write a function called reps_sum_squares which takes an integer $N$ and finds all representations of $N$ as a sum of squares $x^2 + y^2 = N$ for $0 \\leq x \\leq y$. The function returns the representations as a list of tuples. For example, if $N = 50$ then $1^2 + 7^2 = 50$ and $5^2 + 5^2 = 50$ and the function returns the list [(1, 7),(5, 5)] . Let's outline our approach before we write any code: Given $x \\leq y$, the largest possible value for $x$ is $\\sqrt{\\frac{N}{2}}$ For $x \\leq \\sqrt{\\frac{N}{2}}$, the pair $(x,y)$ is a solution if $N - x^2$ is a square Define a helper function called is_square to test if an integer is square def is_square(n): \"Determine if the integer n is a square.\" if round(n**0.5)**2 == n: return True else: return False def reps_sum_squares(N): '''Find all representations of N as a sum of squares x**2 + y**2 = N. Parameters ---------- N : integer Returns ------- reps : list of tuples of integers List of tuples (x,y) of positive integers such that x**2 + y**2 = N. Examples -------- >>> reps_sum_squares(1105) [(4, 33), (9, 32), (12, 31), (23, 24)] ''' reps = [] if is_square(N/2): # If N/2 is a square, search up to x = (N/2)**0.5 max_x = round((N/2)**0.5) else: # If N/2 is not a square, search up to x = floor((N/2)**0.5) max_x = int((N/2)**0.5) for x in range(0,max_x + 1): y_squared = N - x**2 if is_square(y_squared): y = round(y_squared**0.5) # Append solution (x,y) to list of solutions reps.append((x,y)) return reps reps_sum_squares(1105) [(4, 33), (9, 32), (12, 31), (23, 24)] What is the smallest integer which can be expressed as the sum of squares in 5 different ways? N = 1105 num_reps = 4 while num_reps < 5: N = N + 1 reps = reps_sum_squares(N) num_reps = len(reps) print(N,':',reps_sum_squares(N)) 4225 : [(0, 65), (16, 63), (25, 60), (33, 56), (39, 52)] Examples Prime Numbers A positive integer is prime if it is divisible only by 1 and itself. Write a function called is_prime which takes an input parameter n and returns True or False depending on whether n is prime or not. Let's outline our approach before we write any code: An integer $d$ divides $n$ if there is no remainder of $n$ divided by $d$. Use the modulus operator % to compute the remainder. If $d$ divides $n$ then $n = d q$ for some integer $q$ and either $d \\leq \\sqrt{n}$ or $q \\leq \\sqrt{n}$ (and not both), therefore we need only test if $d$ divides $n$ for integers $d \\leq \\sqrt{n}$ def is_prime(n): \"Determine whether or not n is a prime number.\" if n <= 1: return False # Test if d divides n for d <= n**0.5 for d in range(2,round(n**0.5) + 1): if n % d == 0: # n is divisible by d and so n is not prime return False # If we exit the for loop, then n is not divisible by any d # and therefore n is prime return True Let's test our function on the first 30 numbers: for n in range(0,31): if is_prime(n): print(n,'is prime!') 2 is prime! 3 is prime! 5 is prime! 7 is prime! 11 is prime! 13 is prime! 17 is prime! 19 is prime! 23 is prime! 29 is prime! Our function works! Let's find all the primes between 20,000 and 20,100. for n in range(20000,20100): if is_prime(n): print(n,'is prime!') 20011 is prime! 20021 is prime! 20023 is prime! 20029 is prime! 20047 is prime! 20051 is prime! 20063 is prime! 20071 is prime! 20089 is prime! Divisors Let's write a function called divisors which takes a positive integer $N$ and returns the list of positive integers which divide $N$. def divisors(N): \"Return the list of divisors of N.\" # Initialize the list of divisors (which always includes 1) divisor_list = [1] # Check division by d for d <= N/2 for d in range(2,N // 2 + 1): if N % d == 0: divisor_list.append(d) # N divides itself and so we append N to the list of divisors divisor_list.append(N) return divisor_list Let's test our function: divisors(10) [1, 2, 5, 10] divisors(100) [1, 2, 4, 5, 10, 20, 25, 50, 100] divisors(59) [1, 59] Collatz Conjecture Let $a$ be a positive integer and consider the recursive sequence where $x_0 = a$ and $$ x_{n+1} = \\left\\{ \\begin{array}{cl} x_n/2 & \\text{if } x_n \\text{ is even} \\\\ 3x_n+1 & \\text{if } x_n \\text{ is odd} \\end{array} \\right. $$ The Collatz conjecture states that this sequence will always reach 1. For example, if $a = 10$ then $x_0 = 10$, $x_1 = 5$, $x_2 = 16$, $x_3 = 8$, $x_4 = 4$, $x_5 = 2$ and $x_6 = 1$. Write a function called collatz which takes one input parameter a and returns the sequence of integers defined above and ending with the first occurrence $x_n=1$. def collatz(a): \"Compute the Collatz sequence starting at a and ending at 1.\" # Initialize list with first value a sequence = [a] # Compute values until we reach 1 while sequence[-1] > 1: # Check if the last element in the list is even if sequence[-1] % 2 == 0: # Compute and append the new value sequence.append(sequence[-1] // 2) else: # Compute and append the new value sequence.append(3*sequence[-1] + 1) return sequence Let's test our function: print(collatz(10)) [10, 5, 16, 8, 4, 2, 1] collatz(22) [22, 11, 34, 17, 52, 26, 13, 40, 20, 10, 5, 16, 8, 4, 2, 1] The Collatz conjecture is quite amazing. No matter where we start, the sequence always terminates at 1! a = 123456789 seq = collatz(a) print(\"Collatz sequence for a =\",a) print(\"begins with\",seq[:5]) print(\"ends with\",seq[-5:]) print(\"and has\",len(seq),\"terms.\") Collatz sequence for a = 123456789 begins with [123456789, 370370368, 185185184, 92592592, 46296296] ends with [16, 8, 4, 2, 1] and has 178 terms. Which $a < 1000$ produces the longest sequence? max_length = 1 a_max = 1 for a in range(1,1001): seq_length = len(collatz(a)) if seq_length > max_length: max_length = seq_length a_max = a print('Longest sequence begins with a =',a_max,'and has length',max_length) Longest sequence begins with a = 871 and has length 179 Exercises Exercise 1. Fermat's theorem on the sum of two squares states that every prime number $p$ of the form $4k+1$ can be expressed as the sum of two squares. For example, $5 = 2^2 + 1^2$ and $13 = 3^2 + 2^2$. Find the smallest prime greater than $2019$ of the form $4k+1$ and write it as a sum of squares. (Hint: Use the functions is_prime and reps_sum_squares from this section.) Exercise 2. What is the smallest prime number which can be represented as a sum of squares in 2 different ways? Exercise 3. What is the smallest integer which can be represented as a sum of squares in 3 different ways? Exercise 4. Write a function called primes_between which takes two integer inputs $a$ and $b$ and returns the list of primes in the closed interval $[a,b]$. Exercise 5. Write a function called primes_d_mod_N which takes four integer inputs $a$, $b$, $d$ and $N$ and returns the list of primes in the closed interval $[a,b]$ which are congruent to $d$ mod $N$ (this means that the prime has remainder $d$ after division by $N$). This kind of list is called primes in an arithmetic progression . Exercise 6. Write a function called reciprocal_recursion which takes three positive integers $x_0$, $x_1$ and $N$ and returns the sequence $[x_0,x_1,x_2,\\dots,x_N]$ where $$ x_n = \\frac{1}{x_{n-1}} + \\frac{1}{x_{n-2}} $$ Exercise 7. Write a function called root_sequence which takes input parameters $a$ and $N$, both positive integers, and returns the $N$th term $x_N$ in the sequence: $$ \\begin{align} x_0 &= a \\\\ x_n &= 1 + \\sqrt{x_{n-1}} \\end{align} $$ Does the sequence converge to different values for different starting values $a$? Exercise 8. Write a function called fib_less_than which takes one input $N$ and returns the list of Fibonacci numbers less than $N$. Exercise 9. Write a function called fibonacci_primes which takes an input parameter $N$ and returns the list of Fibonacci numbers less than $N$ which are also prime numbers. Exercise 10. Let $w(N)$ be the number of ways $N$ can be expressed as a sum of two squares $x^2 + y^2 = N$ with $1 \\leq x \\leq y$. Then $$ \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=1}^{N} w(n) = \\frac{\\pi}{8} $$ Compute the left side of the formula for $N=100$ and compare the result to $\\pi / 8$. Exercise 11. A list of positive integers $[a,b,c]$ (with $1 \\leq a < b$) are a Pythagorean triple if $a^2 + b^2 = c^2$. Write a function called py_triples which takes an input parameter $N$ and returns the list of Pythagorean triples [a,b,c] with $c \\leq N$.","title":"Loops"},{"location":"python/loops/#loops","text":"","title":"Loops"},{"location":"python/loops/#for-loops","text":"A for loop allows us to execute a block of code multiple times with some parameters updated each time through the loop. A for loop begins with the for statement: iterable = [1,2,3] for item in iterable: # code block indented 4 spaces print(item) 1 2 3 The main points to observe are: for and in keywords iterable is a sequence object such as a list, tuple or range item is a variable which takes each value in iterable end for statement with a colon : code block indented 4 spaces which executes once for each value in iterable For example, let's print $n^2$ for $n$ from 0 to 5: for n in [0,1,2,3,4,5]: square = n**2 print(n,'squared is',square) print('The for loop is complete!') 0 squared is 0 1 squared is 1 2 squared is 4 3 squared is 9 4 squared is 16 5 squared is 25 The for loop is complete! Copy and paste this code and any of the examples below into the Python visualizer to see each step in a for loop!","title":"for Loops"},{"location":"python/loops/#while-loops","text":"What if we want to execute a block of code multiple times but we don't know exactly how many times? We can't write a for loop because this requires us to set the length of the loop in advance. This is a situation when a while loop is useful. The following example illustrates a while loop : n = 5 while n > 0: print(n) n = n - 1 5 4 3 2 1 The main points to observe are: while keyword a logical expression followed by a colon : loop executes its code block if the logical expression evaluates to True update the variable in the logical expression each time through the loop BEWARE! If the logical expression always evaluates to True , then you get an infinite loop ! We prefer for loops over while loops because of the last point. A for loop will never result in an infinite loop. If a loop can be constructed with for or while , we'll always choose for .","title":"while Loops"},{"location":"python/loops/#constructing-sequences","text":"There are several ways to construct a sequence of values and to save them as a Python list. We have already seen Python's list comprehension syntax. There is also the append list method described below.","title":"Constructing Sequences"},{"location":"python/loops/#sequences-by-a-formula","text":"If a sequence is given by a formula then we can use a list comprehension to construct it. For example, the sequence of squares from 1 to 100 can be constructed using a list comprehension: squares = [d**2 for d in range(1,11)] print(squares) [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] However, we can achieve the same result with a for loop and the append method for lists: # Intialize an empty list squares = [] for d in range(1,11): # Append the next square to the list squares.append(d**2) print(squares) [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] In fact, the two examples above are equivalent. The purpose of list comprehensions is to simplify and compress the syntax into a one-line construction.","title":"Sequences by a Formula"},{"location":"python/loops/#recursive-sequences","text":"We can only use a list comprehension to construct a sequence when the sequence values are defined by a formula. But what if we want to construct a sequence where the next value depends on previous values? This is called a recursive sequence . For example, consider the Fibonacci sequence : $$ x_1 = 1, x_2 = 1, x_3 = 2, x_4 = 3, x_5 = 5, ... $$ where $$ x_{n} = x_{n-1} + x_{n-2} $$ We can't use a list comprehension to build the list of Fibonacci numbers, and so we must use a for loop with the append method instead. For example, the first 15 Fibonacci numbers are: fibonacci_numbers = [1,1] for n in range(2,15): fibonacci_n = fibonacci_numbers[n-1] + fibonacci_numbers[n-2] fibonacci_numbers.append(fibonacci_n) print(fibonacci_numbers) [1, 1, 2] [1, 1, 2, 3] [1, 1, 2, 3, 5] [1, 1, 2, 3, 5, 8] [1, 1, 2, 3, 5, 8, 13] [1, 1, 2, 3, 5, 8, 13, 21] [1, 1, 2, 3, 5, 8, 13, 21, 34] [1, 1, 2, 3, 5, 8, 13, 21, 34, 55] [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89] [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144] [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233] [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377] [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610]","title":"Recursive Sequences"},{"location":"python/loops/#computing-sums","text":"Suppose we want to compute the sum of a sequence of numbers $x_0$, $x_1$, $x_2$, $x_3$, $\\dots$, $x_n$. There are at least two approaches: Compute the entire sequence, store it as a list $[x_0,x_1,x_2,\\dots,x_n]$ and then use the built-in function sum . Initialize a variable with value 0 (and name it result for example), create and add each element in the sequence to result one at a time. The advantage of the second approach is that we don't need to store all the values at once. For example, here are two ways to write a function which computes the sum of squares. For the first approach, use a list comprehension: def sum_of_squares_1(N): \"Compute the sum of squares 1**2 + 2**2 + ... + N**2.\" return sum([n**2 for n in range(1,N + 1)]) sum_of_squares_1(4) 30 For the second approach, use a for loop with the initialize-and-update construction: def sum_of_squares_2(N): \"Compute the sum of squares 1**2 + 2**2 + ... + N**2.\" # Initialize the output value to 0 result = 0 for n in range(1,N + 1): # Update the result by adding the next term result = result + n**2 return result sum_of_squares_2(4) 30 Again, both methods yield the same result however the second uses less memory!","title":"Computing Sums"},{"location":"python/loops/#computing-products","text":"There is no built-in function to compute products of sequences therefore we'll use an initialize-and-update construction similar to the example above for computing sums. Write a function called factorial which takes a positive integer $N$ and return the factorial $N!$. def factorial(N): \"Compute N! = N(N-1) ... (2)(1) for N >= 1.\" # Initialize the output variable to 1 product = 1 for n in range(2,N + 1): # Update the output variable product = product * n return product Let's test our function for input values for which we know the result: factorial(2) 2 factorial(5) 120 We can use our function to approximate $e$ using the Taylor series for $e^x$: $$ e^x = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!} $$ For example, let's compute the 100th partial sum of the series with $x=1$: sum([1/factorial(k) for k in range(0,101)]) 2.7182818284590455","title":"Computing Products"},{"location":"python/loops/#searching-for-solutions","text":"We can use for loops to search for integer solutions of equations. For example, suppose we would like to find all representations of a positive integer $N$ as a sum of two squares . In other words, we want to find all integer solutions $(x,y)$ of the equation: $$ x^2 + y^2 = N $$ Write a function called reps_sum_squares which takes an integer $N$ and finds all representations of $N$ as a sum of squares $x^2 + y^2 = N$ for $0 \\leq x \\leq y$. The function returns the representations as a list of tuples. For example, if $N = 50$ then $1^2 + 7^2 = 50$ and $5^2 + 5^2 = 50$ and the function returns the list [(1, 7),(5, 5)] . Let's outline our approach before we write any code: Given $x \\leq y$, the largest possible value for $x$ is $\\sqrt{\\frac{N}{2}}$ For $x \\leq \\sqrt{\\frac{N}{2}}$, the pair $(x,y)$ is a solution if $N - x^2$ is a square Define a helper function called is_square to test if an integer is square def is_square(n): \"Determine if the integer n is a square.\" if round(n**0.5)**2 == n: return True else: return False def reps_sum_squares(N): '''Find all representations of N as a sum of squares x**2 + y**2 = N. Parameters ---------- N : integer Returns ------- reps : list of tuples of integers List of tuples (x,y) of positive integers such that x**2 + y**2 = N. Examples -------- >>> reps_sum_squares(1105) [(4, 33), (9, 32), (12, 31), (23, 24)] ''' reps = [] if is_square(N/2): # If N/2 is a square, search up to x = (N/2)**0.5 max_x = round((N/2)**0.5) else: # If N/2 is not a square, search up to x = floor((N/2)**0.5) max_x = int((N/2)**0.5) for x in range(0,max_x + 1): y_squared = N - x**2 if is_square(y_squared): y = round(y_squared**0.5) # Append solution (x,y) to list of solutions reps.append((x,y)) return reps reps_sum_squares(1105) [(4, 33), (9, 32), (12, 31), (23, 24)] What is the smallest integer which can be expressed as the sum of squares in 5 different ways? N = 1105 num_reps = 4 while num_reps < 5: N = N + 1 reps = reps_sum_squares(N) num_reps = len(reps) print(N,':',reps_sum_squares(N)) 4225 : [(0, 65), (16, 63), (25, 60), (33, 56), (39, 52)]","title":"Searching for Solutions"},{"location":"python/loops/#examples","text":"","title":"Examples"},{"location":"python/loops/#prime-numbers","text":"A positive integer is prime if it is divisible only by 1 and itself. Write a function called is_prime which takes an input parameter n and returns True or False depending on whether n is prime or not. Let's outline our approach before we write any code: An integer $d$ divides $n$ if there is no remainder of $n$ divided by $d$. Use the modulus operator % to compute the remainder. If $d$ divides $n$ then $n = d q$ for some integer $q$ and either $d \\leq \\sqrt{n}$ or $q \\leq \\sqrt{n}$ (and not both), therefore we need only test if $d$ divides $n$ for integers $d \\leq \\sqrt{n}$ def is_prime(n): \"Determine whether or not n is a prime number.\" if n <= 1: return False # Test if d divides n for d <= n**0.5 for d in range(2,round(n**0.5) + 1): if n % d == 0: # n is divisible by d and so n is not prime return False # If we exit the for loop, then n is not divisible by any d # and therefore n is prime return True Let's test our function on the first 30 numbers: for n in range(0,31): if is_prime(n): print(n,'is prime!') 2 is prime! 3 is prime! 5 is prime! 7 is prime! 11 is prime! 13 is prime! 17 is prime! 19 is prime! 23 is prime! 29 is prime! Our function works! Let's find all the primes between 20,000 and 20,100. for n in range(20000,20100): if is_prime(n): print(n,'is prime!') 20011 is prime! 20021 is prime! 20023 is prime! 20029 is prime! 20047 is prime! 20051 is prime! 20063 is prime! 20071 is prime! 20089 is prime!","title":"Prime Numbers"},{"location":"python/loops/#divisors","text":"Let's write a function called divisors which takes a positive integer $N$ and returns the list of positive integers which divide $N$. def divisors(N): \"Return the list of divisors of N.\" # Initialize the list of divisors (which always includes 1) divisor_list = [1] # Check division by d for d <= N/2 for d in range(2,N // 2 + 1): if N % d == 0: divisor_list.append(d) # N divides itself and so we append N to the list of divisors divisor_list.append(N) return divisor_list Let's test our function: divisors(10) [1, 2, 5, 10] divisors(100) [1, 2, 4, 5, 10, 20, 25, 50, 100] divisors(59) [1, 59]","title":"Divisors"},{"location":"python/loops/#collatz-conjecture","text":"Let $a$ be a positive integer and consider the recursive sequence where $x_0 = a$ and $$ x_{n+1} = \\left\\{ \\begin{array}{cl} x_n/2 & \\text{if } x_n \\text{ is even} \\\\ 3x_n+1 & \\text{if } x_n \\text{ is odd} \\end{array} \\right. $$ The Collatz conjecture states that this sequence will always reach 1. For example, if $a = 10$ then $x_0 = 10$, $x_1 = 5$, $x_2 = 16$, $x_3 = 8$, $x_4 = 4$, $x_5 = 2$ and $x_6 = 1$. Write a function called collatz which takes one input parameter a and returns the sequence of integers defined above and ending with the first occurrence $x_n=1$. def collatz(a): \"Compute the Collatz sequence starting at a and ending at 1.\" # Initialize list with first value a sequence = [a] # Compute values until we reach 1 while sequence[-1] > 1: # Check if the last element in the list is even if sequence[-1] % 2 == 0: # Compute and append the new value sequence.append(sequence[-1] // 2) else: # Compute and append the new value sequence.append(3*sequence[-1] + 1) return sequence Let's test our function: print(collatz(10)) [10, 5, 16, 8, 4, 2, 1] collatz(22) [22, 11, 34, 17, 52, 26, 13, 40, 20, 10, 5, 16, 8, 4, 2, 1] The Collatz conjecture is quite amazing. No matter where we start, the sequence always terminates at 1! a = 123456789 seq = collatz(a) print(\"Collatz sequence for a =\",a) print(\"begins with\",seq[:5]) print(\"ends with\",seq[-5:]) print(\"and has\",len(seq),\"terms.\") Collatz sequence for a = 123456789 begins with [123456789, 370370368, 185185184, 92592592, 46296296] ends with [16, 8, 4, 2, 1] and has 178 terms. Which $a < 1000$ produces the longest sequence? max_length = 1 a_max = 1 for a in range(1,1001): seq_length = len(collatz(a)) if seq_length > max_length: max_length = seq_length a_max = a print('Longest sequence begins with a =',a_max,'and has length',max_length) Longest sequence begins with a = 871 and has length 179","title":"Collatz Conjecture"},{"location":"python/loops/#exercises","text":"Exercise 1. Fermat's theorem on the sum of two squares states that every prime number $p$ of the form $4k+1$ can be expressed as the sum of two squares. For example, $5 = 2^2 + 1^2$ and $13 = 3^2 + 2^2$. Find the smallest prime greater than $2019$ of the form $4k+1$ and write it as a sum of squares. (Hint: Use the functions is_prime and reps_sum_squares from this section.) Exercise 2. What is the smallest prime number which can be represented as a sum of squares in 2 different ways? Exercise 3. What is the smallest integer which can be represented as a sum of squares in 3 different ways? Exercise 4. Write a function called primes_between which takes two integer inputs $a$ and $b$ and returns the list of primes in the closed interval $[a,b]$. Exercise 5. Write a function called primes_d_mod_N which takes four integer inputs $a$, $b$, $d$ and $N$ and returns the list of primes in the closed interval $[a,b]$ which are congruent to $d$ mod $N$ (this means that the prime has remainder $d$ after division by $N$). This kind of list is called primes in an arithmetic progression . Exercise 6. Write a function called reciprocal_recursion which takes three positive integers $x_0$, $x_1$ and $N$ and returns the sequence $[x_0,x_1,x_2,\\dots,x_N]$ where $$ x_n = \\frac{1}{x_{n-1}} + \\frac{1}{x_{n-2}} $$ Exercise 7. Write a function called root_sequence which takes input parameters $a$ and $N$, both positive integers, and returns the $N$th term $x_N$ in the sequence: $$ \\begin{align} x_0 &= a \\\\ x_n &= 1 + \\sqrt{x_{n-1}} \\end{align} $$ Does the sequence converge to different values for different starting values $a$? Exercise 8. Write a function called fib_less_than which takes one input $N$ and returns the list of Fibonacci numbers less than $N$. Exercise 9. Write a function called fibonacci_primes which takes an input parameter $N$ and returns the list of Fibonacci numbers less than $N$ which are also prime numbers. Exercise 10. Let $w(N)$ be the number of ways $N$ can be expressed as a sum of two squares $x^2 + y^2 = N$ with $1 \\leq x \\leq y$. Then $$ \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=1}^{N} w(n) = \\frac{\\pi}{8} $$ Compute the left side of the formula for $N=100$ and compare the result to $\\pi / 8$. Exercise 11. A list of positive integers $[a,b,c]$ (with $1 \\leq a < b$) are a Pythagorean triple if $a^2 + b^2 = c^2$. Write a function called py_triples which takes an input parameter $N$ and returns the list of Pythagorean triples [a,b,c] with $c \\leq N$.","title":"Exercises"},{"location":"python/modules-packages/","text":"Modules and Packages Under construction","title":"Modules and Packages"},{"location":"python/modules-packages/#modules-and-packages","text":"Under construction","title":"Modules and Packages"},{"location":"python/numbers/","text":"Numbers The main numeric types in Python are integers, floating point numbers (or floats) and complex numbers. The syntax for arithmetic operators are: addition + , subtraction - , multiplication * , division / and exponentiation ** . Integers Add integers: 8 + 12 20 Subtract integers: 2019 - 21 1998 Multiply integers: 45 * 11 495 Divide integers (and notice that division of integers always returns a float): 100 / 4 25.0 Compute powers of integers: 2**10 1024 Use the built-in function type() to verify the type of a Python object: type(42) int Floating Point Numbers A floating point number (or float) is a real number written in decimal form. Python stores floats and integers in different ways and if we combine integers and floats using arithmetic operations the result is always a float. Approximate $\\sqrt{2} \\,$: 2**0.5 1.4142135623730951 Approximate $2 \\pi$: 2 * 3.14159 6.28318 Use scientific notation to create $0.00001$: 1e-5 1e-05 Again, use the type() function to verify the type of a number: type(42) int type(42.0) float Complex Numbers Use the built-in function complex() to create a complex number in Python or use the letter j for $j = \\sqrt{-1}$. The built-in function complex() takes 2 parameters defining the real and imaginary part of the complex number. Create the complex number $1 + j$: complex(1,1) (1+1j) Add complex numbers: (1 + 2j) + (2 - 3j) (3-1j) Multiply complex numbers: (2 - 1j) * (5 + 2j) (12-1j) Use the type() function to verify the type of a number: type(2 - 7j) complex Arithmetic Operators The syntax for arithmetic operators in Python are: Operator Description + addition - subtraction * multiplication / division ** exponentiation % remainder (or modulo) // integer division Notice that division of integers always returns a float: 4 / 3 1.3333333333333333 Even if the mathematical result is an integer: 4 / 2 2.0 Use parentheses to group combinations of arithmetic operations: 5 * (4 + 3) - 2 33 A positive integer power of an integer is again an integer: 2**4 16 A negative integer power of an integer is a float: 2**(-1) 0.5 An exponent involving a float is a float: 9**0.5 3.0 The remainder operator computes the remainder of division of integers: 11 % 4 3 Integer division is: 11 // 4 2 Examples Taylor Approximation The Taylor series of the exponential function $e^x$ is: $$ e^x = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!} $$ Compute the Taylor polynomial of degree 5 evaluated at $x = 1$ to find an approximation of $e$ $$ e \\approx \\frac{1}{0!} + \\frac{1}{1!} + \\frac{1}{2!} + \\frac{1}{3!} + \\frac{1}{4!} + \\frac{1}{5!} $$ 1 + 1 + 1/2 + 1/(3*2) + 1/(4*3*2) + 1/(5*4*3*2) 2.7166666666666663 Ramanujan's Pi Formula Srinivasa Ramanujan discovered the following beautiful (and very rapidly converging) series representation of $\\pi$: $$ \\frac{1}{\\pi} = \\frac{2 \\sqrt{2}}{99^2} \\sum_{k = 0}^{\\infty} \\frac{(4k)!}{k!^4} \\frac{1103 + 26390k}{396^{4k}} $$ Let's find an approximation of $\\pi$ by computing the reciprocal of the sum of the first 3 terms of the series: $$ \\pi \\approx \\frac{99^2}{2 \\sqrt{2}} \\frac{1}{\\left( 1103 + 4! \\frac{1103 + 26390}{396^{4}} + \\frac{8!}{2^4} \\frac{1103 + 26390(2)}{396^{8}} \\right)} $$ 99**2 / (2 * 2**0.5) / (1103 + 4*3*2 * (26390 + 1103) / 396**4 + 8*7*6*5*4*3*2 / 2**4 * (26390*2 + 1103) / 396**8) 3.141592653589793 These are exactly the first 16 digits of $\\pi$ . Exercises Exercise 1. The Taylor series of $\\cos x$ is: $$ \\cos x = \\sum_{k=0}^{\\infty} (-1)^k \\frac{x^{2k}}{(2k)!} $$ Compute the Taylor polynomial of degree 6 evaluated at $x=2$: $$ \\cos(2) \\approx 1 - \\frac{2^2}{2!} + \\frac{2^4}{4!} - \\frac{2^6}{6!} $$ Exercise 2. The Riemann zeta function is the infinite series: $$ \\zeta(s) = \\sum_{n=1}^{\\infty} \\frac{1}{n^s} $$ and is intimately related to prime numbers by the Euler product formula : $$ \\sum_{n=1}^{\\infty} \\frac{1}{n^s} = \\prod_p \\left( \\frac{1}{1 - p^{-s}} \\right) $$ where the product is over all primes $p = 2,3,5,7,11,13,\\dots$ Compute the 5th partial sum for $s=2$ $$ 1 + \\frac{1}{2^2} + \\frac{1}{3^2} + \\frac{1}{4^2} + \\frac{1}{5^2} $$ Compute the 5th partial product for $s=2$: $$ \\left( \\frac{1}{1 - 2^{-2}} \\right) \\left( \\frac{1}{1 - 3^{-2}} \\right) \\left( \\frac{1}{1 - 5^{-2}} \\right) \\left( \\frac{1}{1 - 7^{-2}} \\right) \\left( \\frac{1}{1 - 11^{-2}} \\right) $$ Given Euler's special value formula : $$ \\zeta(2) = \\frac{\\pi^2}{6} $$ which converges more quickly: the infinite series or product? Exercise 3. The continued fraction for $\\sqrt{2}$ is: $$ \\sqrt{2} = 1 + \\frac{1}{2 + \\frac{1}{2 + \\frac{1}{2 + \\frac{1}{2 + \\ddots}}}} $$ Compute the following (partial) continued fraction to approximate $\\sqrt{2}$: $$ \\sqrt{2} \\approx 1 + \\frac{1}{2 + \\frac{1}{2 + \\frac{1}{2 + \\frac{1}{2}}}} $$ Exercise 4. Suppose type(x) is int and type(y) is float . Determine: type(x / y) if y is nonzero type(x * y) type(x + y) Exercise 5. Suppose both type(x) and type(y) are int . Determine: type(x / y) if y is nonzero type(x * y) type(x + y) Exercise 6. Suppose type(x) is float and type(y) is int . Determine type(x ** y) . Exercise 7. Suppose type(x) is int and type(y) is float . Determine type(x ** y) . Exercise 8. Suppose both type(x) and type(y) are int . Determine type(x ** y) .","title":"Numbers"},{"location":"python/numbers/#numbers","text":"The main numeric types in Python are integers, floating point numbers (or floats) and complex numbers. The syntax for arithmetic operators are: addition + , subtraction - , multiplication * , division / and exponentiation ** .","title":"Numbers"},{"location":"python/numbers/#integers","text":"Add integers: 8 + 12 20 Subtract integers: 2019 - 21 1998 Multiply integers: 45 * 11 495 Divide integers (and notice that division of integers always returns a float): 100 / 4 25.0 Compute powers of integers: 2**10 1024 Use the built-in function type() to verify the type of a Python object: type(42) int","title":"Integers"},{"location":"python/numbers/#floating-point-numbers","text":"A floating point number (or float) is a real number written in decimal form. Python stores floats and integers in different ways and if we combine integers and floats using arithmetic operations the result is always a float. Approximate $\\sqrt{2} \\,$: 2**0.5 1.4142135623730951 Approximate $2 \\pi$: 2 * 3.14159 6.28318 Use scientific notation to create $0.00001$: 1e-5 1e-05 Again, use the type() function to verify the type of a number: type(42) int type(42.0) float","title":"Floating Point Numbers"},{"location":"python/numbers/#complex-numbers","text":"Use the built-in function complex() to create a complex number in Python or use the letter j for $j = \\sqrt{-1}$. The built-in function complex() takes 2 parameters defining the real and imaginary part of the complex number. Create the complex number $1 + j$: complex(1,1) (1+1j) Add complex numbers: (1 + 2j) + (2 - 3j) (3-1j) Multiply complex numbers: (2 - 1j) * (5 + 2j) (12-1j) Use the type() function to verify the type of a number: type(2 - 7j) complex","title":"Complex Numbers"},{"location":"python/numbers/#arithmetic-operators","text":"The syntax for arithmetic operators in Python are: Operator Description + addition - subtraction * multiplication / division ** exponentiation % remainder (or modulo) // integer division Notice that division of integers always returns a float: 4 / 3 1.3333333333333333 Even if the mathematical result is an integer: 4 / 2 2.0 Use parentheses to group combinations of arithmetic operations: 5 * (4 + 3) - 2 33 A positive integer power of an integer is again an integer: 2**4 16 A negative integer power of an integer is a float: 2**(-1) 0.5 An exponent involving a float is a float: 9**0.5 3.0 The remainder operator computes the remainder of division of integers: 11 % 4 3 Integer division is: 11 // 4 2","title":"Arithmetic Operators"},{"location":"python/numbers/#examples","text":"","title":"Examples"},{"location":"python/numbers/#taylor-approximation","text":"The Taylor series of the exponential function $e^x$ is: $$ e^x = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!} $$ Compute the Taylor polynomial of degree 5 evaluated at $x = 1$ to find an approximation of $e$ $$ e \\approx \\frac{1}{0!} + \\frac{1}{1!} + \\frac{1}{2!} + \\frac{1}{3!} + \\frac{1}{4!} + \\frac{1}{5!} $$ 1 + 1 + 1/2 + 1/(3*2) + 1/(4*3*2) + 1/(5*4*3*2) 2.7166666666666663","title":"Taylor Approximation"},{"location":"python/numbers/#ramanujans-pi-formula","text":"Srinivasa Ramanujan discovered the following beautiful (and very rapidly converging) series representation of $\\pi$: $$ \\frac{1}{\\pi} = \\frac{2 \\sqrt{2}}{99^2} \\sum_{k = 0}^{\\infty} \\frac{(4k)!}{k!^4} \\frac{1103 + 26390k}{396^{4k}} $$ Let's find an approximation of $\\pi$ by computing the reciprocal of the sum of the first 3 terms of the series: $$ \\pi \\approx \\frac{99^2}{2 \\sqrt{2}} \\frac{1}{\\left( 1103 + 4! \\frac{1103 + 26390}{396^{4}} + \\frac{8!}{2^4} \\frac{1103 + 26390(2)}{396^{8}} \\right)} $$ 99**2 / (2 * 2**0.5) / (1103 + 4*3*2 * (26390 + 1103) / 396**4 + 8*7*6*5*4*3*2 / 2**4 * (26390*2 + 1103) / 396**8) 3.141592653589793 These are exactly the first 16 digits of $\\pi$ .","title":"Ramanujan's Pi Formula"},{"location":"python/numbers/#exercises","text":"Exercise 1. The Taylor series of $\\cos x$ is: $$ \\cos x = \\sum_{k=0}^{\\infty} (-1)^k \\frac{x^{2k}}{(2k)!} $$ Compute the Taylor polynomial of degree 6 evaluated at $x=2$: $$ \\cos(2) \\approx 1 - \\frac{2^2}{2!} + \\frac{2^4}{4!} - \\frac{2^6}{6!} $$ Exercise 2. The Riemann zeta function is the infinite series: $$ \\zeta(s) = \\sum_{n=1}^{\\infty} \\frac{1}{n^s} $$ and is intimately related to prime numbers by the Euler product formula : $$ \\sum_{n=1}^{\\infty} \\frac{1}{n^s} = \\prod_p \\left( \\frac{1}{1 - p^{-s}} \\right) $$ where the product is over all primes $p = 2,3,5,7,11,13,\\dots$ Compute the 5th partial sum for $s=2$ $$ 1 + \\frac{1}{2^2} + \\frac{1}{3^2} + \\frac{1}{4^2} + \\frac{1}{5^2} $$ Compute the 5th partial product for $s=2$: $$ \\left( \\frac{1}{1 - 2^{-2}} \\right) \\left( \\frac{1}{1 - 3^{-2}} \\right) \\left( \\frac{1}{1 - 5^{-2}} \\right) \\left( \\frac{1}{1 - 7^{-2}} \\right) \\left( \\frac{1}{1 - 11^{-2}} \\right) $$ Given Euler's special value formula : $$ \\zeta(2) = \\frac{\\pi^2}{6} $$ which converges more quickly: the infinite series or product? Exercise 3. The continued fraction for $\\sqrt{2}$ is: $$ \\sqrt{2} = 1 + \\frac{1}{2 + \\frac{1}{2 + \\frac{1}{2 + \\frac{1}{2 + \\ddots}}}} $$ Compute the following (partial) continued fraction to approximate $\\sqrt{2}$: $$ \\sqrt{2} \\approx 1 + \\frac{1}{2 + \\frac{1}{2 + \\frac{1}{2 + \\frac{1}{2}}}} $$ Exercise 4. Suppose type(x) is int and type(y) is float . Determine: type(x / y) if y is nonzero type(x * y) type(x + y) Exercise 5. Suppose both type(x) and type(y) are int . Determine: type(x / y) if y is nonzero type(x * y) type(x + y) Exercise 6. Suppose type(x) is float and type(y) is int . Determine type(x ** y) . Exercise 7. Suppose type(x) is int and type(y) is float . Determine type(x ** y) . Exercise 8. Suppose both type(x) and type(y) are int . Determine type(x ** y) .","title":"Exercises"},{"location":"python/sequences/","text":"Sequences The main sequence types in Python are lists, tuples and range objects. The main differences between these sequence objects are: Lists are mutable and their elements are usually homogeneous (things of the same type making a list of similar objects) Tuples are immutable and their elements are usually heterogeneous (things of different types making a tuple describing a single structure) Range objects are efficient sequences of integers (commonly used in for loops), use a small amount of memory and yield items only when needed Lists Create a list using square brackets [ ... ] with items separated by commas. For example, create a list of square integers, assign it to a variable and use the built-in function print() to display the list: squares = [1,4,9,16,25] print(squares) [1, 4, 9, 16, 25] Lists may contain data of any type including other lists: points = [[0,0],[0,1],[1,1],[0,1]] print(points) [[0, 0], [0, 1], [1, 1], [0, 1]] Index Access the elements of a list by their index: primes = [2,3,5,7,11,13,17,19,23,29] print(primes[0]) 2 Notice that lists are indexed starting at 0: print(primes[1]) print(primes[2]) print(primes[6]) 3 5 17 Use negative indices to access elements starting from the end of the list: print(primes[-1]) print(primes[-2]) 29 23 Since lists are mutable, we may assign new values to entries in a list: primes[0] = -1 print(primes) [-1, 3, 5, 7, 11, 13, 17, 19, 23, 29] Use multiple indices to access entries in a list of lists: pairs = [[0,1],[2,3],[4,5],[6,7]] print(pairs[2][1]) 5 Slice Create a new list from a sublist (called a slice): fibonacci = [1,1,2,3,5,8,13,21,34,55,89,144] print(fibonacci[4:7]) print(fibonacci[6:]) print(fibonacci[:-2]) [5, 8, 13] [13, 21, 34, 55, 89, 144] [1, 1, 2, 3, 5, 8, 13, 21, 34, 55] Notice in the example fibonacci[4:7] the slice begins at index 4 and goes up to but not including index 7. This makes sense since the length of the slice is then 7 - 4 = 3. A slice can skip over entries in a list. For example, create a slice from every third entry from index 0 to 11: print(fibonacci[0:11:3]) [1, 3, 13, 55] Concatenate The addition operator + concatenates lists: one = [1] two = [2,2] three = [3,3,3] numbers = one + two + three print(numbers) [1, 2, 2, 3, 3, 3] Append Add a value to the end of a list using the append() list method: squares = [1,4,9,16,25] squares.append(36) print(squares) [1, 4, 9, 16, 25, 36] What is an object method? First, an object in Python (such as a list) contains data as well as functions (called methods) to manipulate that data. Everything in Python is an object! The list squares in the cell above contains the integer entries (the data) but it also has methods like append() to manipulate the data. We'll see more about objects and methods later on. For now, see the documentation for a complete list of list methods . Tuples Create a tuple with parentheses ( ... ) : triple = (5,12,13) print(triple) (5, 12, 13) Indexing, slicing and concatenating work for tuples in the exact same way as for lists: print(triple[0]) print(triple[-1]) print(triple[1:3]) 5 13 (12, 13) Range Objects Create a range object with the built-in function range() . The parameters a , b and step in range(a,b,step) are integers and the function creates an object which represents the sequence of integers from a to b (exclusively) incremented by step . (The parameter step may be omitted and is equal to 1 by default.) digits_range = range(0,10) print(digits_range) range(0, 10) Notice that a range object does not display the values of its entries when printed. This is because a range object is an efficient sequence which yields values only when needed. Use the built-in function list() to convert a range object to a list: digits_range = range(0,10) digits_list = list(digits_range) print(digits_list) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Create a range of even integers and convert it to a list: even_list = list(range(0,10,2)) print(even_list) [0, 2, 4, 6, 8] Unpacking a Sequence One of the features of a Python sequence is unpacking where we assign all the entries of a sequence to variables in a single operation. For example, create a tuple representing a date and unpack the data as year , month and and day : triple = (5,12,13) x,y,z = triple print(x) print(y) print(z) 5 12 13 List Comprehensions The built-in function range() is an efficient tool for creating sequences of integers but what about an arbitrary sequence? It is very inefficient to create a sequence by manually typing the numbers. For example, simply typing out the numbers from 1 to 20 takes a long time! numbers = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20] print(numbers) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] Python has a beautiful syntax for creating lists called list comprehensions . The syntax is: [expression for item in iterable] where: iterable is a range, list, tuple, or any kind of sequence object item is a variable name which takes each value in the iterable expression is a Python expression which is calculated for each value of item Use a list comprehension to create the list from 1 to 20: numbers = [n for n in range(1,21)] print(numbers) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] Create the list of square integers from $1$ to $100$: squares = [n**2 for n in range(1,11)] print(squares) [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] Create the periodic sequence $0,1,2,0,1,2,0,1,2,\\dots$ of length 21 (using the remainder operator % ): zero_one_two = [n%3 for n in range(0,21)] print(zero_one_two) [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2] Built-in Functions for Sequences Python has several built-in functions for computing with sequences. For example, compute the length of a list: len([1,2,3]) 3 Compute the sum, maximum and minimum of a list of numbers: random = [3,-5,7,8,-1] print(sum(random)) print(max(random)) print(min(random)) 12 8 -5 Sort the list: sorted(random) [-5, -1, 3, 7, 8] Sum the numbers from 1 to 100: one_to_hundred = range(1,101) print(sum(one_to_hundred)) 5050 Examples Triangular Numbers The formula for the sum of integers from 1 to $N$ (also known as triangular numbers ) is given by: $$ \\sum_{k=1}^N k = \\frac{N(N+1)}{2} $$ Let's verify the formula for $N=1000$: N = 1000 left_side = sum([k for k in range(1,N+1)]) right_side = N*(N+1)/2 print(left_side) print(right_side) 500500 500500.0 Notice the results agree (although the right side is a float since we used division). Sum of Squares The sum of squares (a special case of a geometric series ) is given by the formula: $$ \\sum_{k=1}^N k^2 = \\frac{N(N+1)(2N+1)}{6} $$ Let's verify the formula for $N=2000$: N = 2000 left_side = sum([k**2 for k in range(1,N+1)]) right_side = N*(N+1)*(2*N+1)/6 print(left_side) print(right_side) 2668667000 2668667000.0 Riemann Zeta Function The Riemann zeta function is the infinite series: $$ \\zeta(s) = \\sum_{n=1}^{\\infty} \\frac{1}{n^s} $$ Its values are very mysterious! Let's verify the special value formula: $$ \\zeta(4) = \\sum_{n=1}^{\\infty} \\frac{1}{n^4} = \\frac{\\pi^4}{90} $$ Compute the 1000th partial sum of the series: terms = [1/n**4 for n in range(1,1001)] sum(terms) 1.082323233378306 Compare to an approximation of $\\frac{\\pi^4}{90}$: 3.14159**4/90 1.082319576918468 Exercises Exercise 1. The Maclaurin series of $\\arctan(x)$ is: $$ \\arctan(x) = \\sum_{n = 0}^{\\infty} \\frac{(-1)^nx^{2n + 1}}{2n+1} $$ Substituting $x = 1$ gives a series representation of $\\pi/4$. Compute the partial sum up to $N=5000$ to approximate: $$ \\frac{\\pi}{4} \\approx \\sum_{n = 0}^{5000} \\frac{(-1)^nx^{2n + 1}}{2n+1} $$ Exercise 2. Compute the partial sum of the alternating harmonic series : $$\\sum_{n=1}^{2000}\\frac{(-1)^{n+1}}{n}$$ Exercise 3. Write a list comprehension to create the list of lists: [[0, 0], [1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36], [7, 49]]","title":"Sequences"},{"location":"python/sequences/#sequences","text":"The main sequence types in Python are lists, tuples and range objects. The main differences between these sequence objects are: Lists are mutable and their elements are usually homogeneous (things of the same type making a list of similar objects) Tuples are immutable and their elements are usually heterogeneous (things of different types making a tuple describing a single structure) Range objects are efficient sequences of integers (commonly used in for loops), use a small amount of memory and yield items only when needed","title":"Sequences"},{"location":"python/sequences/#lists","text":"Create a list using square brackets [ ... ] with items separated by commas. For example, create a list of square integers, assign it to a variable and use the built-in function print() to display the list: squares = [1,4,9,16,25] print(squares) [1, 4, 9, 16, 25] Lists may contain data of any type including other lists: points = [[0,0],[0,1],[1,1],[0,1]] print(points) [[0, 0], [0, 1], [1, 1], [0, 1]]","title":"Lists"},{"location":"python/sequences/#index","text":"Access the elements of a list by their index: primes = [2,3,5,7,11,13,17,19,23,29] print(primes[0]) 2 Notice that lists are indexed starting at 0: print(primes[1]) print(primes[2]) print(primes[6]) 3 5 17 Use negative indices to access elements starting from the end of the list: print(primes[-1]) print(primes[-2]) 29 23 Since lists are mutable, we may assign new values to entries in a list: primes[0] = -1 print(primes) [-1, 3, 5, 7, 11, 13, 17, 19, 23, 29] Use multiple indices to access entries in a list of lists: pairs = [[0,1],[2,3],[4,5],[6,7]] print(pairs[2][1]) 5","title":"Index"},{"location":"python/sequences/#slice","text":"Create a new list from a sublist (called a slice): fibonacci = [1,1,2,3,5,8,13,21,34,55,89,144] print(fibonacci[4:7]) print(fibonacci[6:]) print(fibonacci[:-2]) [5, 8, 13] [13, 21, 34, 55, 89, 144] [1, 1, 2, 3, 5, 8, 13, 21, 34, 55] Notice in the example fibonacci[4:7] the slice begins at index 4 and goes up to but not including index 7. This makes sense since the length of the slice is then 7 - 4 = 3. A slice can skip over entries in a list. For example, create a slice from every third entry from index 0 to 11: print(fibonacci[0:11:3]) [1, 3, 13, 55]","title":"Slice"},{"location":"python/sequences/#concatenate","text":"The addition operator + concatenates lists: one = [1] two = [2,2] three = [3,3,3] numbers = one + two + three print(numbers) [1, 2, 2, 3, 3, 3]","title":"Concatenate"},{"location":"python/sequences/#append","text":"Add a value to the end of a list using the append() list method: squares = [1,4,9,16,25] squares.append(36) print(squares) [1, 4, 9, 16, 25, 36] What is an object method? First, an object in Python (such as a list) contains data as well as functions (called methods) to manipulate that data. Everything in Python is an object! The list squares in the cell above contains the integer entries (the data) but it also has methods like append() to manipulate the data. We'll see more about objects and methods later on. For now, see the documentation for a complete list of list methods .","title":"Append"},{"location":"python/sequences/#tuples","text":"Create a tuple with parentheses ( ... ) : triple = (5,12,13) print(triple) (5, 12, 13) Indexing, slicing and concatenating work for tuples in the exact same way as for lists: print(triple[0]) print(triple[-1]) print(triple[1:3]) 5 13 (12, 13)","title":"Tuples"},{"location":"python/sequences/#range-objects","text":"Create a range object with the built-in function range() . The parameters a , b and step in range(a,b,step) are integers and the function creates an object which represents the sequence of integers from a to b (exclusively) incremented by step . (The parameter step may be omitted and is equal to 1 by default.) digits_range = range(0,10) print(digits_range) range(0, 10) Notice that a range object does not display the values of its entries when printed. This is because a range object is an efficient sequence which yields values only when needed. Use the built-in function list() to convert a range object to a list: digits_range = range(0,10) digits_list = list(digits_range) print(digits_list) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Create a range of even integers and convert it to a list: even_list = list(range(0,10,2)) print(even_list) [0, 2, 4, 6, 8]","title":"Range Objects"},{"location":"python/sequences/#unpacking-a-sequence","text":"One of the features of a Python sequence is unpacking where we assign all the entries of a sequence to variables in a single operation. For example, create a tuple representing a date and unpack the data as year , month and and day : triple = (5,12,13) x,y,z = triple print(x) print(y) print(z) 5 12 13","title":"Unpacking a Sequence"},{"location":"python/sequences/#list-comprehensions","text":"The built-in function range() is an efficient tool for creating sequences of integers but what about an arbitrary sequence? It is very inefficient to create a sequence by manually typing the numbers. For example, simply typing out the numbers from 1 to 20 takes a long time! numbers = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20] print(numbers) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] Python has a beautiful syntax for creating lists called list comprehensions . The syntax is: [expression for item in iterable] where: iterable is a range, list, tuple, or any kind of sequence object item is a variable name which takes each value in the iterable expression is a Python expression which is calculated for each value of item Use a list comprehension to create the list from 1 to 20: numbers = [n for n in range(1,21)] print(numbers) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] Create the list of square integers from $1$ to $100$: squares = [n**2 for n in range(1,11)] print(squares) [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] Create the periodic sequence $0,1,2,0,1,2,0,1,2,\\dots$ of length 21 (using the remainder operator % ): zero_one_two = [n%3 for n in range(0,21)] print(zero_one_two) [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]","title":"List Comprehensions"},{"location":"python/sequences/#built-in-functions-for-sequences","text":"Python has several built-in functions for computing with sequences. For example, compute the length of a list: len([1,2,3]) 3 Compute the sum, maximum and minimum of a list of numbers: random = [3,-5,7,8,-1] print(sum(random)) print(max(random)) print(min(random)) 12 8 -5 Sort the list: sorted(random) [-5, -1, 3, 7, 8] Sum the numbers from 1 to 100: one_to_hundred = range(1,101) print(sum(one_to_hundred)) 5050","title":"Built-in Functions for Sequences"},{"location":"python/sequences/#examples","text":"","title":"Examples"},{"location":"python/sequences/#triangular-numbers","text":"The formula for the sum of integers from 1 to $N$ (also known as triangular numbers ) is given by: $$ \\sum_{k=1}^N k = \\frac{N(N+1)}{2} $$ Let's verify the formula for $N=1000$: N = 1000 left_side = sum([k for k in range(1,N+1)]) right_side = N*(N+1)/2 print(left_side) print(right_side) 500500 500500.0 Notice the results agree (although the right side is a float since we used division).","title":"Triangular Numbers"},{"location":"python/sequences/#sum-of-squares","text":"The sum of squares (a special case of a geometric series ) is given by the formula: $$ \\sum_{k=1}^N k^2 = \\frac{N(N+1)(2N+1)}{6} $$ Let's verify the formula for $N=2000$: N = 2000 left_side = sum([k**2 for k in range(1,N+1)]) right_side = N*(N+1)*(2*N+1)/6 print(left_side) print(right_side) 2668667000 2668667000.0","title":"Sum of Squares"},{"location":"python/sequences/#riemann-zeta-function","text":"The Riemann zeta function is the infinite series: $$ \\zeta(s) = \\sum_{n=1}^{\\infty} \\frac{1}{n^s} $$ Its values are very mysterious! Let's verify the special value formula: $$ \\zeta(4) = \\sum_{n=1}^{\\infty} \\frac{1}{n^4} = \\frac{\\pi^4}{90} $$ Compute the 1000th partial sum of the series: terms = [1/n**4 for n in range(1,1001)] sum(terms) 1.082323233378306 Compare to an approximation of $\\frac{\\pi^4}{90}$: 3.14159**4/90 1.082319576918468","title":"Riemann Zeta Function"},{"location":"python/sequences/#exercises","text":"Exercise 1. The Maclaurin series of $\\arctan(x)$ is: $$ \\arctan(x) = \\sum_{n = 0}^{\\infty} \\frac{(-1)^nx^{2n + 1}}{2n+1} $$ Substituting $x = 1$ gives a series representation of $\\pi/4$. Compute the partial sum up to $N=5000$ to approximate: $$ \\frac{\\pi}{4} \\approx \\sum_{n = 0}^{5000} \\frac{(-1)^nx^{2n + 1}}{2n+1} $$ Exercise 2. Compute the partial sum of the alternating harmonic series : $$\\sum_{n=1}^{2000}\\frac{(-1)^{n+1}}{n}$$ Exercise 3. Write a list comprehension to create the list of lists: [[0, 0], [1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36], [7, 49]]","title":"Exercises"},{"location":"python/text/","text":"Text Under construction","title":"Text"},{"location":"python/text/#text","text":"Under construction","title":"Text"},{"location":"python/variables/","text":"Variables Just like the familiar variables $x$ and $y$ in mathematics, we use variables in programming to easily manipulate values. The assignment operator = assigns values to variables in Python. Choose descriptive variable names and avoid special Python reserved words. Assign Values to Variables Assign a value to a variable using the assignment operator = . For example, assign the integer 2 to the variable x : x = 2 The assignment operator does not produce any output and so the cell above does not produce any output in a Jupyter notebook. Use the built-in function print to display the value assigned to a variable: print(x) 2 Compute new values using variables and operators: 1 + x + x**2 + x**3 15 Use the built-in function type to verify the datatype of the value assigned to a variable: pi = 3.14159 type(pi) float Naming Conventions We can use any set of letters, numbers and underscores to make variable names however a variable name cannot begin with a number. There are many different kinds of naming conventions and we refer to the Style Guide for Python Code (PEP8) for a summary. In this book we use lower_case_with_underscores variable names and single lowercase letter variable names such as x . It is good practice to use descriptive variable names to make your code more readable for other people. For example, the distance from Vancouver to Halifax along the Trans-Canada Highway is approximately 5799 kilometres. We write the following code to convert this value to miles: distance_km = 5799 miles_per_km = 0.6214 distance_miles = distance_km * miles_per_km print(distance_miles) 3603.4986 Names to Avoid It is good practice to use variable names which describe the value assigned to it. However there are words that we should not use as variable names because these words already have special meaning in Python. Reserved Words Summarized below are the reserved words in Python 3 . Python will raise an error if you try to assign a value to any of these keywords and so you must avoid these as variable names. False class finally is return None continue for lambda try True def from nonlocal while and del global not with as elif if or yield assert else import pass break except in raise Built-in Function Names There are several functions which are included in the standard Python library. Do not use the names of these functions as variable names otherwise the reference to the built-in function will be lost. For example, do not use sum , min , max , list or sorted as a variable name. See the full list of builtin functions . Jupyter Magic: whos The Jupyer magic command whos lists all variables in the current Jupyter notebook and their types: x = 2 pi = 3.14159 distance_km = 5799 miles_per_km = 0.6214 distance_miles = distance_km * miles_per_km whos Variable Type Data/Info ----------------------------------- distance_km int 5799 distance_miles float 3603.4986 miles_per_km float 0.6214 pi float 3.14159 x int 2 Exercises Exercise 1. True or False: Python does not allow assigning a value to variable name which is already a builtin Python function such as sum , min , max .","title":"Variables"},{"location":"python/variables/#variables","text":"Just like the familiar variables $x$ and $y$ in mathematics, we use variables in programming to easily manipulate values. The assignment operator = assigns values to variables in Python. Choose descriptive variable names and avoid special Python reserved words.","title":"Variables"},{"location":"python/variables/#assign-values-to-variables","text":"Assign a value to a variable using the assignment operator = . For example, assign the integer 2 to the variable x : x = 2 The assignment operator does not produce any output and so the cell above does not produce any output in a Jupyter notebook. Use the built-in function print to display the value assigned to a variable: print(x) 2 Compute new values using variables and operators: 1 + x + x**2 + x**3 15 Use the built-in function type to verify the datatype of the value assigned to a variable: pi = 3.14159 type(pi) float","title":"Assign Values to Variables"},{"location":"python/variables/#naming-conventions","text":"We can use any set of letters, numbers and underscores to make variable names however a variable name cannot begin with a number. There are many different kinds of naming conventions and we refer to the Style Guide for Python Code (PEP8) for a summary. In this book we use lower_case_with_underscores variable names and single lowercase letter variable names such as x . It is good practice to use descriptive variable names to make your code more readable for other people. For example, the distance from Vancouver to Halifax along the Trans-Canada Highway is approximately 5799 kilometres. We write the following code to convert this value to miles: distance_km = 5799 miles_per_km = 0.6214 distance_miles = distance_km * miles_per_km print(distance_miles) 3603.4986","title":"Naming Conventions"},{"location":"python/variables/#names-to-avoid","text":"It is good practice to use variable names which describe the value assigned to it. However there are words that we should not use as variable names because these words already have special meaning in Python.","title":"Names to Avoid"},{"location":"python/variables/#reserved-words","text":"Summarized below are the reserved words in Python 3 . Python will raise an error if you try to assign a value to any of these keywords and so you must avoid these as variable names. False class finally is return None continue for lambda try True def from nonlocal while and del global not with as elif if or yield assert else import pass break except in raise","title":"Reserved Words"},{"location":"python/variables/#built-in-function-names","text":"There are several functions which are included in the standard Python library. Do not use the names of these functions as variable names otherwise the reference to the built-in function will be lost. For example, do not use sum , min , max , list or sorted as a variable name. See the full list of builtin functions .","title":"Built-in Function Names"},{"location":"python/variables/#jupyter-magic-whos","text":"The Jupyer magic command whos lists all variables in the current Jupyter notebook and their types: x = 2 pi = 3.14159 distance_km = 5799 miles_per_km = 0.6214 distance_miles = distance_km * miles_per_km whos Variable Type Data/Info ----------------------------------- distance_km int 5799 distance_miles float 3603.4986 miles_per_km float 0.6214 pi float 3.14159 x int 2","title":"Jupyter Magic: whos"},{"location":"python/variables/#exercises","text":"Exercise 1. True or False: Python does not allow assigning a value to variable name which is already a builtin Python function such as sum , min , max .","title":"Exercises"},{"location":"root-finding/bisection/","text":"Bisection Method The simplest root finding algorithm is the bisection method . The algorithm applies to any continuous function $f(x)$ on an interval $[a,b]$ where the value of the function $f(x)$ changes sign from $a$ to $b$. The idea is simple: divide the interval in two, a solution must exist within one subinterval, select the subinterval where the sign of $f(x)$ changes and repeat. Algorithm The bisection method procedure is: Choose a starting interval $[a_0,b_0]$ such that $f(a_0)f(b_0) < 0$. Compute $f(m_0)$ where $m_0 = (a_0+b_0)/2$ is the midpoint. Determine the next subinterval $[a_1,b_1]$: If $f(a_0)f(m_0) < 0$, then let $[a_1,b_1]$ be the next interval with $a_1=a_0$ and $b_1=m_0$. If $f(b_0)f(m_0) < 0$, then let $[a_1,b_1]$ be the next interval with $a_1=m_0$ and $b_1=b_0$. Repeat (2) and (3) until the interval $[a_N,b_N]$ reaches some predetermined length. Return the midpoint value $m_N=(a_N+b_N)/2$. A solution of the equation $f(x)=0$ in the interval $[a,b]$ is guaranteed by the Intermediate Value Theorem provided $f(x)$ is continuous on $[a,b]$ and $f(a)f(b) < 0$. In other words, the function changes sign over the interval and therefore must equal 0 at some point in the interval $[a,b]$. Absolute Error The bisection method does not (in general) produce an exact solution of an equation $f(x)=0$. However, we can give an estimate of the absolute error in the approxiation. Theorem . Let $f(x)$ be a continuous function on $[a,b]$ such that $f(a)f(b) < 0$. After $N$ iterations of the biection method, let $x_N$ be the midpoint in the $N$th subinterval $[a_N,b_N]$ $$ x_N = \\frac{a_N + b_N}{2} $$ There exists an exact solution $x_{\\mathrm{true}}$ of the equation $f(x)=0$ in the subinterval $[a_N,b_N]$ and the absolute error is $$ \\left| \\ x_{\\text{true}} - x_N \\, \\right| \\leq \\frac{b-a}{2^{N+1}} $$ Note that we can rearrange the error bound to see the minimum number of iterations required to guarantee absolute error less than a prescribed $\\epsilon$: \\begin{align} \\frac{b-a}{2^{N+1}} & < \\epsilon \\\\ \\frac{b-a}{\\epsilon} & < 2^{N+1} \\\\ \\ln \\left( \\frac{b-a}{\\epsilon} \\right) & < (N+1)\\ln(2) \\\\ \\frac{\\ln \\left( \\frac{b-a}{\\epsilon} \\right)}{\\ln(2)} - 1 & < N \\end{align} Implementation Write a function called bisection which takes 4 input parameters f , a , b and N and returns the approximation of a solution of $f(x)=0$ given by $N$ iterations of the bisection method. If $f(a_n)f(b_n) \\geq 0$ at any point in the iteration (caused either by a bad initial interval or rounding error in computations), then print \"Bisection method fails.\" and return None . def bisection(f,a,b,N): '''Approximate solution of f(x)=0 on interval [a,b] by bisection method. Parameters ---------- f : function The function for which we are trying to approximate a solution f(x)=0. a,b : numbers The interval in which to search for a solution. The function returns None if f(a)*f(b) >= 0 since a solution is not guaranteed. N : (positive) integer The number of iterations to implement. Returns ------- x_N : number The midpoint of the Nth interval computed by the bisection method. The initial interval [a_0,b_0] is given by [a,b]. If f(m_n) == 0 for some midpoint m_n = (a_n + b_n)/2, then the function returns this solution. If all signs of values f(a_n), f(b_n) and f(m_n) are the same at any iteration, the bisection method fails and return None. Examples -------- >>> f = lambda x: x**2 - x - 1 >>> bisection(f,1,2,25) 1.618033990263939 >>> f = lambda x: (2*x - 1)*(x - 3) >>> bisection(f,0,1,10) 0.5 ''' if f(a)*f(b) >= 0: print(\"Bisection method fails.\") return None a_n = a b_n = b for n in range(1,N+1): m_n = (a_n + b_n)/2 f_m_n = f(m_n) if f(a_n)*f_m_n < 0: a_n = a_n b_n = m_n elif f(b_n)*f_m_n < 0: a_n = m_n b_n = b_n elif f_m_n == 0: print(\"Found exact solution.\") return m_n else: print(\"Bisection method fails.\") return None return (a_n + b_n)/2 Examples Golden Ratio Let's use our function with input parameters $f(x)=x^2 - x - 1$ and $N=25$ iterations on $[1,2]$ to approximate the golden ratio $$ \\phi = \\frac{1 + \\sqrt{5}}{2} $$ The golden ratio $\\phi$ is a root of the quadratic polynomial $x^2 - x - 1 = 0$. f = lambda x: x**2 - x - 1 approx_phi = bisection(f,1,2,25) print(approx_phi) 1.618033990263939 The absolute error is guaranteed to be less than $(2 - 1)/(2^{26})$ which is: error_bound = 2**(-26) print(error_bound) 1.4901161193847656e-08 Let's verify the absolute error is then than this error bound: abs( (1 + 5**0.5)/2 - approx_phi) < error_bound True Exercises Under construction","title":"Bisection Method"},{"location":"root-finding/bisection/#bisection-method","text":"The simplest root finding algorithm is the bisection method . The algorithm applies to any continuous function $f(x)$ on an interval $[a,b]$ where the value of the function $f(x)$ changes sign from $a$ to $b$. The idea is simple: divide the interval in two, a solution must exist within one subinterval, select the subinterval where the sign of $f(x)$ changes and repeat.","title":"Bisection Method"},{"location":"root-finding/bisection/#algorithm","text":"The bisection method procedure is: Choose a starting interval $[a_0,b_0]$ such that $f(a_0)f(b_0) < 0$. Compute $f(m_0)$ where $m_0 = (a_0+b_0)/2$ is the midpoint. Determine the next subinterval $[a_1,b_1]$: If $f(a_0)f(m_0) < 0$, then let $[a_1,b_1]$ be the next interval with $a_1=a_0$ and $b_1=m_0$. If $f(b_0)f(m_0) < 0$, then let $[a_1,b_1]$ be the next interval with $a_1=m_0$ and $b_1=b_0$. Repeat (2) and (3) until the interval $[a_N,b_N]$ reaches some predetermined length. Return the midpoint value $m_N=(a_N+b_N)/2$. A solution of the equation $f(x)=0$ in the interval $[a,b]$ is guaranteed by the Intermediate Value Theorem provided $f(x)$ is continuous on $[a,b]$ and $f(a)f(b) < 0$. In other words, the function changes sign over the interval and therefore must equal 0 at some point in the interval $[a,b]$.","title":"Algorithm"},{"location":"root-finding/bisection/#absolute-error","text":"The bisection method does not (in general) produce an exact solution of an equation $f(x)=0$. However, we can give an estimate of the absolute error in the approxiation. Theorem . Let $f(x)$ be a continuous function on $[a,b]$ such that $f(a)f(b) < 0$. After $N$ iterations of the biection method, let $x_N$ be the midpoint in the $N$th subinterval $[a_N,b_N]$ $$ x_N = \\frac{a_N + b_N}{2} $$ There exists an exact solution $x_{\\mathrm{true}}$ of the equation $f(x)=0$ in the subinterval $[a_N,b_N]$ and the absolute error is $$ \\left| \\ x_{\\text{true}} - x_N \\, \\right| \\leq \\frac{b-a}{2^{N+1}} $$ Note that we can rearrange the error bound to see the minimum number of iterations required to guarantee absolute error less than a prescribed $\\epsilon$: \\begin{align} \\frac{b-a}{2^{N+1}} & < \\epsilon \\\\ \\frac{b-a}{\\epsilon} & < 2^{N+1} \\\\ \\ln \\left( \\frac{b-a}{\\epsilon} \\right) & < (N+1)\\ln(2) \\\\ \\frac{\\ln \\left( \\frac{b-a}{\\epsilon} \\right)}{\\ln(2)} - 1 & < N \\end{align}","title":"Absolute Error"},{"location":"root-finding/bisection/#implementation","text":"Write a function called bisection which takes 4 input parameters f , a , b and N and returns the approximation of a solution of $f(x)=0$ given by $N$ iterations of the bisection method. If $f(a_n)f(b_n) \\geq 0$ at any point in the iteration (caused either by a bad initial interval or rounding error in computations), then print \"Bisection method fails.\" and return None . def bisection(f,a,b,N): '''Approximate solution of f(x)=0 on interval [a,b] by bisection method. Parameters ---------- f : function The function for which we are trying to approximate a solution f(x)=0. a,b : numbers The interval in which to search for a solution. The function returns None if f(a)*f(b) >= 0 since a solution is not guaranteed. N : (positive) integer The number of iterations to implement. Returns ------- x_N : number The midpoint of the Nth interval computed by the bisection method. The initial interval [a_0,b_0] is given by [a,b]. If f(m_n) == 0 for some midpoint m_n = (a_n + b_n)/2, then the function returns this solution. If all signs of values f(a_n), f(b_n) and f(m_n) are the same at any iteration, the bisection method fails and return None. Examples -------- >>> f = lambda x: x**2 - x - 1 >>> bisection(f,1,2,25) 1.618033990263939 >>> f = lambda x: (2*x - 1)*(x - 3) >>> bisection(f,0,1,10) 0.5 ''' if f(a)*f(b) >= 0: print(\"Bisection method fails.\") return None a_n = a b_n = b for n in range(1,N+1): m_n = (a_n + b_n)/2 f_m_n = f(m_n) if f(a_n)*f_m_n < 0: a_n = a_n b_n = m_n elif f(b_n)*f_m_n < 0: a_n = m_n b_n = b_n elif f_m_n == 0: print(\"Found exact solution.\") return m_n else: print(\"Bisection method fails.\") return None return (a_n + b_n)/2","title":"Implementation"},{"location":"root-finding/bisection/#examples","text":"","title":"Examples"},{"location":"root-finding/bisection/#golden-ratio","text":"Let's use our function with input parameters $f(x)=x^2 - x - 1$ and $N=25$ iterations on $[1,2]$ to approximate the golden ratio $$ \\phi = \\frac{1 + \\sqrt{5}}{2} $$ The golden ratio $\\phi$ is a root of the quadratic polynomial $x^2 - x - 1 = 0$. f = lambda x: x**2 - x - 1 approx_phi = bisection(f,1,2,25) print(approx_phi) 1.618033990263939 The absolute error is guaranteed to be less than $(2 - 1)/(2^{26})$ which is: error_bound = 2**(-26) print(error_bound) 1.4901161193847656e-08 Let's verify the absolute error is then than this error bound: abs( (1 + 5**0.5)/2 - approx_phi) < error_bound True","title":"Golden Ratio"},{"location":"root-finding/bisection/#exercises","text":"Under construction","title":"Exercises"},{"location":"root-finding/newton/","text":"Newton's Method Newton's method is a root finding method that uses linear approximation. In particular, we guess a solution $x_0$ of the equation $f(x)=0$, compute the linear approximation of $f(x)$ at $x_0$ and then find the $x$-intercept of the linear approximation. Formula Let $f(x)$ be a differentiable function. If $x_0$ is near a solution of $f(x)=0$ then we can approximate $f(x)$ by the tangent line at $x_0$ and compute the $x$-intercept of the tangent line. The equation of the tangent line at $x_0$ is $$ y = f'(x_0)(x - x_0) + f(x_0) $$ The $x$-intercept is the solution $x_1$ of the equation $$ 0 = f'(x_0)(x_1 - x_0) + f(x_0) $$ and we solve for $x_1$ $$ x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)} $$ If we implement this procedure repeatedly, then we obtain a sequence given by the recursive formula $$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} $$ which (potentially) converges to a solution of the equation $f(x)=0$. Advantages/Disadvantages When it converges, Newton's method usually converges very quickly and this is its main advantage. However, Newton's method is not guaranteed to converge and this is obviously a big disadvantage especially compared to the bisection and secant methods which are guaranteed to converge to a solution (provided they start with an interval containing a root). Newton's method also requires computing values of the derivative of the function in question. This is potentially a disadvantage if the derivative is difficult to compute. The stopping criteria for Newton's method differs from the bisection and secant methods. In those methods, we know how close we are to a solution because we are computing intervals which contain a solution. In Newton's method, we don't know how close we are to a solution. All we can compute is the value $f(x)$ and so we implement a stopping criteria based on $f(x)$. Finally, there's no guarantee that the method converges to a solution and we should set a maximum number of iterations so that our implementation ends if we don't find a solution. Implementation Let's write a function called newton which takes 5 input parameters f , Df , x0 , epsilon and max_iter and returns an approximation of a solution of $f(x)=0$ by Newton's method. The function may terminate in 3 ways: If abs(f(xn)) < epsilon , the algorithm has found an approximate solution and returns xn . If f'(xn) == 0 , the algorithm stops and returns None . If the number of iterations exceed max_iter , the algorithm stops and returns None . def newton(f,Df,x0,epsilon,max_iter): '''Approximate solution of f(x)=0 by Newton's method. Parameters ---------- f : function Function for which we are searching for a solution f(x)=0. Df : function Derivative of f(x). x0 : number Initial guess for a solution f(x)=0. epsilon : number Stopping criteria is abs(f(x)) < epsilon. max_iter : integer Maximum number of iterations of Newton's method. Returns ------- xn : number Implement Newton's method: compute the linear approximation of f(x) at xn and find x intercept by the formula x = xn - f(xn)/Df(xn) Continue until abs(f(xn)) < epsilon and return xn. If Df(xn) == 0, return None. If the number of iterations exceeds max_iter, then return None. Examples -------- >>> f = lambda x: x**2 - x - 1 >>> Df = lambda x: 2*x - 1 >>> newton(f,Df,1,1e-8,10) Found solution after 5 iterations. 1.618033988749989 ''' xn = x0 for n in range(0,max_iter): fxn = f(xn) if abs(fxn) < epsilon: print('Found solution after',n,'iterations.') return xn Dfxn = Df(xn) if Dfxn == 0: print('Zero derivative. No solution found.') return None xn = xn - fxn/Dfxn print('Exceeded maximum iterations. No solution found.') return None Examples Supergolden Ratio Let's test our function newton on the polynomial $p(x) = x^3 - x^2 - 1$ to approximate the super golden ratio . p = lambda x: x**3 - x**2 - 1 Dp = lambda x: 3*x**2 - 2*x approx = newton(p,Dp,1,1e-10,10) print(approx) Found solution after 6 iterations. 1.4655712318767877 How many iterations of the bisection method starting with the interval $[1,2]$ can achieve the same accuracy? Divergent Example Newton's method diverges in certain cases. For example, if the tangent line at the root is vertical as in $f(x)=x^{1/3}$. Note that bisection and secant methods would converge in this case. f = lambda x: x**(1/3) Df = lambda x: (1/3)*x**(-2/3) approx = newton(f,Df,0.1,1e-2,100) Exceeded maximum iterations. No solution found. Exercises Exercise 1. Let $p(x) = x^3 - x - 1$. The only real root of $p(x)$ is called the plastic number and is given by $$ \\frac{\\sqrt[3]{108 + 12\\sqrt{69}} + \\sqrt[3]{108 - 12\\sqrt{69}}}{6} $$ Exercise 2. Choose $x_0 = 1$ and implement 2 iterations of Newton's method to approximate the plastic number. Exercise 3. Use the exact value above to compute the absolute error after 2 iterations of Newton's method. Exercise 4. Starting with the subinterval $[1,2]$, how many iterations of the bisection method is required to achieve the same accuracy?","title":"Newton's Method"},{"location":"root-finding/newton/#newtons-method","text":"Newton's method is a root finding method that uses linear approximation. In particular, we guess a solution $x_0$ of the equation $f(x)=0$, compute the linear approximation of $f(x)$ at $x_0$ and then find the $x$-intercept of the linear approximation.","title":"Newton's Method"},{"location":"root-finding/newton/#formula","text":"Let $f(x)$ be a differentiable function. If $x_0$ is near a solution of $f(x)=0$ then we can approximate $f(x)$ by the tangent line at $x_0$ and compute the $x$-intercept of the tangent line. The equation of the tangent line at $x_0$ is $$ y = f'(x_0)(x - x_0) + f(x_0) $$ The $x$-intercept is the solution $x_1$ of the equation $$ 0 = f'(x_0)(x_1 - x_0) + f(x_0) $$ and we solve for $x_1$ $$ x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)} $$ If we implement this procedure repeatedly, then we obtain a sequence given by the recursive formula $$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} $$ which (potentially) converges to a solution of the equation $f(x)=0$.","title":"Formula"},{"location":"root-finding/newton/#advantagesdisadvantages","text":"When it converges, Newton's method usually converges very quickly and this is its main advantage. However, Newton's method is not guaranteed to converge and this is obviously a big disadvantage especially compared to the bisection and secant methods which are guaranteed to converge to a solution (provided they start with an interval containing a root). Newton's method also requires computing values of the derivative of the function in question. This is potentially a disadvantage if the derivative is difficult to compute. The stopping criteria for Newton's method differs from the bisection and secant methods. In those methods, we know how close we are to a solution because we are computing intervals which contain a solution. In Newton's method, we don't know how close we are to a solution. All we can compute is the value $f(x)$ and so we implement a stopping criteria based on $f(x)$. Finally, there's no guarantee that the method converges to a solution and we should set a maximum number of iterations so that our implementation ends if we don't find a solution.","title":"Advantages/Disadvantages"},{"location":"root-finding/newton/#implementation","text":"Let's write a function called newton which takes 5 input parameters f , Df , x0 , epsilon and max_iter and returns an approximation of a solution of $f(x)=0$ by Newton's method. The function may terminate in 3 ways: If abs(f(xn)) < epsilon , the algorithm has found an approximate solution and returns xn . If f'(xn) == 0 , the algorithm stops and returns None . If the number of iterations exceed max_iter , the algorithm stops and returns None . def newton(f,Df,x0,epsilon,max_iter): '''Approximate solution of f(x)=0 by Newton's method. Parameters ---------- f : function Function for which we are searching for a solution f(x)=0. Df : function Derivative of f(x). x0 : number Initial guess for a solution f(x)=0. epsilon : number Stopping criteria is abs(f(x)) < epsilon. max_iter : integer Maximum number of iterations of Newton's method. Returns ------- xn : number Implement Newton's method: compute the linear approximation of f(x) at xn and find x intercept by the formula x = xn - f(xn)/Df(xn) Continue until abs(f(xn)) < epsilon and return xn. If Df(xn) == 0, return None. If the number of iterations exceeds max_iter, then return None. Examples -------- >>> f = lambda x: x**2 - x - 1 >>> Df = lambda x: 2*x - 1 >>> newton(f,Df,1,1e-8,10) Found solution after 5 iterations. 1.618033988749989 ''' xn = x0 for n in range(0,max_iter): fxn = f(xn) if abs(fxn) < epsilon: print('Found solution after',n,'iterations.') return xn Dfxn = Df(xn) if Dfxn == 0: print('Zero derivative. No solution found.') return None xn = xn - fxn/Dfxn print('Exceeded maximum iterations. No solution found.') return None","title":"Implementation"},{"location":"root-finding/newton/#examples","text":"","title":"Examples"},{"location":"root-finding/newton/#supergolden-ratio","text":"Let's test our function newton on the polynomial $p(x) = x^3 - x^2 - 1$ to approximate the super golden ratio . p = lambda x: x**3 - x**2 - 1 Dp = lambda x: 3*x**2 - 2*x approx = newton(p,Dp,1,1e-10,10) print(approx) Found solution after 6 iterations. 1.4655712318767877 How many iterations of the bisection method starting with the interval $[1,2]$ can achieve the same accuracy?","title":"Supergolden Ratio"},{"location":"root-finding/newton/#divergent-example","text":"Newton's method diverges in certain cases. For example, if the tangent line at the root is vertical as in $f(x)=x^{1/3}$. Note that bisection and secant methods would converge in this case. f = lambda x: x**(1/3) Df = lambda x: (1/3)*x**(-2/3) approx = newton(f,Df,0.1,1e-2,100) Exceeded maximum iterations. No solution found.","title":"Divergent Example"},{"location":"root-finding/newton/#exercises","text":"Exercise 1. Let $p(x) = x^3 - x - 1$. The only real root of $p(x)$ is called the plastic number and is given by $$ \\frac{\\sqrt[3]{108 + 12\\sqrt{69}} + \\sqrt[3]{108 - 12\\sqrt{69}}}{6} $$ Exercise 2. Choose $x_0 = 1$ and implement 2 iterations of Newton's method to approximate the plastic number. Exercise 3. Use the exact value above to compute the absolute error after 2 iterations of Newton's method. Exercise 4. Starting with the subinterval $[1,2]$, how many iterations of the bisection method is required to achieve the same accuracy?","title":"Exercises"},{"location":"root-finding/root-finding/","text":"Root Finding Root finding refers to the general problem of searching for a solution of an equation $F(x)=0$ for some function $F(x)$. This is a very general problem and it comes up a lot in mathematics! For example, if we want to optimize a function $f(x)$ then we need to find critical points and therefore solve the equation $f'(x)=0$. There are few examples where there exist exact methods for finding solutions. For example, the quadratic formula $$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ gives us an exact method for finding roots of the equation $$ ax^2 + bx + c = 0 $$ There is a general formula to solve a cubic equation and even a quartic (degree 4) equation (but the formula is too complicated to be useful). But there does not exist a formula for a quintic (degree 5) polynomial . And there are many more examples of equations with no known method to solve them exactly. What can we do? Use numerical methods to find approximate solutions.","title":"Root Finding"},{"location":"root-finding/root-finding/#root-finding","text":"Root finding refers to the general problem of searching for a solution of an equation $F(x)=0$ for some function $F(x)$. This is a very general problem and it comes up a lot in mathematics! For example, if we want to optimize a function $f(x)$ then we need to find critical points and therefore solve the equation $f'(x)=0$. There are few examples where there exist exact methods for finding solutions. For example, the quadratic formula $$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ gives us an exact method for finding roots of the equation $$ ax^2 + bx + c = 0 $$ There is a general formula to solve a cubic equation and even a quartic (degree 4) equation (but the formula is too complicated to be useful). But there does not exist a formula for a quintic (degree 5) polynomial . And there are many more examples of equations with no known method to solve them exactly. What can we do? Use numerical methods to find approximate solutions.","title":"Root Finding"},{"location":"root-finding/secant/","text":"Secant Method The secant method is very similar to the bisection method except instead of dividing each interval by choosing the midpoint the secant method divides each interval by the secant line connecting the endpoints. The secant method always converges to a root of $f(x)=0$ provided that $f(x)$ is continuous on $[a,b]$ and $f(a)f(b)<0$. Secant Line Formula Let $f(x)$ be a continuous function on a closed interval $[a,b]$ such that $f(a)f(b) < 0$. A solution of the equation $f(x) = 0$ for $x \\in [a,b]$ is guaranteed by the Intermediate Value Theorem . Consider the line connecting the endpoint values $(a,f(a))$ and $(b,f(b))$. The line connecting these two points is called the secant line and is given by the formula $$ y = \\frac{f(b) - f(a)}{b - a}(x - a) + f(a) $$ The point where the secant line crosses the $x$-axis is $$ 0 = \\frac{f(b) - f(a)}{b - a}(x - a) + f(a) $$ which we solve for $x$ $$ x = a - f(a)\\frac{b - a}{f(b) - f(a)} $$ Algorithm The secant method procedure is almost identical to the bisection method. The only difference it how we divide each subinterval. Choose a starting interval $[a_0,b_0]$ such that $f(a_0)f(b_0) < 0$. Compute $f(x_0)$ where $x_0$ is given by the secant line $$ x_0 = a_0 - f(a_0)\\frac{b_0 - a_0}{f(b_0) - f(a_0)} $$ Determine the next subinterval $[a_1,b_1]$: If $f(a_0)f(x_0) < 0$, then let $[a_1,b_1]$ be the next interval with $a_1=a_0$ and $b_1=x_0$. If $f(b_0)f(x_0) < 0$, then let $[a_1,b_1]$ be the next interval with $a_1=x_0$ and $b_1=b_0$. Repeat (2) and (3) until the interval $[a_N,b_N]$ reaches some predetermined length. Return the value $x_N$, the $x$-intercept of the $N$th subinterval. A solution of the equation $f(x)=0$ in the interval $[a,b]$ is guaranteed by the Intermediate Value Theorem provided $f(x)$ is continuous on $[a,b]$ and $f(a)f(b) < 0$. In other words, the function changes sign over the interval and therefore must equal 0 at some point in the interval $[a,b]$. Implementation Write a function called secant which takes 4 input parameters f , a , b and N and returns the approximation of a solution of $f(x)=0$ given by $N$ iterations of the secant method. If $f(a_n)f(b_n) \\geq 0$ at any point in the iteration (caused either by a bad initial interval or rounding error in computations), then print \"Secant method fails.\" and return None . def secant(f,a,b,N): '''Approximate solution of f(x)=0 on interval [a,b] by the secant method. Parameters ---------- f : function The function for which we are trying to approximate a solution f(x)=0. a,b : numbers The interval in which to search for a solution. The function returns None if f(a)*f(b) >= 0 since a solution is not guaranteed. N : (positive) integer The number of iterations to implement. Returns ------- m_N : number The x intercept of the secant line on the the Nth interval m_n = a_n - f(a_n)*(b_n - a_n)/(f(b_n) - f(a_n)) The initial interval [a_0,b_0] is given by [a,b]. If f(m_n) == 0 for some intercept m_n then the function returns this solution. If all signs of values f(a_n), f(b_n) and f(m_n) are the same at any iterations, the secant method fails and return None. Examples -------- >>> f = lambda x: x**2 - x - 1 >>> secant(f,1,2,5) 1.6180257510729614 ''' if f(a)*f(b) >= 0: print(\"Secant method fails.\") return None a_n = a b_n = b for n in range(1,N+1): m_n = a_n - f(a_n)*(b_n - a_n)/(f(b_n) - f(a_n)) f_m_n = f(m_n) if f(a_n)*f_m_n < 0: a_n = a_n b_n = m_n elif f(b_n)*f_m_n < 0: a_n = m_n b_n = b_n elif f_m_n == 0: print(\"Found exact solution.\") return m_n else: print(\"Secant method fails.\") return None return a_n - f(a_n)*(b_n - a_n)/(f(b_n) - f(a_n)) Examples Supergolden Ratio Let's test our function with input values for which we know the correct output. Let's find an approximation of the supergolden ratio : the only real root of the polynomial $p(x) = x^3 - x^2 - 1$. p = lambda x: x**3 - x**2 - 1 print(p(1)) print(p(2)) -1 3 Since the polynomial changes sign in the interval $[1,2]$, we can apply the secant method with this as the starting interval: approx = secant(p,1,2,20) print(approx) 1.4655712311394433 The exact value of the supergolden ratio is $$ \\frac{1 + \\sqrt[3]{\\frac{29 + 3\\sqrt{93}}{2}} + \\sqrt[3]{\\frac{29 - 3\\sqrt{93}}{2}}}{3} $$ supergolden = (1 + ((29 + 3*93**0.5)/2)**(1/3) + ((29 - 3*93**0.5)/2)**(1/3))/3 print(supergolden) 1.4655712318767682 Let's compare our approximation with the exact solution: error = abs(supergolden - approx) print(error) 7.373248678277378e-10 Exercises Under construction","title":"Secant Method"},{"location":"root-finding/secant/#secant-method","text":"The secant method is very similar to the bisection method except instead of dividing each interval by choosing the midpoint the secant method divides each interval by the secant line connecting the endpoints. The secant method always converges to a root of $f(x)=0$ provided that $f(x)$ is continuous on $[a,b]$ and $f(a)f(b)<0$.","title":"Secant Method"},{"location":"root-finding/secant/#secant-line-formula","text":"Let $f(x)$ be a continuous function on a closed interval $[a,b]$ such that $f(a)f(b) < 0$. A solution of the equation $f(x) = 0$ for $x \\in [a,b]$ is guaranteed by the Intermediate Value Theorem . Consider the line connecting the endpoint values $(a,f(a))$ and $(b,f(b))$. The line connecting these two points is called the secant line and is given by the formula $$ y = \\frac{f(b) - f(a)}{b - a}(x - a) + f(a) $$ The point where the secant line crosses the $x$-axis is $$ 0 = \\frac{f(b) - f(a)}{b - a}(x - a) + f(a) $$ which we solve for $x$ $$ x = a - f(a)\\frac{b - a}{f(b) - f(a)} $$","title":"Secant Line Formula"},{"location":"root-finding/secant/#algorithm","text":"The secant method procedure is almost identical to the bisection method. The only difference it how we divide each subinterval. Choose a starting interval $[a_0,b_0]$ such that $f(a_0)f(b_0) < 0$. Compute $f(x_0)$ where $x_0$ is given by the secant line $$ x_0 = a_0 - f(a_0)\\frac{b_0 - a_0}{f(b_0) - f(a_0)} $$ Determine the next subinterval $[a_1,b_1]$: If $f(a_0)f(x_0) < 0$, then let $[a_1,b_1]$ be the next interval with $a_1=a_0$ and $b_1=x_0$. If $f(b_0)f(x_0) < 0$, then let $[a_1,b_1]$ be the next interval with $a_1=x_0$ and $b_1=b_0$. Repeat (2) and (3) until the interval $[a_N,b_N]$ reaches some predetermined length. Return the value $x_N$, the $x$-intercept of the $N$th subinterval. A solution of the equation $f(x)=0$ in the interval $[a,b]$ is guaranteed by the Intermediate Value Theorem provided $f(x)$ is continuous on $[a,b]$ and $f(a)f(b) < 0$. In other words, the function changes sign over the interval and therefore must equal 0 at some point in the interval $[a,b]$.","title":"Algorithm"},{"location":"root-finding/secant/#implementation","text":"Write a function called secant which takes 4 input parameters f , a , b and N and returns the approximation of a solution of $f(x)=0$ given by $N$ iterations of the secant method. If $f(a_n)f(b_n) \\geq 0$ at any point in the iteration (caused either by a bad initial interval or rounding error in computations), then print \"Secant method fails.\" and return None . def secant(f,a,b,N): '''Approximate solution of f(x)=0 on interval [a,b] by the secant method. Parameters ---------- f : function The function for which we are trying to approximate a solution f(x)=0. a,b : numbers The interval in which to search for a solution. The function returns None if f(a)*f(b) >= 0 since a solution is not guaranteed. N : (positive) integer The number of iterations to implement. Returns ------- m_N : number The x intercept of the secant line on the the Nth interval m_n = a_n - f(a_n)*(b_n - a_n)/(f(b_n) - f(a_n)) The initial interval [a_0,b_0] is given by [a,b]. If f(m_n) == 0 for some intercept m_n then the function returns this solution. If all signs of values f(a_n), f(b_n) and f(m_n) are the same at any iterations, the secant method fails and return None. Examples -------- >>> f = lambda x: x**2 - x - 1 >>> secant(f,1,2,5) 1.6180257510729614 ''' if f(a)*f(b) >= 0: print(\"Secant method fails.\") return None a_n = a b_n = b for n in range(1,N+1): m_n = a_n - f(a_n)*(b_n - a_n)/(f(b_n) - f(a_n)) f_m_n = f(m_n) if f(a_n)*f_m_n < 0: a_n = a_n b_n = m_n elif f(b_n)*f_m_n < 0: a_n = m_n b_n = b_n elif f_m_n == 0: print(\"Found exact solution.\") return m_n else: print(\"Secant method fails.\") return None return a_n - f(a_n)*(b_n - a_n)/(f(b_n) - f(a_n))","title":"Implementation"},{"location":"root-finding/secant/#examples","text":"","title":"Examples"},{"location":"root-finding/secant/#supergolden-ratio","text":"Let's test our function with input values for which we know the correct output. Let's find an approximation of the supergolden ratio : the only real root of the polynomial $p(x) = x^3 - x^2 - 1$. p = lambda x: x**3 - x**2 - 1 print(p(1)) print(p(2)) -1 3 Since the polynomial changes sign in the interval $[1,2]$, we can apply the secant method with this as the starting interval: approx = secant(p,1,2,20) print(approx) 1.4655712311394433 The exact value of the supergolden ratio is $$ \\frac{1 + \\sqrt[3]{\\frac{29 + 3\\sqrt{93}}{2}} + \\sqrt[3]{\\frac{29 - 3\\sqrt{93}}{2}}}{3} $$ supergolden = (1 + ((29 + 3*93**0.5)/2)**(1/3) + ((29 - 3*93**0.5)/2)**(1/3))/3 print(supergolden) 1.4655712318767682 Let's compare our approximation with the exact solution: error = abs(supergolden - approx) print(error) 7.373248678277378e-10","title":"Supergolden Ratio"},{"location":"root-finding/secant/#exercises","text":"Under construction","title":"Exercises"},{"location":"scipy/matplotlib/","text":"Matplotlib Matplotlib is a Python package for 2D plotting and the matplotlib.pyplot sub-module contains many plotting functions to create various kinds of plots. Let's get started by importing matplotlib.pyplot . import numpy as np import matplotlib.pyplot as plt Basic Plotting Procedure The general procedure to create a 2D line plot is: Create a sequence of $x$ values. Create a sequence of $y$ values. Enter plt.plot(x,y,[fmt],**kwargs) where [fmt] is a (optional) format string and **kwargs are (optional) keyword arguments specifying line properties of the plot. Use pyplot functions to add features to the figure such as a title, legend, grid lines, etc. Enter plt.show() to display the resulting figure. Let's begin with a basic example with a few random points: x = [-5,-2,0,1,3] y = [2,-1,1,-4,3] plt.plot(x,y) plt.show() The main things to notice are: The sequences x and y define the coordinates of the points in the plot. The line in the plot is constructed by connecting the points by straight lines. The second observation implies that if we want to plot a smooth curve then we need to plot lots of points otherwise the plot will not be smooth. For example, we could try plotting the parabola $y = x^2$ for $x \\in [-2,2]$ using only 5 points: x = [-2,-1,0,1,2] y = [4,1,0,1,4] plt.plot(x,y) plt.show() This is too few points to plot a smooth curve such as $y = x^2$ and so we need more points! Let's try again using the NumPy function np.linspace to create 100 points! x = np.linspace(-2,2,100) y = x**2 plt.plot(x,y) plt.show() That's a better representation of the parabola $y = x^2$. Note that the number of points we use in a line plot (100 in this case) is completely arbitrary but the goal is to show a smooth graph for a smooth curve and so we just need to pick a big enough number depending on the function. But be careful not to generate too many points since a very large number of points will take a long time to plot! Now that we have the general idea, let's look at adding style and features to our plots! Line Properties A line appearing in a plot has several properties including color, transparency, style, width and markers. We can set these properties when we call plt.plot using the following keyword arguments: Property Description alpha transparency (0.0 transparent through 1.0 opaque) color (or c ) any matplotlib color label text appearing in legend linestyle (or ls ) solid , dashed , dashdot , dotted linewidth (or lw ) set width of the line marker set marker style markeredgecolor (or mec ) any matplotlib color markerfacecolor (or mfc ) any matplotlib color markersize (or ms ) size of the marker Note that we can specify a matplotlib color in several different ways including by name such as blue or red , or by a RGB tuple such as (1,0,1) for purple. For example, let's plot the function $$ y = e^{-x^2}\\cos(2 \\pi x) \\ \\ , \\ \\ x \\in [-2,2] $$ x = np.linspace(-2,2,41) y = np.exp(-x**2) * np.cos(2*np.pi*x) plt.plot(x,y,alpha=0.4,label='Decaying Cosine', color='red',linestyle='dashed',linewidth=2, marker='o',markersize=5,markerfacecolor='blue', markeredgecolor='blue') plt.ylim([-2,2]) plt.legend() plt.show() Notice that we used the pyplot function plt.legend to display the figure with a legend (showing the line label) and and plt.ylim to set the limits on the vertical axis to [-2,2] . Format Strings A format string gives us a shortcut to add color, markers and line style to a line plot. For example, if we want to plot the function $$ y = \\frac{1}{1 + x^2} \\ , \\ x \\in [-5,5] $$ with a dashed black line and square markers, we could use keyword arguments: x = np.linspace(-5,5,41) y = 1/(1 + x**2) plt.plot(x,y,color='black',linestyle='dashed',marker='s') plt.show() Or we could use the corresponding format string 'ks--' where k denotes a black line, s a square marker and -- a dashed line: x = np.linspace(-5,5,41) y = 1/(1 + x**2) plt.plot(x,y,'ks--') plt.show() Much easier! See below for a list of colors, markers and linestyles. Colors Character Color b blue g green r red c cyan m magenta y yellow k black w white Markers Character Marker . point o circle v triangle down ^ triangle up s square p pentagon * star + plus x x D diamond Line Styles Character Line Style - solid line style -- dashed line style -. dash-dot line style : dotted line style See the matplotlib.pyplot.plot documentation for more options. Pyplot Functions There are many pyplot functions available for us to customize our figures. For example: Fucntion Description plt.xlim set $x$ limits plt.ylim set $y$ limits plt.grid add grid lines plt.title add a title plt.xlabel add label to the horizontal axis plt.ylabel add label to the vertical axis plt.axis set axis properties ( equal , off , scaled , etc.) plt.xticks set tick locations on the horizontal axis plt.yticks set tick locations on the vertical axis plt.legend display legend for several lines in the same figure plt.savefig save figure (as .png, .pdf, etc.) to working directory plt.figure create a new figure and set its properties See the pyplot documentation for a full list of functions. Examples Taylor Polynomials Plot the function $y = \\cos(x)$ along with its Taylor polynomials of degrees 2 and 4. x = np.linspace(-6,6,50) # Plot y = cos(x) y = np.cos(x) plt.plot(x,y,'b',label='cos(x)') # Plot degree 2 Taylor polynomial y2 = 1 - x**2/2 plt.plot(x,y2,'r-.',label='Degree 2') # Plot degree 4 Taylor polynomial y4 = 1 - x**2/2 + x**4/24 plt.plot(x,y4,'g:',label='Degree 4') # Add features to our figure plt.legend() plt.grid(True,linestyle=':') plt.xlim([-6,6]) plt.ylim([-4,4]) plt.title('Taylor Polynomials of cos(x) at x=0') plt.xlabel('x') plt.ylabel('y') plt.show() Heart Curve Plot the heart curve: \\begin{align} x &= 16 \\sin^3(t) \\\\ y &= 13 \\cos(t) - 5 \\cos(2t) - 2 \\cos(3t) - \\cos(4t) \\end{align} for $t \\in [0,2\\pi]$. t = np.linspace(0,2*np.pi,100) x = 16*np.sin(t)**3 y = 13*np.cos(t) - 5*np.cos(2*t) - 2*np.cos(3*t) - np.cos(4*t) # Plot line with RGB tuple (red=1, green=0.2, blue=0.5) # and 20pt line width plt.plot(x,y,c=(1,0.2,0.5),lw=20) # Add features to our figure plt.title('Heart!') plt.axis('equal') plt.axis('off') plt.show() Subplots The plt.subplot function takes at least 3 inputs $n$, $m$ and $i$ and creates a figure with a $n$ by $m$ grid of subplots and then sets the $i$th subplot (counting across the rows) as the current plot (ie. current axes object). For example, consider the sawtooth wave $$ f(t) = \\frac{1}{2} - \\frac{1}{\\pi} \\sum_{k=1}^{\\infty} (-1)^k \\frac{\\sin(2 \\pi k t)}{k} $$ and let $f_N(t)$ denote the $N$th partial sum of the sawtooth wave: $$ f_N(t) = \\frac{1}{2} - \\frac{1}{\\pi} \\sum_{k=1}^{N} (-1)^k \\frac{\\sin(2 \\pi k t)}{k} $$ Create a 2 by 2 grid of subplots to plot the first 4 partial sums: \\begin{align} f_1(t) &= \\frac{1}{2} + \\frac{\\sin(2 \\pi t)}{\\pi} \\\\ f_2(t) &= \\frac{1}{2} + \\frac{\\sin(2 \\pi t)}{\\pi} - \\frac{\\sin(4 \\pi t)}{2\\pi} \\\\ f_3(t) &= \\frac{1}{2} + \\frac{\\sin(2 \\pi t)}{\\pi} - \\frac{\\sin(4 \\pi t)}{2\\pi} + \\frac{\\sin(6 \\pi t)}{3\\pi} \\\\ f_4(t) &= \\frac{1}{2} + \\frac{\\sin(2 \\pi t)}{\\pi} - \\frac{\\sin(4 \\pi t)}{2\\pi} + \\frac{\\sin(6 \\pi t)}{3\\pi} - \\frac{\\sin(8 \\pi t)}{4\\pi} \\end{align} t = np.linspace(0,4,200) fN = 1/2 for N in [1,2,3,4]: fN = fN - (-1)**N * np.sin(2*N*np.pi*t)/(N*np.pi) plt.subplot(2,2,N) plt.plot(t,fN) plt.title('N = {}'.format(N)) plt.tight_layout() plt.show() See the documentation for more about subplots. Beyond Line Plots Scatter plots A scatter plot has 4 dimensions: $x$ coordinate, $y$ coordinate, size and color. Let's make a random scatter plot: # Set the number of dots in the plot N = 2000 # Create random x and y coordinates sampled uniformly from [0,1] x = np.random.rand(N) y = np.random.rand(N) # Create random array sampled uniformly from [20,120] # `size` array is used below to set the size of each dot size = 100*np.random.rand(N) + 20 # Create random 4-tuples sampled uniformly from [0,1] # `colors` array is used below to set the color # (red,green,blue,alpha) of each dot colors = np.random.rand(N,4) # Create a figure of size 12 by 5 and create scatter plot plt.figure(figsize=(12,5)) plt.scatter(x,y,c=colors,s=size) plt.axis('off') plt.show() Histograms Generate an array of 10000 random numbers sampled from the normal distribution and create a histogram . Let's also superimpose the normal distribution: $$ y = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} $$ samples = np.random.randn(10000) plt.hist(samples,bins=20,density=True,alpha=0.5,color=(0.3,0.8,0.1)) plt.title('Random Samples - Normal Distribution') plt.ylabel('Frequency') x = np.linspace(-4,4,100) y = 1/(2*np.pi)**0.5 * np.exp(-x**2/2) plt.plot(x,y,'b',alpha=0.8) plt.show() Bar plots Plot the total precipitation in Vancouver by month as a bar plot : month = range(1,13) precipitation = [98.8,128.8,206.0,138.5,102.2,46.4,1.8,5.0,29.4,114.8,197.0,170.6] plt.bar(month,precipitation) plt.xticks(month) plt.yticks(range(0,300,50)) plt.grid(True,alpha=0.5,linestyle='--') plt.title('Precipitation in Vancouver, 2017') plt.ylabel('Total Precipitation (mm)') plt.xlabel('Month') plt.show() Figure and Axes Objects Under construction Exercises Exercise 1. Plot the following functions: $f(x) = \\sqrt{|x|}$, $x \\in [-9,9]$ $f(x) = \\sin(x) + \\sin(2x)$, $x \\in [0,4\\pi]$ $f(x) = \\arctan(x)$, $x \\in [-5,5]$ $f(x) = 2x\\ln|x|$, $x \\not=0$, $f(0)=0$, $x \\in [-1,1]$ $f(x) = (x+2)(x+1)(x-1)(x-2)(x - 3)$, $x \\in [-2,3]$ $f(x) = e^{-x^2}$, $x \\in [-2,2]$ Exercise 2. Plot the figure eight curve : \\begin{align} x &= \\sin(t) \\\\ y &= \\sin(t) \\cos(t) \\end{align} for $t \\in [0,2\\pi]$. Exercise 3. Plot the trefoil knot : \\begin{align} x &= \\sin t + 2 \\sin 2t \\\\ y &= \\cos t - 2 \\cos 2t \\end{align} for $t \\in [0,2\\pi]$. Exercise 4. Plot the butterfly curve : \\begin{align} x &= \\sin(t) \\left( e^{\\cos t } - 2 \\cos(4t) - \\sin^5(t/12) \\right) \\\\ y &= \\cos(t) \\left( e^{\\cos t } - 2 \\cos(4t) - \\sin^5(t/12) \\right) \\end{align} for $t \\in [0,100]$. Exercise 5. Write a function called parametric_plots which takes input parameters a and k and plots the parametric equation \\begin{align} x(t) &= 2 k \\cos(t) - a \\cos(k t) \\\\ y(t) &= 2 k \\sin(t) - a \\sin(k t) \\end{align} for $t \\in [0,2\\pi]$. Include a title for each subplot to display the values for $a$ and $k$, and use plt.axis('equal') to display the curve properly.","title":"Matplotlib"},{"location":"scipy/matplotlib/#matplotlib","text":"Matplotlib is a Python package for 2D plotting and the matplotlib.pyplot sub-module contains many plotting functions to create various kinds of plots. Let's get started by importing matplotlib.pyplot . import numpy as np import matplotlib.pyplot as plt","title":"Matplotlib"},{"location":"scipy/matplotlib/#basic-plotting","text":"","title":"Basic Plotting"},{"location":"scipy/matplotlib/#procedure","text":"The general procedure to create a 2D line plot is: Create a sequence of $x$ values. Create a sequence of $y$ values. Enter plt.plot(x,y,[fmt],**kwargs) where [fmt] is a (optional) format string and **kwargs are (optional) keyword arguments specifying line properties of the plot. Use pyplot functions to add features to the figure such as a title, legend, grid lines, etc. Enter plt.show() to display the resulting figure. Let's begin with a basic example with a few random points: x = [-5,-2,0,1,3] y = [2,-1,1,-4,3] plt.plot(x,y) plt.show() The main things to notice are: The sequences x and y define the coordinates of the points in the plot. The line in the plot is constructed by connecting the points by straight lines. The second observation implies that if we want to plot a smooth curve then we need to plot lots of points otherwise the plot will not be smooth. For example, we could try plotting the parabola $y = x^2$ for $x \\in [-2,2]$ using only 5 points: x = [-2,-1,0,1,2] y = [4,1,0,1,4] plt.plot(x,y) plt.show() This is too few points to plot a smooth curve such as $y = x^2$ and so we need more points! Let's try again using the NumPy function np.linspace to create 100 points! x = np.linspace(-2,2,100) y = x**2 plt.plot(x,y) plt.show() That's a better representation of the parabola $y = x^2$. Note that the number of points we use in a line plot (100 in this case) is completely arbitrary but the goal is to show a smooth graph for a smooth curve and so we just need to pick a big enough number depending on the function. But be careful not to generate too many points since a very large number of points will take a long time to plot! Now that we have the general idea, let's look at adding style and features to our plots!","title":"Procedure"},{"location":"scipy/matplotlib/#line-properties","text":"A line appearing in a plot has several properties including color, transparency, style, width and markers. We can set these properties when we call plt.plot using the following keyword arguments: Property Description alpha transparency (0.0 transparent through 1.0 opaque) color (or c ) any matplotlib color label text appearing in legend linestyle (or ls ) solid , dashed , dashdot , dotted linewidth (or lw ) set width of the line marker set marker style markeredgecolor (or mec ) any matplotlib color markerfacecolor (or mfc ) any matplotlib color markersize (or ms ) size of the marker Note that we can specify a matplotlib color in several different ways including by name such as blue or red , or by a RGB tuple such as (1,0,1) for purple. For example, let's plot the function $$ y = e^{-x^2}\\cos(2 \\pi x) \\ \\ , \\ \\ x \\in [-2,2] $$ x = np.linspace(-2,2,41) y = np.exp(-x**2) * np.cos(2*np.pi*x) plt.plot(x,y,alpha=0.4,label='Decaying Cosine', color='red',linestyle='dashed',linewidth=2, marker='o',markersize=5,markerfacecolor='blue', markeredgecolor='blue') plt.ylim([-2,2]) plt.legend() plt.show() Notice that we used the pyplot function plt.legend to display the figure with a legend (showing the line label) and and plt.ylim to set the limits on the vertical axis to [-2,2] .","title":"Line Properties"},{"location":"scipy/matplotlib/#format-strings","text":"A format string gives us a shortcut to add color, markers and line style to a line plot. For example, if we want to plot the function $$ y = \\frac{1}{1 + x^2} \\ , \\ x \\in [-5,5] $$ with a dashed black line and square markers, we could use keyword arguments: x = np.linspace(-5,5,41) y = 1/(1 + x**2) plt.plot(x,y,color='black',linestyle='dashed',marker='s') plt.show() Or we could use the corresponding format string 'ks--' where k denotes a black line, s a square marker and -- a dashed line: x = np.linspace(-5,5,41) y = 1/(1 + x**2) plt.plot(x,y,'ks--') plt.show() Much easier! See below for a list of colors, markers and linestyles.","title":"Format Strings"},{"location":"scipy/matplotlib/#pyplot-functions","text":"There are many pyplot functions available for us to customize our figures. For example: Fucntion Description plt.xlim set $x$ limits plt.ylim set $y$ limits plt.grid add grid lines plt.title add a title plt.xlabel add label to the horizontal axis plt.ylabel add label to the vertical axis plt.axis set axis properties ( equal , off , scaled , etc.) plt.xticks set tick locations on the horizontal axis plt.yticks set tick locations on the vertical axis plt.legend display legend for several lines in the same figure plt.savefig save figure (as .png, .pdf, etc.) to working directory plt.figure create a new figure and set its properties See the pyplot documentation for a full list of functions.","title":"Pyplot Functions"},{"location":"scipy/matplotlib/#examples","text":"","title":"Examples"},{"location":"scipy/matplotlib/#taylor-polynomials","text":"Plot the function $y = \\cos(x)$ along with its Taylor polynomials of degrees 2 and 4. x = np.linspace(-6,6,50) # Plot y = cos(x) y = np.cos(x) plt.plot(x,y,'b',label='cos(x)') # Plot degree 2 Taylor polynomial y2 = 1 - x**2/2 plt.plot(x,y2,'r-.',label='Degree 2') # Plot degree 4 Taylor polynomial y4 = 1 - x**2/2 + x**4/24 plt.plot(x,y4,'g:',label='Degree 4') # Add features to our figure plt.legend() plt.grid(True,linestyle=':') plt.xlim([-6,6]) plt.ylim([-4,4]) plt.title('Taylor Polynomials of cos(x) at x=0') plt.xlabel('x') plt.ylabel('y') plt.show()","title":"Taylor Polynomials"},{"location":"scipy/matplotlib/#heart-curve","text":"Plot the heart curve: \\begin{align} x &= 16 \\sin^3(t) \\\\ y &= 13 \\cos(t) - 5 \\cos(2t) - 2 \\cos(3t) - \\cos(4t) \\end{align} for $t \\in [0,2\\pi]$. t = np.linspace(0,2*np.pi,100) x = 16*np.sin(t)**3 y = 13*np.cos(t) - 5*np.cos(2*t) - 2*np.cos(3*t) - np.cos(4*t) # Plot line with RGB tuple (red=1, green=0.2, blue=0.5) # and 20pt line width plt.plot(x,y,c=(1,0.2,0.5),lw=20) # Add features to our figure plt.title('Heart!') plt.axis('equal') plt.axis('off') plt.show()","title":"Heart Curve"},{"location":"scipy/matplotlib/#subplots","text":"The plt.subplot function takes at least 3 inputs $n$, $m$ and $i$ and creates a figure with a $n$ by $m$ grid of subplots and then sets the $i$th subplot (counting across the rows) as the current plot (ie. current axes object). For example, consider the sawtooth wave $$ f(t) = \\frac{1}{2} - \\frac{1}{\\pi} \\sum_{k=1}^{\\infty} (-1)^k \\frac{\\sin(2 \\pi k t)}{k} $$ and let $f_N(t)$ denote the $N$th partial sum of the sawtooth wave: $$ f_N(t) = \\frac{1}{2} - \\frac{1}{\\pi} \\sum_{k=1}^{N} (-1)^k \\frac{\\sin(2 \\pi k t)}{k} $$ Create a 2 by 2 grid of subplots to plot the first 4 partial sums: \\begin{align} f_1(t) &= \\frac{1}{2} + \\frac{\\sin(2 \\pi t)}{\\pi} \\\\ f_2(t) &= \\frac{1}{2} + \\frac{\\sin(2 \\pi t)}{\\pi} - \\frac{\\sin(4 \\pi t)}{2\\pi} \\\\ f_3(t) &= \\frac{1}{2} + \\frac{\\sin(2 \\pi t)}{\\pi} - \\frac{\\sin(4 \\pi t)}{2\\pi} + \\frac{\\sin(6 \\pi t)}{3\\pi} \\\\ f_4(t) &= \\frac{1}{2} + \\frac{\\sin(2 \\pi t)}{\\pi} - \\frac{\\sin(4 \\pi t)}{2\\pi} + \\frac{\\sin(6 \\pi t)}{3\\pi} - \\frac{\\sin(8 \\pi t)}{4\\pi} \\end{align} t = np.linspace(0,4,200) fN = 1/2 for N in [1,2,3,4]: fN = fN - (-1)**N * np.sin(2*N*np.pi*t)/(N*np.pi) plt.subplot(2,2,N) plt.plot(t,fN) plt.title('N = {}'.format(N)) plt.tight_layout() plt.show() See the documentation for more about subplots.","title":"Subplots"},{"location":"scipy/matplotlib/#beyond-line-plots","text":"","title":"Beyond Line Plots"},{"location":"scipy/matplotlib/#scatter-plots","text":"A scatter plot has 4 dimensions: $x$ coordinate, $y$ coordinate, size and color. Let's make a random scatter plot: # Set the number of dots in the plot N = 2000 # Create random x and y coordinates sampled uniformly from [0,1] x = np.random.rand(N) y = np.random.rand(N) # Create random array sampled uniformly from [20,120] # `size` array is used below to set the size of each dot size = 100*np.random.rand(N) + 20 # Create random 4-tuples sampled uniformly from [0,1] # `colors` array is used below to set the color # (red,green,blue,alpha) of each dot colors = np.random.rand(N,4) # Create a figure of size 12 by 5 and create scatter plot plt.figure(figsize=(12,5)) plt.scatter(x,y,c=colors,s=size) plt.axis('off') plt.show()","title":"Scatter plots"},{"location":"scipy/matplotlib/#histograms","text":"Generate an array of 10000 random numbers sampled from the normal distribution and create a histogram . Let's also superimpose the normal distribution: $$ y = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} $$ samples = np.random.randn(10000) plt.hist(samples,bins=20,density=True,alpha=0.5,color=(0.3,0.8,0.1)) plt.title('Random Samples - Normal Distribution') plt.ylabel('Frequency') x = np.linspace(-4,4,100) y = 1/(2*np.pi)**0.5 * np.exp(-x**2/2) plt.plot(x,y,'b',alpha=0.8) plt.show()","title":"Histograms"},{"location":"scipy/matplotlib/#bar-plots","text":"Plot the total precipitation in Vancouver by month as a bar plot : month = range(1,13) precipitation = [98.8,128.8,206.0,138.5,102.2,46.4,1.8,5.0,29.4,114.8,197.0,170.6] plt.bar(month,precipitation) plt.xticks(month) plt.yticks(range(0,300,50)) plt.grid(True,alpha=0.5,linestyle='--') plt.title('Precipitation in Vancouver, 2017') plt.ylabel('Total Precipitation (mm)') plt.xlabel('Month') plt.show()","title":"Bar plots"},{"location":"scipy/matplotlib/#figure-and-axes-objects","text":"Under construction","title":"Figure and Axes Objects"},{"location":"scipy/matplotlib/#exercises","text":"Exercise 1. Plot the following functions: $f(x) = \\sqrt{|x|}$, $x \\in [-9,9]$ $f(x) = \\sin(x) + \\sin(2x)$, $x \\in [0,4\\pi]$ $f(x) = \\arctan(x)$, $x \\in [-5,5]$ $f(x) = 2x\\ln|x|$, $x \\not=0$, $f(0)=0$, $x \\in [-1,1]$ $f(x) = (x+2)(x+1)(x-1)(x-2)(x - 3)$, $x \\in [-2,3]$ $f(x) = e^{-x^2}$, $x \\in [-2,2]$ Exercise 2. Plot the figure eight curve : \\begin{align} x &= \\sin(t) \\\\ y &= \\sin(t) \\cos(t) \\end{align} for $t \\in [0,2\\pi]$. Exercise 3. Plot the trefoil knot : \\begin{align} x &= \\sin t + 2 \\sin 2t \\\\ y &= \\cos t - 2 \\cos 2t \\end{align} for $t \\in [0,2\\pi]$. Exercise 4. Plot the butterfly curve : \\begin{align} x &= \\sin(t) \\left( e^{\\cos t } - 2 \\cos(4t) - \\sin^5(t/12) \\right) \\\\ y &= \\cos(t) \\left( e^{\\cos t } - 2 \\cos(4t) - \\sin^5(t/12) \\right) \\end{align} for $t \\in [0,100]$. Exercise 5. Write a function called parametric_plots which takes input parameters a and k and plots the parametric equation \\begin{align} x(t) &= 2 k \\cos(t) - a \\cos(k t) \\\\ y(t) &= 2 k \\sin(t) - a \\sin(k t) \\end{align} for $t \\in [0,2\\pi]$. Include a title for each subplot to display the values for $a$ and $k$, and use plt.axis('equal') to display the curve properly.","title":"Exercises"},{"location":"scipy/numpy/","text":"NumPy NumPy is the core Python package for numerical computing. The main features of NumPy are: $N$-dimensional array object ndarray Vectorized operations and functions which broadcast across arrays for fast computation To get started with NumPy, let's adopt the standard convention and import it using the name np : import numpy as np NumPy Arrays The fundamental object provided by the NumPy package is the ndarray . We can think of a 1D (1-dimensional) ndarray as a list, a 2D (2-dimensional) ndarray as a matrix, a 3D (3-dimensional) ndarray as a 3-tensor (or a \"cube\" of numbers), and so on. See the NumPy tutorial for more about NumPy arrays. Creating Arrays The function numpy.array creates a NumPy array from a Python sequence such as a list, a tuple or a list of lists. For example, create a 1D NumPy array from a Python list: a = np.array([1,2,3,4,5]) print(a) [1 2 3 4 5] Notice that when we print a NumPy array it looks a lot like a Python list except that the entries are separated by spaces whereas entries in a Python list are separated by commas: print([1,2,3,4,5]) [1, 2, 3, 4, 5] Notice also that a NumPy array is displayed slightly differently when output by a cell (as opposed to being explicitly printed to output by the print function): a array([1, 2, 3, 4, 5]) Use the built-in function type to verify the type: type(a) numpy.ndarray Create a 2D NumPy array from a Python list of lists: M = np.array([[1,2,3],[4,5,6]]) print(M) [[1 2 3] [4 5 6]] type(M) numpy.ndarray Create an $n$-dimensional NumPy array from nested Python lists. For example, the following is a 3D NumPy array: N = np.array([ [[1,2],[3,4]] , [[5,6],[7,8]] , [[9,10],[11,12]] ]) print(N) [[[ 1 2] [ 3 4]] [[ 5 6] [ 7 8]] [[ 9 10] [11 12]]] There are several NumPy functions for creating arrays : Function Description numpy.array(a) Create $n$-dimensional NumPy array from sequence a numpy.linspace(a,b,N) Create 1D NumPy array with N equally spaced values from a to b (inclusively) numpy.arange(a,b,step) Create 1D NumPy array with values from a to b (exclusively) incremented by step numpy.zeros(N) Create 1D NumPy array of zeros of length $N$ numpy.zeros((n,m)) Create 2D NumPy array of zeros with $n$ rows and $m$ columns numpy.ones(N) Create 1D NumPy array of ones of length $N$ numpy.ones((n,m)) Create 2D NumPy array of ones with $n$ rows and $m$ columns numpy.eye(N) Create 2D NumPy array with $N$ rows and $N$ columns with ones on the diagonal (ie. the identity matrix of size $N$) Create a 1D NumPy array with 11 equally spaced values from 0 to 1: x = np.linspace(0,1,11) print(x) [0. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ] Create a 1D NumPy array with values from 0 to 20 (exclusively) incremented by 2.5: y = np.arange(0,20,2.5) print(y) [ 0. 2.5 5. 7.5 10. 12.5 15. 17.5] These are the functions that we'll use most often when creating NumPy arrays. The function numpy.linspace works best when we know the number of points we want in the array, and numpy.arange works best when we know step size between values in the array. Create a 1D NumPy array of zeros of length 5: z = np.zeros(5) print(z) [0. 0. 0. 0. 0.] Create a 2D NumPy array of zeros with 2 rows and 5 columns: M = np.zeros((2,5)) print(M) [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]] Create a 1D NumPy array of ones of length 7: w = np.ones(7) print(w) [1. 1. 1. 1. 1. 1. 1.] Create a 2D NumPy array of ones with 3 rows and 2 columns: N = np.ones((3,2)) print(N) [[1. 1.] [1. 1.] [1. 1.]] Create the identity matrix of size 10: I = np.eye(10) print(I) [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] Array Datatype NumPy arrays are homogeneous : all entries in the array are the same datatype. We will only work with numeric arrays and our arrays will contain either integers, floats, complex numbers or booleans. There are different kinds of datatypes provided by NumPy for different applications but we'll mostly be working with the default integer type numpy.int64 and the default float type numpy.float64 . These are very similar to the built-in Python datatypes int and float but with some differences that we won't go into. Check out the NumPy documentation on numeric datatypes for more information. The most important point for now is to know how to determine if a NumPy array contains integers elements or float elements. We can access the datatype of a NumPy array by its .dtype attribute. For example, create a 2D NumPy array from a list of lists of integers: A = np.array([[1,2,3],[4,5,6]]) print(A) [[1 2 3] [4 5 6]] We expect the datatype of A to be integers and we verify: A.dtype dtype('int64') Most of the other NumPy functions which create arrays use the numpy.float64 datatype by default. For example, using numpy.linspace : u = np.linspace(0,1,5) print(u) [0. 0.25 0.5 0.75 1. ] u.dtype dtype('float64') Notice that numbers are printed with a decimal point when the datatype of the NumPy array is any kind of float. Dimension, Shape and Size We can think of a 1D NumPy array as a list of numbers, a 2D NumPy array as a matrix, a 3D NumPy array as a cube of numbers, and so on. Given a NumPy array, we can find out how many dimensions it has by accessing its .ndim attribute. The result is a number telling us how many dimensions it has. For example, create a 2D NumPy array: A = np.array([[1,2],[3,4],[5,6]]) print(A) [[1 2] [3 4] [5 6]] A.ndim 2 The result tells us that A has 2 dimensions. The first dimension corresponds to the vertical direction counting the rows and the second dimension corresponds to the horizontal direction counting the columns. We can find out how many rows and columns A has by accessing its .shape attribute: A.shape (3, 2) The result is a tuple (3,2) of length 2 which means that A is a 2D array with 3 rows and 2 columns. We can also find out how many entries A has in total by accessing its .size attribute: A.size 6 This is the expected result since we know that A has 3 rows and 2 columns and therefore 2(3) = 6 total entries. Create a 1D NumPy array and inspect its dimension, shape and size: r = np.array([9,3,1,7]) print(r) [9 3 1 7] r.ndim 1 r.shape (4,) r.size 4 The variable r is assigned to a 1D NumPy array of length 4. Notice that r.shape is a tuple with a single entry (4,) . Slicing and Indexing Accessing the entries in an array is called indexing and accessing rows and columns (or subarrays) is called slicing . See the NumPy documentation for more information about indexing and slicing . Create a 1D NumPy array: v = np.linspace(0,5,11) print(v) [0. 0.5 1. 1.5 2. 2.5 3. 3.5 4. 4.5 5. ] Access the entries in a 1D array using the square brackets notation just like a Python list. For example, access the entry at index 3: v[3] 1.5 Notice that NumPy array indices start at 0 just like Python sequences. Create a 2D array of integers: B = np.array([[6, 5, 3, 1, 1],[1, 0, 4, 0, 1],[5, 9, 2, 2, 9]]) print(B) [[6 5 3 1 1] [1 0 4 0 1] [5 9 2 2 9]] Access the entries in a 2D array using the square brackets with 2 indices. In particular, access the entry at row index 1 and column index 2: B[1,2] 4 Access the top left entry in the array: B[0,0] 6 Negative indices work for NumPy arrays as they do for Python sequences. Access the bottom right entry in the array: B[-1,-1] 9 Access the row at index 2 using the colon : syntax: B[2,:] array([5, 9, 2, 2, 9]) Access the column at index 3: B[:,3] array([1, 0, 2]) Select the subarray of rows at index 1 and 2, and columns at index 2, 3 and 4: subB = B[1:3,2:5] print(subB) [[4 0 1] [2 2 9]] Slices of NumPy arrays are again NumPy arrays but possibly of a different dimension: subB.ndim 2 subB.shape (2, 3) type(subB) numpy.ndarray The variable subB is assigned to a 2D NumPy array of shape 2 by 2. Let's do the same for the column at index 2: colB = B[:,2] print(colB) [3 4 2] colB.ndim 1 colB.shape (3,) type(colB) numpy.ndarray The variable colB is assigned to a 1D NumPy array of length 3. Stacking We can build bigger arrays out of smaller arrays by stacking along different dimensions using the functions numpy.hstack and numpy.vstack . Stack 3 different 1D NumPy arrays of length 3 vertically forming a 3 by 3 matrix: x = np.array([1,1,1]) y = np.array([2,2,2]) z = np.array([3,3,3]) vstacked = np.vstack((x,y,z)) print(vstacked) [[1 1 1] [2 2 2] [3 3 3]] Stack 1D NumPy arrays horizontally to create another 1D array: hstacked = np.hstack((x,y,z)) print(hstacked) [1 1 1 2 2 2 3 3 3] Use numpy.hstack and numpy.vstack to build the matrix $T$ where $$ T = \\begin{bmatrix} 1 & 1 & 2 & 2 \\ 1 & 1 & 2 & 2 \\ 3 & 3 & 4 & 4 \\ 3 & 3 & 4 & 4 \\end{bmatrix} $$ A = np.ones((2,2)) B = 2*np.ones((2,2)) C = 3*np.ones((2,2)) D = 4*np.ones((2,2)) A_B = np.hstack((A,B)) print(A_B) [[1. 1. 2. 2.] [1. 1. 2. 2.]] C_D = np.hstack((C,D)) print(C_D) [[3. 3. 4. 4.] [3. 3. 4. 4.]] T = np.vstack((A_B,C_D)) print(T) [[1. 1. 2. 2.] [1. 1. 2. 2.] [3. 3. 4. 4.] [3. 3. 4. 4.]] Copies versus Views Under construction Operations and Functions Array Operations Arithmetic operators including addition + , subtraction - , multiplication * , division / and exponentiation ** are applied to arrays elementwise . For addition and substraction, these are the familiar vector operations we see in linear algebra: v = np.array([1,2,3]) w = np.array([1,0,-1]) v + w array([2, 2, 2]) v - w array([0, 2, 4]) In the same way, array multiplication and division are performed element by element: v * w array([ 1, 0, -3]) w / v array([ 1. , 0. , -0.33333333]) Notice that the datatype of both v and w is numpy.int64 however division w / v returns an array with datatype numpy.float64 . The exponent operator ** also acts element by element in the array: v ** 2 array([1, 4, 9]) Let's see these operations for 2D arrays: A = np.array([[3,1],[2,-1]]) B = np.array([[2,-2],[5,1]]) A + B array([[ 5, -1], [ 7, 0]]) A - B array([[ 1, 3], [-3, -2]]) A / B array([[ 1.5, -0.5], [ 0.4, -1. ]]) A * B array([[ 6, -2], [10, -1]]) A ** 2 array([[9, 1], [4, 1]]) Notice that array multiplication and exponentiation are performed elementwise. In Python 3.5+, the symbol @ computes matrix multiplication for NumPy arrays: A @ B array([[11, -5], [-1, -5]]) Matrix powers are performed by the function numpy.linalg.matrix_power . It's a long function name and so it's convenient to import it with a shorter name: from numpy.linalg import matrix_power as mpow Compute $A^3$: mpow(A,3) array([[37, 9], [18, 1]]) Equivalently, use the @ operator to compute $A^3$: A @ A @ A array([[37, 9], [18, 1]]) Broadcasting We know from linear algebra that we can only add matrices of the same size. Braodcasting is a set of NumPy rules which relaxes this constraint and allows us to combine a smaller array with a bigger when it makes sense. For example, suppose we want to create a 1D NumPy array of $y$ values for $x=0.0,0.25,0.5,0.75,1.0$ for the function $y = x^2 + 1$. From what we've seen so far, it makes sense to create x , then x**2 and then add an array of ones [1. 1. 1. 1. 1.] : x = np.array([0,0.25,0.5,0.75,1.0]) y = x**2 + np.array([1,1,1,1,1]) print(y) [1. 1.0625 1.25 1.5625 2. ] An example of broadcasting in NumPy is the following equivalent operation: x = np.array([0,0.25,0.5,0.75,1.0]) y = x**2 + 1 print(y) [1. 1.0625 1.25 1.5625 2. ] The number 1 is a scalar and we are adding it to a 1D NumPy array of length 5. The broadcasting rule in this case is to broadcast the scalar value 1 across the larger array. The result is a simpler syntax for a very comman operation. Let's try another example. What happens when we try to add a 1D NumPy array of length 4 to a 2D NumPy array of size 3 by 4? u = np.array([1,2,3,4]) print(u) [1 2 3 4] A = np.array([[1,1,1,1],[2,2,2,2],[3,3,3,3]]) print(A) [[1 1 1 1] [2 2 2 2] [3 3 3 3]] result = A + u print(result) [[2 3 4 5] [3 4 5 6] [4 5 6 7]] The 1D NumPy array is broadcast across the 2D array because the length of the first dimension in each array are equal! Array Functions There are many array functions we can use to compute with NumPy arrays. The following is a partial list and we'll look closer at mathematical functions in the next section. numpy.sum numpy.prod numpy.mean numpy.max numpy.min numpy.std numpy.argmax numpy.argmin numpy.var Create a 1D NumPy array with random values and compute: arr = np.array([8,-2,4,7,-3]) print(arr) [ 8 -2 4 7 -3] Compute the mean of the values in the array: np.mean(arr) 2.8 Verify the mean once more: m = np.sum(arr) / arr.size print(m) 2.8 Find the index of the maximum element in the array: max_i = np.argmax(arr) print(max_i) 0 Verify the maximum value in the array: np.max(arr) 8 arr[max_i] 8 Array functions apply to 2D arrays as well (and $N$-dimensional arrays in general) with the added feature that we can choose to apply array functions to the entire array, down the columns or across the rows (or any axis). Create a 2D NumPy array with random values and compute the sum of all the entries: M = np.array([[2,4,2],[2,1,1],[3,2,0],[0,6,2]]) print(M) [[2 4 2] [2 1 1] [3 2 0] [0 6 2]] np.sum(M) 25 The function numpy.sum also takes a keyword argument axis which determines along which dimension to compute the sum: np.sum(M,axis=0) # Sum of the columns array([ 7, 13, 5]) np.sum(M,axis=1) # Sum of the rows array([8, 4, 5, 8]) Mathematical Functions Mathematical functions in NumPy are called universal functions and are vectorized . Vectorized functions operate elementwise on arrays producing arrays as output and are built to compute values across arrays very quickly. The following is a partial list of mathematical functions: numpy.sin numpy.cos numpy.tan numpy.exp numpy.log numpy.log10 numpy.arcsin numpy.arccos numpy.arctan Compute the values $\\sin(2 \\pi x)$ for $x = 0,0.25,0.5\\dots,1.75$: x = np.arange(0,1.25,0.25) print(x) [0. 0.25 0.5 0.75 1. ] np.sin(2*np.pi*x) array([ 0.0000000e+00, 1.0000000e+00, 1.2246468e-16, -1.0000000e+00, -2.4492936e-16]) We expect the array [0. 1. 0. -1. 0.] however there is (as always with floating point numbers) some rounding errors in the result. In numerical computing, we can interpret a number such as $10^{-16}$ as $0$. Compute the values $\\log_{10}(x)$ for $x = 1,10,100,1000,10000$: x = np.array([1,10,100,1000,10000]) print(x) [ 1 10 100 1000 10000] np.log10(x) array([0., 1., 2., 3., 4.]) Note that we can also evaluate mathematical functions with scalar values: np.sin(0) 0.0 NumPy also provides familiar mathematical constants such as $\\pi$ and $e$: np.pi 3.141592653589793 np.e 2.718281828459045 For example, verify the limit $$ \\lim_{x \\to \\infty} \\arctan(x) = \\frac{\\pi}{2} $$ by evaluating $\\arctan(x)$ for some (arbitrary) large value $x$: np.arctan(10000) 1.5706963267952299 np.pi/2 1.5707963267948966 Random Number Generators The subpackage numpy.random contains functions to generate NumPy arrays of random numbers sampled from different distributions. The following is a partial list of distributions: Function Description numpy.random.rand(d1,...,dn) Create a NumPy array (with shape (d1,...,dn) ) with entries sampled uniformly from [0,1) numpy.random.randn(d1,...,dn) Create a NumPy array (with shape (d1,...,dn) ) with entries sampled from the standard normal distribution numpy.random.randint(a,b,size) Create a NumPy array (with shape size ) with integer entries from low (inclusive) to high (exclusive) Sample a random number from the uniform distribution : np.random.rand() 0.6695906195141056 Sample 3 random numbers: np.random.rand(3) array([0.40770395, 0.12158461, 0.72083088]) Create 2D NumPy array of random samples: np.random.rand(2,4) array([[0.61244625, 0.32645792, 0.55859886, 0.97613741], [0.57227614, 0.14315638, 0.49034299, 0.00099473]]) Random samples from the standard normal distribution : np.random.randn() 1.4440026351051256 np.random.randn(3) array([ 0.6172098 , -1.67631666, -2.20365265]) np.random.randn(3,1) array([[ 0.29643549], [-0.44039303], [-1.52246126]]) Random integers sampled uniformly from various intervals: np.random.randint(-10,10) 2 np.random.randint(0,2,(4,8)) array([[1, 0, 0, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 1, 1, 1, 1]]) np.random.randint(-9,10,(5,2)) array([[ 3, 9], [ 5, -6], [ 7, -7], [ 0, -5], [ 0, 0]]) Examples Brute Force Optimization Find the absolute maximum and minimum values of the function $$ f(x) = x\\sin(x)+\\cos(4x) $$ on the interval $[0,2\\pi]$. We know that the maximum and minimum values must occur at either the endpoints $x=0,2\\pi$ or at critical points where $f'(x)=0$. However, the derivative is given by $$ f'(x) = \\sin(x) + x \\cos(x) - 4\\sin(4x) $$ and the equation $f'(x) = 0$ is impossible to solve explicitly. Instead, create a 1D NumPy array of $x$ values from $0$ to $2\\pi$ of length $N$ (for some arbitrarily large value $N$) and use the functions numpy.min and numpy.max to find maximum and minimum $y$ values, and the functions numpy.argmin and numpy.argmax to find the indices of the corresponding $x$ values. N = 10000 x = np.linspace(0,2*np.pi,N) y = x * np.sin(x) + np.cos(4*x) y_max = np.max(y) y_min = np.min(y) x_max = x[np.argmax(y)] x_min = x[np.argmin(y)] print('Absolute maximum value is y =',y_max,'at x =',x_max) print('Absolute minimum value is y =',y_min,'at x =',x_min) Absolute maximum value is y = 2.5992726072887007 at x = 1.628136126702901 Absolute minimum value is y = -5.129752039182 at x = 5.34187001663503 Riemann Sums Write a function called exp_int which takes input parameters $b$ and $N$ and returns the (left) Riemann sum $$ \\int_0^b e^{-x^2} dx \\approx \\sum_{k=0}^{N-1} e^{-x_k^2} \\Delta x $$ for $\\Delta x = b/N$ and the partition $x_k=k \\, \\Delta x$, $k=0,\\dots,N$. def exp_int(b,N): \"Compute left Riemann sum of exp(-x^2) from 0 to b with N subintervals.\" x = np.linspace(0,b,N+1) x_left_endpoints = x[:-1] Delta_x = b/N I = Delta_x * np.sum(np.exp(-x_left_endpoints**2)) return I The infinite integral satisfies the beautiful identity $$ \\int_0^{\\infty} e^{-x^2} dx = \\frac{\\sqrt{\\pi}}{2} $$ Compute the integral with large values of $b$ and $N$: exp_int(100,100000) 0.886726925452758 Compare to the true value: np.pi**0.5/2 0.8862269254527579 Infinite Products The cosine function has the following infinite product representation $$ \\cos x = \\prod_{k = 1}^{\\infty} \\left(1 - \\frac{4 x^2}{\\pi^2 (2k - 1)^2} \\right) $$ Write a function called cos_product which takes input parameters $x$ and $N$ and returns the $N$th partial product $$ \\prod_{k = 1}^{N} \\left(1 - \\frac{4 x^2}{\\pi^2 (2k - 1)^2} \\right) $$ def cos_product(x,N): \"Compute the product \\prod_{k=1}^N (1 - 4x^2/(pi^2 (2k - 1)^2).\" k = np.arange(1,N+1) terms = 1 - 4*x**2 / (np.pi**2 * (2*k - 1)**2) return np.prod(terms) Verify our function using values for which we know the result. For example, $\\cos(0)=1$, $\\cos(\\pi)=-1$ and $\\cos(\\pi/4) = \\frac{1}{\\sqrt{2}}$. cos_product(0,10) 1.0 cos_product(np.pi,10000) -1.0001000050002433 cos_product(np.pi/4,10000000) 0.7071067856245614 1/2**0.5 0.7071067811865475 Matrix Multiplication Under construction Exercises Exercise 1. The natural log satisfies the following definite integral $$ \\int_1^e \\frac{\\ln x \\ dx}{(1 + \\ln x)^2} = \\frac{e}{2} - 1 $$ Write a function called log_integral which takes input parameters $c$ and $N$ and returns the value of the (right) Riemann sum $$ \\int_1^c \\frac{\\ln x \\ dx}{(1 + \\ln x)^2} \\approx \\sum_{k=1}^N \\frac{\\ln x_k \\ \\Delta x}{(1 + \\ln x_k)^2} \\ , \\ \\ \\Delta x = \\frac{c - 1}{N} $$ for the partition $x_k = 1 + k \\Delta x$, for $k = 0, \\dots , N$. Exercise 2. Write a function called k_sum which takes input parameters k and N and returns the partial sum $$ \\sum_{n=1}^{N} \\frac{1}{n^k} $$ Verify your function by comparing to the infinite series identity $$ \\sum_{n=1}^{\\infty} \\frac{(-1)^{n+1}}{n^2} = \\frac{\\pi^2}{12} $$ Exercise 3. Write a function called dot which takes 3 inputs M , i and j where M is a square NumPy array and the function returns the dot product of the $i$th row and the $j$th column of $M$.","title":"NumPy"},{"location":"scipy/numpy/#numpy","text":"NumPy is the core Python package for numerical computing. The main features of NumPy are: $N$-dimensional array object ndarray Vectorized operations and functions which broadcast across arrays for fast computation To get started with NumPy, let's adopt the standard convention and import it using the name np : import numpy as np","title":"NumPy"},{"location":"scipy/numpy/#numpy-arrays","text":"The fundamental object provided by the NumPy package is the ndarray . We can think of a 1D (1-dimensional) ndarray as a list, a 2D (2-dimensional) ndarray as a matrix, a 3D (3-dimensional) ndarray as a 3-tensor (or a \"cube\" of numbers), and so on. See the NumPy tutorial for more about NumPy arrays.","title":"NumPy Arrays"},{"location":"scipy/numpy/#creating-arrays","text":"The function numpy.array creates a NumPy array from a Python sequence such as a list, a tuple or a list of lists. For example, create a 1D NumPy array from a Python list: a = np.array([1,2,3,4,5]) print(a) [1 2 3 4 5] Notice that when we print a NumPy array it looks a lot like a Python list except that the entries are separated by spaces whereas entries in a Python list are separated by commas: print([1,2,3,4,5]) [1, 2, 3, 4, 5] Notice also that a NumPy array is displayed slightly differently when output by a cell (as opposed to being explicitly printed to output by the print function): a array([1, 2, 3, 4, 5]) Use the built-in function type to verify the type: type(a) numpy.ndarray Create a 2D NumPy array from a Python list of lists: M = np.array([[1,2,3],[4,5,6]]) print(M) [[1 2 3] [4 5 6]] type(M) numpy.ndarray Create an $n$-dimensional NumPy array from nested Python lists. For example, the following is a 3D NumPy array: N = np.array([ [[1,2],[3,4]] , [[5,6],[7,8]] , [[9,10],[11,12]] ]) print(N) [[[ 1 2] [ 3 4]] [[ 5 6] [ 7 8]] [[ 9 10] [11 12]]] There are several NumPy functions for creating arrays : Function Description numpy.array(a) Create $n$-dimensional NumPy array from sequence a numpy.linspace(a,b,N) Create 1D NumPy array with N equally spaced values from a to b (inclusively) numpy.arange(a,b,step) Create 1D NumPy array with values from a to b (exclusively) incremented by step numpy.zeros(N) Create 1D NumPy array of zeros of length $N$ numpy.zeros((n,m)) Create 2D NumPy array of zeros with $n$ rows and $m$ columns numpy.ones(N) Create 1D NumPy array of ones of length $N$ numpy.ones((n,m)) Create 2D NumPy array of ones with $n$ rows and $m$ columns numpy.eye(N) Create 2D NumPy array with $N$ rows and $N$ columns with ones on the diagonal (ie. the identity matrix of size $N$) Create a 1D NumPy array with 11 equally spaced values from 0 to 1: x = np.linspace(0,1,11) print(x) [0. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ] Create a 1D NumPy array with values from 0 to 20 (exclusively) incremented by 2.5: y = np.arange(0,20,2.5) print(y) [ 0. 2.5 5. 7.5 10. 12.5 15. 17.5] These are the functions that we'll use most often when creating NumPy arrays. The function numpy.linspace works best when we know the number of points we want in the array, and numpy.arange works best when we know step size between values in the array. Create a 1D NumPy array of zeros of length 5: z = np.zeros(5) print(z) [0. 0. 0. 0. 0.] Create a 2D NumPy array of zeros with 2 rows and 5 columns: M = np.zeros((2,5)) print(M) [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]] Create a 1D NumPy array of ones of length 7: w = np.ones(7) print(w) [1. 1. 1. 1. 1. 1. 1.] Create a 2D NumPy array of ones with 3 rows and 2 columns: N = np.ones((3,2)) print(N) [[1. 1.] [1. 1.] [1. 1.]] Create the identity matrix of size 10: I = np.eye(10) print(I) [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]","title":"Creating Arrays"},{"location":"scipy/numpy/#array-datatype","text":"NumPy arrays are homogeneous : all entries in the array are the same datatype. We will only work with numeric arrays and our arrays will contain either integers, floats, complex numbers or booleans. There are different kinds of datatypes provided by NumPy for different applications but we'll mostly be working with the default integer type numpy.int64 and the default float type numpy.float64 . These are very similar to the built-in Python datatypes int and float but with some differences that we won't go into. Check out the NumPy documentation on numeric datatypes for more information. The most important point for now is to know how to determine if a NumPy array contains integers elements or float elements. We can access the datatype of a NumPy array by its .dtype attribute. For example, create a 2D NumPy array from a list of lists of integers: A = np.array([[1,2,3],[4,5,6]]) print(A) [[1 2 3] [4 5 6]] We expect the datatype of A to be integers and we verify: A.dtype dtype('int64') Most of the other NumPy functions which create arrays use the numpy.float64 datatype by default. For example, using numpy.linspace : u = np.linspace(0,1,5) print(u) [0. 0.25 0.5 0.75 1. ] u.dtype dtype('float64') Notice that numbers are printed with a decimal point when the datatype of the NumPy array is any kind of float.","title":"Array Datatype"},{"location":"scipy/numpy/#dimension-shape-and-size","text":"We can think of a 1D NumPy array as a list of numbers, a 2D NumPy array as a matrix, a 3D NumPy array as a cube of numbers, and so on. Given a NumPy array, we can find out how many dimensions it has by accessing its .ndim attribute. The result is a number telling us how many dimensions it has. For example, create a 2D NumPy array: A = np.array([[1,2],[3,4],[5,6]]) print(A) [[1 2] [3 4] [5 6]] A.ndim 2 The result tells us that A has 2 dimensions. The first dimension corresponds to the vertical direction counting the rows and the second dimension corresponds to the horizontal direction counting the columns. We can find out how many rows and columns A has by accessing its .shape attribute: A.shape (3, 2) The result is a tuple (3,2) of length 2 which means that A is a 2D array with 3 rows and 2 columns. We can also find out how many entries A has in total by accessing its .size attribute: A.size 6 This is the expected result since we know that A has 3 rows and 2 columns and therefore 2(3) = 6 total entries. Create a 1D NumPy array and inspect its dimension, shape and size: r = np.array([9,3,1,7]) print(r) [9 3 1 7] r.ndim 1 r.shape (4,) r.size 4 The variable r is assigned to a 1D NumPy array of length 4. Notice that r.shape is a tuple with a single entry (4,) .","title":"Dimension, Shape and Size"},{"location":"scipy/numpy/#slicing-and-indexing","text":"Accessing the entries in an array is called indexing and accessing rows and columns (or subarrays) is called slicing . See the NumPy documentation for more information about indexing and slicing . Create a 1D NumPy array: v = np.linspace(0,5,11) print(v) [0. 0.5 1. 1.5 2. 2.5 3. 3.5 4. 4.5 5. ] Access the entries in a 1D array using the square brackets notation just like a Python list. For example, access the entry at index 3: v[3] 1.5 Notice that NumPy array indices start at 0 just like Python sequences. Create a 2D array of integers: B = np.array([[6, 5, 3, 1, 1],[1, 0, 4, 0, 1],[5, 9, 2, 2, 9]]) print(B) [[6 5 3 1 1] [1 0 4 0 1] [5 9 2 2 9]] Access the entries in a 2D array using the square brackets with 2 indices. In particular, access the entry at row index 1 and column index 2: B[1,2] 4 Access the top left entry in the array: B[0,0] 6 Negative indices work for NumPy arrays as they do for Python sequences. Access the bottom right entry in the array: B[-1,-1] 9 Access the row at index 2 using the colon : syntax: B[2,:] array([5, 9, 2, 2, 9]) Access the column at index 3: B[:,3] array([1, 0, 2]) Select the subarray of rows at index 1 and 2, and columns at index 2, 3 and 4: subB = B[1:3,2:5] print(subB) [[4 0 1] [2 2 9]] Slices of NumPy arrays are again NumPy arrays but possibly of a different dimension: subB.ndim 2 subB.shape (2, 3) type(subB) numpy.ndarray The variable subB is assigned to a 2D NumPy array of shape 2 by 2. Let's do the same for the column at index 2: colB = B[:,2] print(colB) [3 4 2] colB.ndim 1 colB.shape (3,) type(colB) numpy.ndarray The variable colB is assigned to a 1D NumPy array of length 3.","title":"Slicing and Indexing"},{"location":"scipy/numpy/#stacking","text":"We can build bigger arrays out of smaller arrays by stacking along different dimensions using the functions numpy.hstack and numpy.vstack . Stack 3 different 1D NumPy arrays of length 3 vertically forming a 3 by 3 matrix: x = np.array([1,1,1]) y = np.array([2,2,2]) z = np.array([3,3,3]) vstacked = np.vstack((x,y,z)) print(vstacked) [[1 1 1] [2 2 2] [3 3 3]] Stack 1D NumPy arrays horizontally to create another 1D array: hstacked = np.hstack((x,y,z)) print(hstacked) [1 1 1 2 2 2 3 3 3] Use numpy.hstack and numpy.vstack to build the matrix $T$ where $$ T = \\begin{bmatrix} 1 & 1 & 2 & 2 \\ 1 & 1 & 2 & 2 \\ 3 & 3 & 4 & 4 \\ 3 & 3 & 4 & 4 \\end{bmatrix} $$ A = np.ones((2,2)) B = 2*np.ones((2,2)) C = 3*np.ones((2,2)) D = 4*np.ones((2,2)) A_B = np.hstack((A,B)) print(A_B) [[1. 1. 2. 2.] [1. 1. 2. 2.]] C_D = np.hstack((C,D)) print(C_D) [[3. 3. 4. 4.] [3. 3. 4. 4.]] T = np.vstack((A_B,C_D)) print(T) [[1. 1. 2. 2.] [1. 1. 2. 2.] [3. 3. 4. 4.] [3. 3. 4. 4.]]","title":"Stacking"},{"location":"scipy/numpy/#copies-versus-views","text":"Under construction","title":"Copies versus Views"},{"location":"scipy/numpy/#operations-and-functions","text":"","title":"Operations and Functions"},{"location":"scipy/numpy/#array-operations","text":"Arithmetic operators including addition + , subtraction - , multiplication * , division / and exponentiation ** are applied to arrays elementwise . For addition and substraction, these are the familiar vector operations we see in linear algebra: v = np.array([1,2,3]) w = np.array([1,0,-1]) v + w array([2, 2, 2]) v - w array([0, 2, 4]) In the same way, array multiplication and division are performed element by element: v * w array([ 1, 0, -3]) w / v array([ 1. , 0. , -0.33333333]) Notice that the datatype of both v and w is numpy.int64 however division w / v returns an array with datatype numpy.float64 . The exponent operator ** also acts element by element in the array: v ** 2 array([1, 4, 9]) Let's see these operations for 2D arrays: A = np.array([[3,1],[2,-1]]) B = np.array([[2,-2],[5,1]]) A + B array([[ 5, -1], [ 7, 0]]) A - B array([[ 1, 3], [-3, -2]]) A / B array([[ 1.5, -0.5], [ 0.4, -1. ]]) A * B array([[ 6, -2], [10, -1]]) A ** 2 array([[9, 1], [4, 1]]) Notice that array multiplication and exponentiation are performed elementwise. In Python 3.5+, the symbol @ computes matrix multiplication for NumPy arrays: A @ B array([[11, -5], [-1, -5]]) Matrix powers are performed by the function numpy.linalg.matrix_power . It's a long function name and so it's convenient to import it with a shorter name: from numpy.linalg import matrix_power as mpow Compute $A^3$: mpow(A,3) array([[37, 9], [18, 1]]) Equivalently, use the @ operator to compute $A^3$: A @ A @ A array([[37, 9], [18, 1]])","title":"Array Operations"},{"location":"scipy/numpy/#broadcasting","text":"We know from linear algebra that we can only add matrices of the same size. Braodcasting is a set of NumPy rules which relaxes this constraint and allows us to combine a smaller array with a bigger when it makes sense. For example, suppose we want to create a 1D NumPy array of $y$ values for $x=0.0,0.25,0.5,0.75,1.0$ for the function $y = x^2 + 1$. From what we've seen so far, it makes sense to create x , then x**2 and then add an array of ones [1. 1. 1. 1. 1.] : x = np.array([0,0.25,0.5,0.75,1.0]) y = x**2 + np.array([1,1,1,1,1]) print(y) [1. 1.0625 1.25 1.5625 2. ] An example of broadcasting in NumPy is the following equivalent operation: x = np.array([0,0.25,0.5,0.75,1.0]) y = x**2 + 1 print(y) [1. 1.0625 1.25 1.5625 2. ] The number 1 is a scalar and we are adding it to a 1D NumPy array of length 5. The broadcasting rule in this case is to broadcast the scalar value 1 across the larger array. The result is a simpler syntax for a very comman operation. Let's try another example. What happens when we try to add a 1D NumPy array of length 4 to a 2D NumPy array of size 3 by 4? u = np.array([1,2,3,4]) print(u) [1 2 3 4] A = np.array([[1,1,1,1],[2,2,2,2],[3,3,3,3]]) print(A) [[1 1 1 1] [2 2 2 2] [3 3 3 3]] result = A + u print(result) [[2 3 4 5] [3 4 5 6] [4 5 6 7]] The 1D NumPy array is broadcast across the 2D array because the length of the first dimension in each array are equal!","title":"Broadcasting"},{"location":"scipy/numpy/#array-functions","text":"There are many array functions we can use to compute with NumPy arrays. The following is a partial list and we'll look closer at mathematical functions in the next section. numpy.sum numpy.prod numpy.mean numpy.max numpy.min numpy.std numpy.argmax numpy.argmin numpy.var Create a 1D NumPy array with random values and compute: arr = np.array([8,-2,4,7,-3]) print(arr) [ 8 -2 4 7 -3] Compute the mean of the values in the array: np.mean(arr) 2.8 Verify the mean once more: m = np.sum(arr) / arr.size print(m) 2.8 Find the index of the maximum element in the array: max_i = np.argmax(arr) print(max_i) 0 Verify the maximum value in the array: np.max(arr) 8 arr[max_i] 8 Array functions apply to 2D arrays as well (and $N$-dimensional arrays in general) with the added feature that we can choose to apply array functions to the entire array, down the columns or across the rows (or any axis). Create a 2D NumPy array with random values and compute the sum of all the entries: M = np.array([[2,4,2],[2,1,1],[3,2,0],[0,6,2]]) print(M) [[2 4 2] [2 1 1] [3 2 0] [0 6 2]] np.sum(M) 25 The function numpy.sum also takes a keyword argument axis which determines along which dimension to compute the sum: np.sum(M,axis=0) # Sum of the columns array([ 7, 13, 5]) np.sum(M,axis=1) # Sum of the rows array([8, 4, 5, 8])","title":"Array Functions"},{"location":"scipy/numpy/#mathematical-functions","text":"Mathematical functions in NumPy are called universal functions and are vectorized . Vectorized functions operate elementwise on arrays producing arrays as output and are built to compute values across arrays very quickly. The following is a partial list of mathematical functions: numpy.sin numpy.cos numpy.tan numpy.exp numpy.log numpy.log10 numpy.arcsin numpy.arccos numpy.arctan Compute the values $\\sin(2 \\pi x)$ for $x = 0,0.25,0.5\\dots,1.75$: x = np.arange(0,1.25,0.25) print(x) [0. 0.25 0.5 0.75 1. ] np.sin(2*np.pi*x) array([ 0.0000000e+00, 1.0000000e+00, 1.2246468e-16, -1.0000000e+00, -2.4492936e-16]) We expect the array [0. 1. 0. -1. 0.] however there is (as always with floating point numbers) some rounding errors in the result. In numerical computing, we can interpret a number such as $10^{-16}$ as $0$. Compute the values $\\log_{10}(x)$ for $x = 1,10,100,1000,10000$: x = np.array([1,10,100,1000,10000]) print(x) [ 1 10 100 1000 10000] np.log10(x) array([0., 1., 2., 3., 4.]) Note that we can also evaluate mathematical functions with scalar values: np.sin(0) 0.0 NumPy also provides familiar mathematical constants such as $\\pi$ and $e$: np.pi 3.141592653589793 np.e 2.718281828459045 For example, verify the limit $$ \\lim_{x \\to \\infty} \\arctan(x) = \\frac{\\pi}{2} $$ by evaluating $\\arctan(x)$ for some (arbitrary) large value $x$: np.arctan(10000) 1.5706963267952299 np.pi/2 1.5707963267948966","title":"Mathematical Functions"},{"location":"scipy/numpy/#random-number-generators","text":"The subpackage numpy.random contains functions to generate NumPy arrays of random numbers sampled from different distributions. The following is a partial list of distributions: Function Description numpy.random.rand(d1,...,dn) Create a NumPy array (with shape (d1,...,dn) ) with entries sampled uniformly from [0,1) numpy.random.randn(d1,...,dn) Create a NumPy array (with shape (d1,...,dn) ) with entries sampled from the standard normal distribution numpy.random.randint(a,b,size) Create a NumPy array (with shape size ) with integer entries from low (inclusive) to high (exclusive) Sample a random number from the uniform distribution : np.random.rand() 0.6695906195141056 Sample 3 random numbers: np.random.rand(3) array([0.40770395, 0.12158461, 0.72083088]) Create 2D NumPy array of random samples: np.random.rand(2,4) array([[0.61244625, 0.32645792, 0.55859886, 0.97613741], [0.57227614, 0.14315638, 0.49034299, 0.00099473]]) Random samples from the standard normal distribution : np.random.randn() 1.4440026351051256 np.random.randn(3) array([ 0.6172098 , -1.67631666, -2.20365265]) np.random.randn(3,1) array([[ 0.29643549], [-0.44039303], [-1.52246126]]) Random integers sampled uniformly from various intervals: np.random.randint(-10,10) 2 np.random.randint(0,2,(4,8)) array([[1, 0, 0, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 1, 1, 1, 1]]) np.random.randint(-9,10,(5,2)) array([[ 3, 9], [ 5, -6], [ 7, -7], [ 0, -5], [ 0, 0]])","title":"Random Number Generators"},{"location":"scipy/numpy/#examples","text":"","title":"Examples"},{"location":"scipy/numpy/#brute-force-optimization","text":"Find the absolute maximum and minimum values of the function $$ f(x) = x\\sin(x)+\\cos(4x) $$ on the interval $[0,2\\pi]$. We know that the maximum and minimum values must occur at either the endpoints $x=0,2\\pi$ or at critical points where $f'(x)=0$. However, the derivative is given by $$ f'(x) = \\sin(x) + x \\cos(x) - 4\\sin(4x) $$ and the equation $f'(x) = 0$ is impossible to solve explicitly. Instead, create a 1D NumPy array of $x$ values from $0$ to $2\\pi$ of length $N$ (for some arbitrarily large value $N$) and use the functions numpy.min and numpy.max to find maximum and minimum $y$ values, and the functions numpy.argmin and numpy.argmax to find the indices of the corresponding $x$ values. N = 10000 x = np.linspace(0,2*np.pi,N) y = x * np.sin(x) + np.cos(4*x) y_max = np.max(y) y_min = np.min(y) x_max = x[np.argmax(y)] x_min = x[np.argmin(y)] print('Absolute maximum value is y =',y_max,'at x =',x_max) print('Absolute minimum value is y =',y_min,'at x =',x_min) Absolute maximum value is y = 2.5992726072887007 at x = 1.628136126702901 Absolute minimum value is y = -5.129752039182 at x = 5.34187001663503","title":"Brute Force Optimization"},{"location":"scipy/numpy/#riemann-sums","text":"Write a function called exp_int which takes input parameters $b$ and $N$ and returns the (left) Riemann sum $$ \\int_0^b e^{-x^2} dx \\approx \\sum_{k=0}^{N-1} e^{-x_k^2} \\Delta x $$ for $\\Delta x = b/N$ and the partition $x_k=k \\, \\Delta x$, $k=0,\\dots,N$. def exp_int(b,N): \"Compute left Riemann sum of exp(-x^2) from 0 to b with N subintervals.\" x = np.linspace(0,b,N+1) x_left_endpoints = x[:-1] Delta_x = b/N I = Delta_x * np.sum(np.exp(-x_left_endpoints**2)) return I The infinite integral satisfies the beautiful identity $$ \\int_0^{\\infty} e^{-x^2} dx = \\frac{\\sqrt{\\pi}}{2} $$ Compute the integral with large values of $b$ and $N$: exp_int(100,100000) 0.886726925452758 Compare to the true value: np.pi**0.5/2 0.8862269254527579","title":"Riemann Sums"},{"location":"scipy/numpy/#infinite-products","text":"The cosine function has the following infinite product representation $$ \\cos x = \\prod_{k = 1}^{\\infty} \\left(1 - \\frac{4 x^2}{\\pi^2 (2k - 1)^2} \\right) $$ Write a function called cos_product which takes input parameters $x$ and $N$ and returns the $N$th partial product $$ \\prod_{k = 1}^{N} \\left(1 - \\frac{4 x^2}{\\pi^2 (2k - 1)^2} \\right) $$ def cos_product(x,N): \"Compute the product \\prod_{k=1}^N (1 - 4x^2/(pi^2 (2k - 1)^2).\" k = np.arange(1,N+1) terms = 1 - 4*x**2 / (np.pi**2 * (2*k - 1)**2) return np.prod(terms) Verify our function using values for which we know the result. For example, $\\cos(0)=1$, $\\cos(\\pi)=-1$ and $\\cos(\\pi/4) = \\frac{1}{\\sqrt{2}}$. cos_product(0,10) 1.0 cos_product(np.pi,10000) -1.0001000050002433 cos_product(np.pi/4,10000000) 0.7071067856245614 1/2**0.5 0.7071067811865475","title":"Infinite Products"},{"location":"scipy/numpy/#matrix-multiplication","text":"Under construction","title":"Matrix Multiplication"},{"location":"scipy/numpy/#exercises","text":"Exercise 1. The natural log satisfies the following definite integral $$ \\int_1^e \\frac{\\ln x \\ dx}{(1 + \\ln x)^2} = \\frac{e}{2} - 1 $$ Write a function called log_integral which takes input parameters $c$ and $N$ and returns the value of the (right) Riemann sum $$ \\int_1^c \\frac{\\ln x \\ dx}{(1 + \\ln x)^2} \\approx \\sum_{k=1}^N \\frac{\\ln x_k \\ \\Delta x}{(1 + \\ln x_k)^2} \\ , \\ \\ \\Delta x = \\frac{c - 1}{N} $$ for the partition $x_k = 1 + k \\Delta x$, for $k = 0, \\dots , N$. Exercise 2. Write a function called k_sum which takes input parameters k and N and returns the partial sum $$ \\sum_{n=1}^{N} \\frac{1}{n^k} $$ Verify your function by comparing to the infinite series identity $$ \\sum_{n=1}^{\\infty} \\frac{(-1)^{n+1}}{n^2} = \\frac{\\pi^2}{12} $$ Exercise 3. Write a function called dot which takes 3 inputs M , i and j where M is a square NumPy array and the function returns the dot product of the $i$th row and the $j$th column of $M$.","title":"Exercises"},{"location":"scipy/scipy/","text":"SciPy Under construction","title":"SciPy"},{"location":"scipy/scipy/#scipy","text":"Under construction","title":"SciPy"},{"location":"solutions/solutions/","text":"Solutions to Exercises Jupyter LaTeX Exercise 1 $$\\cos(\\alpha \\pm \\beta) = \\cos \\alpha \\cos \\beta \\mp \\sin \\alpha \\sin \\beta$$ Exercise 2 $$\\int \\frac{1}{1 + x^2} \\, dx = \\arctan x + C$$ Exercise 3 $$\\frac{\\partial \\mathbf{u}}{\\partial t} + (\\mathbf{u} \\cdot \\nabla) \\mathbf{u} - \\nu \\nabla^2 \\mathbf{u} = - \\nabla w + \\mathbf{g}$$ Exercise 4 $$\\oint_C (L dx + M dy) = \\iint_D \\left( \\frac{\\partial M}{\\partial x} - \\frac{\\partial L}{\\partial y} \\right) dx \\, dy$$ Exercise 5 $$\\lim_{x \\to \\infty} \\frac{\\pi(x)}{ \\frac{x}{\\log(x)}} = 1$$ Exercise 6 $$\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n!} (x-a)^n$$ Exercise 7 $$\\int_{\\partial \\Omega} \\omega = \\int_{\\Omega} d \\omega$$ Exercise 8 $$\\mathrm{Hom}(U \\otimes V,W) \\cong \\mathrm{Hom}(U, \\mathrm{Hom}(V,W))$$ Exercise 9 $$\\mathscr{L} \\{ f(t) \\} = F(s) = \\int_0^{\\infty} f(t) e^{-st} dt$$ Exercise 10 $$\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}$$ Exercise 11 $$\\sin x = x \\prod_{n=1}^{\\infty} \\left( 1 - \\frac{x^2}{\\pi^2 n^2} \\right)$$ Python Numbers Exercise 1 1 - 2**2/(2*1) + 2**4/(4*3*2*1) - 2**6/(6*5*4*3*2*1) -0.4222222222222223 Exercise 2 1 + 1/2**2 + 1/3**2 + 1/4**2 + 1/5**2 1.4636111111111112 1/((1 - 2**(-2)) * (1 - 3**(-2)) * (1 - 5**(-2)) * (1 - 7**(-2)) * (1 - 11**(-2))) 1.6083441840277781 3.14159**2/6 1.6449312880166664 Looks the like product converges more quickly. Exercise 3 1 + 1/(2 + 1/(2 + 1/(2 + 1/2))) 1.4137931034482758 Exercise 4 All are float . Exercise 5 type(x / y) is float . type(x * y) and type(x + y) are int . Exercise 6 float Exercise 7 float or complex Exercise 8 int or float Variables Exercise 1 False Sequences Exercise 1 x = 1 sum([(-1)**n * x**(2*n + 1)/(2*n + 1) for n in range(0,5001)]) 0.7854481533989477 Exercise 2 sum([(-1)**(n+1)/n for n in range(1,2001)]) 0.6928972430599403 Exercise 3 [[n,n**2] for n in range(0,8)] [[0, 0], [1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36], [7, 49]]","title":"Solutions to Exercises"},{"location":"solutions/solutions/#solutions-to-exercises","text":"","title":"Solutions to Exercises"},{"location":"solutions/solutions/#jupyter","text":"","title":"Jupyter"},{"location":"solutions/solutions/#latex","text":"Exercise 1 $$\\cos(\\alpha \\pm \\beta) = \\cos \\alpha \\cos \\beta \\mp \\sin \\alpha \\sin \\beta$$ Exercise 2 $$\\int \\frac{1}{1 + x^2} \\, dx = \\arctan x + C$$ Exercise 3 $$\\frac{\\partial \\mathbf{u}}{\\partial t} + (\\mathbf{u} \\cdot \\nabla) \\mathbf{u} - \\nu \\nabla^2 \\mathbf{u} = - \\nabla w + \\mathbf{g}$$ Exercise 4 $$\\oint_C (L dx + M dy) = \\iint_D \\left( \\frac{\\partial M}{\\partial x} - \\frac{\\partial L}{\\partial y} \\right) dx \\, dy$$ Exercise 5 $$\\lim_{x \\to \\infty} \\frac{\\pi(x)}{ \\frac{x}{\\log(x)}} = 1$$ Exercise 6 $$\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n!} (x-a)^n$$ Exercise 7 $$\\int_{\\partial \\Omega} \\omega = \\int_{\\Omega} d \\omega$$ Exercise 8 $$\\mathrm{Hom}(U \\otimes V,W) \\cong \\mathrm{Hom}(U, \\mathrm{Hom}(V,W))$$ Exercise 9 $$\\mathscr{L} \\{ f(t) \\} = F(s) = \\int_0^{\\infty} f(t) e^{-st} dt$$ Exercise 10 $$\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}$$ Exercise 11 $$\\sin x = x \\prod_{n=1}^{\\infty} \\left( 1 - \\frac{x^2}{\\pi^2 n^2} \\right)$$","title":"LaTeX"},{"location":"solutions/solutions/#python","text":"","title":"Python"},{"location":"solutions/solutions/#numbers","text":"Exercise 1 1 - 2**2/(2*1) + 2**4/(4*3*2*1) - 2**6/(6*5*4*3*2*1) -0.4222222222222223 Exercise 2 1 + 1/2**2 + 1/3**2 + 1/4**2 + 1/5**2 1.4636111111111112 1/((1 - 2**(-2)) * (1 - 3**(-2)) * (1 - 5**(-2)) * (1 - 7**(-2)) * (1 - 11**(-2))) 1.6083441840277781 3.14159**2/6 1.6449312880166664 Looks the like product converges more quickly. Exercise 3 1 + 1/(2 + 1/(2 + 1/(2 + 1/2))) 1.4137931034482758 Exercise 4 All are float . Exercise 5 type(x / y) is float . type(x * y) and type(x + y) are int . Exercise 6 float Exercise 7 float or complex Exercise 8 int or float","title":"Numbers"},{"location":"solutions/solutions/#variables","text":"Exercise 1 False","title":"Variables"},{"location":"solutions/solutions/#sequences","text":"Exercise 1 x = 1 sum([(-1)**n * x**(2*n + 1)/(2*n + 1) for n in range(0,5001)]) 0.7854481533989477 Exercise 2 sum([(-1)**(n+1)/n for n in range(1,2001)]) 0.6928972430599403 Exercise 3 [[n,n**2] for n in range(0,8)] [[0, 0], [1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36], [7, 49]]","title":"Sequences"}]}